* Notes
  :PROPERTIES:
  :ID:       4391d46d-e2d6-4f77-8822-faa1c972b411
  :END:
* Mutt FAQ  https://www.fefe.de/muttfaq/faq.html             :Technical:NOTE:
 [2021-06-26 Sat 03:51]
* Linux Kernel WIKI  https://wiki.kernel.org/                :Technical:NOTE:
 [2021-06-26 Sat]
* Vim Tips  https://vim.fandom.com/wiki/Best_Vim_Tips        :Technical:NOTE:
 [2021-06-26 Sat 16:54]
 :NOTE: [[https://vim.fandom.com/wiki/Best_Vim_Tips][Best vim tips site]]
* This are details of passport for the family                 :NOTE: :ATTACH:
  :PROPERTIES:
  :ID:       75395de8-7503-4034-8c5b-283d650bc50b
  :END:
  :LOGBOOK:
  CLOCK: [2021-06-05 Sat 11:18]--[2021-06-05 Sat 11:21] =>  0:03
  :END:
[2021-06-05 Sat 11:18]
[[file:~/.emacs.d/OrgFiles/task.org::*Reminds about passport renewals][Reminds about passport renewals]]
* GCC: [[https://gcc.gnu.org/][The GNU Compiler Collection]]                           :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-06-24 Thu 14:16]--[2021-06-24 Thu 14:17] =>  0:01
  :END:
[2021-06-24 Thu 14:16]
[[deft:contacts.org][contacts]]
* Clang: [[https://clang.llvm.org/][A C language family frontend for LLVM]]               :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-06-24 Thu 14:18]--[2021-06-24 Thu 14:19] =>  0:01
  :END:
[2021-06-24 Thu 14:18]
[[deft:notes.org][notes]]
* LLVM: [[https://www.llvm.org/][The LLVM Compiler Infrastructure]]                     :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-06-24 Thu 14:19]--[2021-06-24 Thu 14:20] =>  0:01
  :END:
[2021-06-24 Thu 14:19]
[[deft:notes.org][notes]]
* LFS: [[https://www.linuxfromscratch.org/][Linux From Scratch!]]                                   :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-06-24 Thu 14:21]--[2021-06-24 Thu 14:22] =>  0:01
  :END:
[2021-06-24 Thu 14:21]
[[deft:notes.org][notes]]
* Bash Wiki: [[http://mywiki.wooledge.org/BashFAQ/006][Bash FAQ]]                                        :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-06-24 Thu 14:39]--[2021-06-24 Thu 14:40] =>  0:01
  :END:
[2021-06-24 Thu 14:39]
* Bash-Awesome: [[https://github.com/awesome-lists/awesome-bash][A curated bash goodies  ]]                     :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-06-24 Thu 14:41]--[2021-06-24 Thu 14:41] =>  0:00
  :END:
[2021-06-24 Thu 14:41]
* My Profile                                              :@work:NOTE:ATTACH:
  :PROPERTIES:
  :ID:       7743bef9-38b9-47dc-bb7f-fb8af19c54e8
  :END:
  :LOGBOOK:
  CLOCK: [2021-07-20 Tue 04:21]--[2021-07-20 Tue 04:23] =>  0:02
  :END:
[2021-07-20 Tue 04:21]
* GPG backup and restore keys                                   :NOTE:ATTACH:
  :PROPERTIES:
  :ID:       5dc4304b-f412-476a-ab07-c463bb3cfaca
  :END:

  :LOGBOOK:
  CLOCK: [2021-07-20 Tue 04:26]--[2021-07-20 Tue 04:27] =>  0:01
  :END:
[2021-07-20 Tue 04:26]
* Questions to Recuiters                                        :NOTE:ATTACH:
  :PROPERTIES:
  :ID:       82a6e98a-bc92-4590-853b-77f93dc6a2f3
  :END:

  :LOGBOOK:
  CLOCK: [2021-07-20 Tue 04:28]--[2021-07-20 Tue 04:28] =>  0:00
  :END:
[2021-07-20 Tue 04:28]
* SSH Cheatsheet                                                :NOTE:ATTACH:
  :PROPERTIES:
  :ID:       ebd5f654-430f-4e5c-828a-478c1a451e5d
  :END:
    :LOGBOOK:
  CLOCK: [2021-07-20 Tue 04:41]--[2021-07-20 Tue 04:41] =>  0:00
  :END:
[2021-07-20 Tue 04:41]
* WPA Cli procedure get connected to internet                   :NOTE:ATTACH:
  :PROPERTIES:
  :ID:       5b554888-b040-4a5e-b68f-8efa1dc7c486
  :END:
    :LOGBOOK:
  CLOCK: [2021-07-20 Tue 04:42]--[2021-07-20 Tue 04:42] =>  0:00
  :END:
[2021-07-20 Tue 04:42]
* Firefox userChrome and userContent css files                  :NOTE:ATTACH:
  :PROPERTIES:
  :ID:       b3ff2e72-38c4-4137-80a9-98f170b632cf
  :END:
    :LOGBOOK:
  CLOCK: [2021-07-20 Tue 04:43]--[2021-07-20 Tue 04:44] =>  0:01
  :END:
[2021-07-20 Tue 04:43]
* Bash Oneliner                                              :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-09 Mon 12:09]--[2021-08-09 Mon 12:10] =>  0:01
  :END:
[2021-08-09 Mon 12:09]
[[https://onceupon.github.io/Bash-Oneliner/][https://onceupon.github.io/Bash-Oneliner/]]
* Git bisection to find bug in the kernel                    :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-19 Thu 14:17]--[2021-08-19 Thu 14:18] =>  0:01
  :END:
[2021-08-19 Thu 14:17]
[[https://mirrors.edge.kernel.org/pub/software/scm/git/docs/git-bisect-lk2009.html][https://mirrors.edge.kernel.org/pub/software/scm/git/docs/git-bisect-lk2009.html]]
* Sysadmin Resources                                         :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-23 Mon 14:17]--[2021-08-23 Mon 14:18] =>  0:01
  :END:
[2021-08-23 Mon 14:17]
[[https://github.com/kahun/awesome-sysadmin][https://github.com/kahun/awesome-sysadmin]]
* Aws-scripting                                              :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-31 Tue 11:10]--[2021-08-31 Tue 11:11] =>  0:01
  :END:
[2021-08-31 Tue 11:10]
[[http://bruxy.regnet.cz/web/programming/EN/awscli/][http://bruxy.regnet.cz/web/programming/EN/awscli/]]
* SSH Tricks: Reverse Tunnel and via Bastion Host            :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-31 Tue 11:30]--[2021-08-31 Tue 11:31] =>  0:01
  :END:
[2021-08-31 Tue 11:30]
[[http://bruxy.regnet.cz/web/linux/EN/ssh-bastion/][http://bruxy.regnet.cz/web/linux/EN/ssh-bastion/]]
* Online disk resizing in Cloud and on perm                  :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-31 Tue 11:40]--[2021-08-31 Tue 11:40] =>  0:00
  :END:
[2021-08-31 Tue 11:40]
[[http://bruxy.regnet.cz/web/linux/EN/disk-resize/][http://bruxy.regnet.cz/web/linux/EN/disk-resize/]]
* SSH-Agent Abuse ...illegal access                          :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-31 Tue 12:25]--[2021-08-31 Tue 12:25] =>  0:00
  :END:
[2021-08-31 Tue 12:25]
[[http://bruxy.regnet.cz/web/linux/EN/ssh-agent/][http://bruxy.regnet.cz/web/linux/EN/ssh-agent/]]
* AWS DNS related stuff                                      :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-31 Tue 12:56]--[2021-08-31 Tue 12:57] =>  0:01
  :END:
[2021-08-31 Tue 12:56]
[[http://bruxy.regnet.cz/web/linux/EN/route53-domain-transfer-problem/][http://bruxy.regnet.cz/web/linux/EN/route53-domain-transfer-problem/]]
* VIM Cheatsheet                                             :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-31 Tue 13:31]--[2021-08-31 Tue 13:31] =>  0:00
  :END:
[2021-08-31 Tue 13:31]
[[http://bruxy.regnet.cz/web/programming/EN/vim-cheat-sheet/][http://bruxy.regnet.cz/web/programming/EN/vim-cheat-sheet/]]
* Nginx redirection based on username                        :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-31 Tue 13:38]--[2021-08-31 Tue 13:39] =>  0:01
  :END:
[2021-08-31 Tue 13:38]
[[http://bruxy.regnet.cz/web/linux/EN/nginx-auth-redir/][http://bruxy.regnet.cz/web/linux/EN/nginx-auth-redir/]]
* CPU Isolation Part-I                                       :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-05 Sun 10:49]--[2021-09-05 Sun 10:49] =>  0:00
  :END:
[2021-09-05 Sun 10:49]
[[https://www.suse.com/c/cpu-isolation-introduction-part-1/][https://www.suse.com/c/cpu-isolation-introduction-part-1/]]
* CPU Isolation  Part-II                                     :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-05 Sun 10:49]--[2021-09-05 Sun 10:50] =>  0:01
  :END:
[2021-09-05 Sun 10:49]
[[https://www.suse.com/c/cpu-isolation-full-dynticks-part2/][https://www.suse.com/c/cpu-isolation-full-dynticks-part2/]]
* CPU Isolation PART-III                                     :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-05 Sun 10:50]--[2021-09-05 Sun 10:51] =>  0:01
  :END:
[2021-09-05 Sun 10:50]
[[https://www.suse.com/c/cpu-isolation-nohz_full-part-3/][https://www.suse.com/c/cpu-isolation-nohz_full-part-3/]]
* CPU Isolation Part-IV                                      :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-05 Sun 10:51]--[2021-09-05 Sun 10:52] =>  0:01
  :END:
[2021-09-05 Sun 10:51]
[[https://www.suse.com/c/cpu-isolation-housekeeping-and-tradeoffs-part-4/][https://www.suse.com/c/cpu-isolation-housekeeping-and-tradeoffs-part-4/]]
* CPU Isolation Part-V                                       :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-12-14 Wed 08:07]--[2022-12-14 Wed 08:08] =>  0:01
:END:
[2022-12-14 Wed 08:07]
[[https://www.suse.com/c/cpu-isolation-practical-example-part-5/][https://www.suse.com/c/cpu-isolation-practical-example-part-5/]]
* CPU Isolation Part -VI                                     :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-12-14 Wed 08:03]--[2022-12-14 Wed 08:04] =>  0:01
:END:
[2022-12-14 Wed 08:03]
[[https://www.suse.com/c/cpu-isolation-nohz_full-troubleshooting-tsc-clocksource-by-suse-labs-part-6/][https://www.suse.com/c/cpu-isolation-nohz_full-troubleshooting-tsc-clocksource-by-suse-labs-part-6/]]
* Do Not Use -Werror in release code                         :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-06 Mon 17:15]--[2021-09-06 Mon 17:15] =>  0:00
  :END:
[2021-09-06 Mon 17:15]
[[https://flameeyes.blog/2009/02/25/future-proof-your-code-dont-use-werror/][https://flameeyes.blog/2009/02/25/future-proof-your-code-dont-use-werror/]]

* Discovering ELF Part-I                                     :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-12-23 Thu 17:18]--[2021-12-23 Thu 17:19] =>  0:01
  :END:
[2021-12-23 Thu 17:18]
[[https://kestrelcomputer.github.io/kestrel/2018/01/29/on-elf][https://kestrelcomputer.github.io/kestrel/2018/01/29/on-elf]]
* Discovering ELF Part-II                                    :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-12-23 Thu 17:19]--[2021-12-23 Thu 17:19] =>  0:00
  :END:
[2021-12-23 Thu 17:19]
[[https://kestrelcomputer.github.io/kestrel/2018/02/01/on-elf-2][https://kestrelcomputer.github.io/kestrel/2018/02/01/on-elf-2]]
* Linux ELF                                                  :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-12-24 Fri 03:58]--[2021-12-24 Fri 03:59] =>  0:01
  :END:
[2021-12-24 Fri 03:58]
[[https://www.muppetlabs.com/~breadbox/software/tiny/teensy.html][https://www.muppetlabs.com/~breadbox/software/tiny/teensy.html]]
* TIS ELF                                                       :NOTE:ATTACH:

  :PROPERTIES:
  :ID:       90f3db0e-8286-4bb2-8ed8-2e244f24c99d
  :END:

  :LOGBOOK:
  CLOCK: [2021-12-24 Fri 04:54]--[2021-12-24 Fri 04:56] =>  0:02
  :END:
[2021-12-24 Fri 04:54]
* File Text Transcription ELF                                :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-12-24 Fri 04:52]--[2021-12-24 Fri 04:53] =>  0:01
  :END:
[2021-12-24 Fri 04:52]
[[https://www.muppetlabs.com/~breadbox/software/ELF.txt][https://www.muppetlabs.com/~breadbox/software/ELF.txt]]
* Linux Page Cache                                           :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-10-03 Sun 05:04]--[2021-10-03 Sun 05:05] =>  0:01
  :END:
[2021-10-03 Sun 05:04]
https://biriukov.dev/docs/page-cache/4-page-cache-eviction-and-page-reclaim/
* Linux Kernel Debugging tools and presentations                  :Technical:
  :LOGBOOK:
  CLOCK: [2021-09-08 Wed 15:40]--[2021-09-08 Wed 15:40] =>  0:00
  :END:
[2021-09-08 Wed 15:40]
[[https://elinux.org/Tools_and_Debugging_Presentations][https://elinux.org/Tools_and_Debugging_Presentations]]
* POSIX thread create and access ABI                         :Technical:NOTE:
  Posix threads this type of variable can be created via pthread_key_create and
  accessed via pthread_getspecific and pthread_setspecific.
  :LOGBOOK:
  CLOCK: [2022-01-11 Tue 11:27]--[2022-01-11 Tue 11:28] =>  0:01
  :END:
[2022-01-11 Tue 11:27]
* TTY DeMystified                                            :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2022-02-20 Sun 09:39]--[2022-02-20 Sun 09:40] =>  0:01
  :END:
[2022-02-20 Sun 09:39]
[[https://www.linusakesson.net/programming/tty/][https://www.linusakesson.net/programming/tty/]]
* Toolchains                                                 :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2022-01-11 Tue 11:24]--[2022-01-11 Tue 11:25] =>  0:01
  :END:
[2022-01-11 Tue 11:24]
[[https://www.toolchains.net/][https://www.toolchains.net/]]
* Beginner's Guide to Linkers                                :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2022-03-06 Sun 15:26]--[2022-03-06 Sun 15:28] =>  0:02
  :END:
[2022-03-06 Sun 15:26]
[[https://www.lurklurk.org/linkers/linkers.html][https://www.lurklurk.org/linkers/linkers.html]]
  :LOGBOOK:
* OBJCOPY : A low level linux tool                           :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2022-03-02 Wed 08:25]--[2022-03-02 Wed 08:26] =>  0:01
  :END:
[2022-03-02 Wed 08:25]
[[https://www.sanfoundry.com/objcopy-command-usage-examples-in-linux/][https://www.sanfoundry.com/objcopy-command-usage-examples-in-linux/]]
* Autotools scripts                                          :Technical:NOTE:

tool	          description	                         requires	                     produces
autoscan	generates template configure.ac based on your application source		configure.scan
autoheader	generates a header that can contains platform specific constants	configure.ac	config.h.in
autoconf	generates the configure shell script that will be used for system profiling	configure.ac , config.h.in	configure, config.h
automake	works in conjunction with autoconf to produce Makefiles	Makefile.am	Makefile.in
(g)libtoolize	copies scripts to package to enable the building of shared libraries
aclocal	creates a file containing macros required for automake	configure.ac, Makefile.am	aclocal.m4
autoreconf	runs autoconf, autoheader, aclocal, automake, libtoolize	all of the above	configure

Check it : http://www.ifnamemain.com/posts/2014/Mar/13/autoconf_automake/
  :LOGBOOK:
  CLOCK: [2022-03-20 Sun 13:56]--[2022-03-20 Sun 13:57] =>  0:01
  :END:
[2022-03-20 Sun 13:55]
* AWK explained                                              :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-10 Fri 03:18]--[2021-09-10 Fri 03:18] =>  0:00
  :END:
[2021-09-10 Fri 03:18]
[[https://medium.com/analytics-vidhya/use-awk-to-save-time-and-money-in-data-science-eb4ea0b7523f][https://medium.com/analytics-vidhya/use-awk-to-save-time-and-money-in-data-science-eb4ea0b7523f]]
* AWK pipeline for data processing                           :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-10 Fri 03:41]--[2021-09-10 Fri 03:42] =>  0:01
  :END:
[2021-09-10 Fri 03:41]
[[https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html][https://adamdrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html]]
* AWK Wrappers                                               :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-10 Fri 04:30]--[2021-09-10 Fri 04:31] =>  0:01
  :END:
[2021-09-10 Fri 04:30]
[[https://github.com/cheusov/runawk][https://github.com/cheusov/runawk]]
* AWK Debugger                                               :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-10 Fri 04:32]--[2021-09-10 Fri 04:32] =>  0:00
  :END:
[2021-09-10 Fri 04:32]
[[https://www.gnu.org/software/gawk/manual/html_node/Debugger.html][https://www.gnu.org/software/gawk/manual/html_node/Debugger.html]]
* The UNIX Operating System                                  :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-09-14 Tue 02:58]--[2021-09-14 Tue 02:58] =>  0:00
  :END:
[2021-09-14 Tue 02:58]
[[https://dra.vin/unix-info/][https://dra.vin/unix-info/]]
* MIT Assem                                                            :NOTE:
  :LOGBOOK:
  CLOCK: [2021-10-04 Mon 16:12]--[2021-10-04 Mon 16:13] =>  0:01
  :END:
[2021-10-04 Mon 16:12]
[[https://www.youtube.com/watch?v][https://www.youtube.com/watch?v]]
* Plan9 As desktop                                           :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-10-05 Tue 02:22]--[2021-10-05 Tue 02:22] =>  0:00
  :END:
[2021-10-05 Tue 02:21]
[[https://pspodcasting.net/dan/blog/2019/plan9_desktop.html][https://pspodcasting.net/dan/blog/2019/plan9_desktop.html]]
* Here are the most important keys to know when dealing with clocks :Technical:NOTE:
Key	Calls	Action
C-c C-x C-i	org-clock-in	Clock in to the section you're currently in
C-c C-x C-x	org-clock-in-last	Clock in to the last clocked task
C-c C-x C-o	org-clock-out	Clock out of whatever you're clocked in to
C-c C-x C-j	org-clock-goto	Jump to whatever headline you are currently clocked in to
C-c C-x C-q	org-clock-cancel	Cancel the current clock (removes all of it's current time)
C-c C-x C-d	org-clock-display	Display clock times for headlines in current file
C-c C-x C-r	org-clock-report	Generate a report for clock activity
C-c C-x C-z	org-resolve-clocks	Resolve any half-open clocks
  :LOGBOOK:
  CLOCK: [2021-10-21 Thu 15:24]--[2021-10-21 Thu 15:25] =>  0:01
  :END:
[2021-10-21 Thu 15:24]
* org-insert code                                            :Technical:NOTE:
  (defun org-insert-image ()
  (interactive)
  (let* ((path (concat default-directory "img/"))
         (image-file (concat
                      path
                      (buffer-name)
                      (format-time-string "_%Y%m%d_%H%M%S.png"))))
    (if (not (file-exists-p path))
        (mkdir path))
    (shell-command (concat "pngpaste " image-file))
    (org-insert-link nil (concat "file:" image-file) ""))
  )
  :LOGBOOK:
  CLOCK: [2021-12-12 Sun 16:53]--[2021-12-12 Sun 16:54] =>  0:01
  :END:
[2021-12-12 Sun 16:53]
* Rob Pike's 5 rules for programming                         :Technical:NOTE:
  Rob Pike's 5 Rules of Programming

    Rule 1. You can't tell where a program is going to spend its time. Bottlenecks occur in surprising places, so don't try to second guess and put in a speed hack until you've proven that's where the bottleneck is.

    Rule 2. Measure. Don't tune for speed until you've measured, and even then don't unless one part of the code overwhelms the rest.

    Rule 3. Fancy algorithms are slow when n is small, and n is usually small. Fancy algorithms have big constants. Until you know that n is frequently going to be big, don't get fancy. (Even if n does get big, use Rule 2 first.)

    Rule 4. Fancy algorithms are buggier than simple ones, and they're much harder to implement. Use simple algorithms as well as simple data structures.

    Rule 5. Data dominates. If you've chosen the right data structures and organized things well, the algorithms will almost always be self-evident. Data structures, not algorithms, are central to programming.

Pike's rules 1 and 2 restate Tony Hoare's famous maxim "Premature optimization is the root of all evil." Ken Thompson rephrased Pike's rules 3 and 4 as "When in doubt, use brute force.". Rules 3 and 4 are instances of the design philosophy KISS. Rule 5 was previously stated by Fred Brooks in The Mythical Man-Month. Rule 5 is often shortened to "write stupid code that uses smart objects".
  :LOGBOOK:
  CLOCK: [2021-12-22 Wed 05:33]--[2021-12-22 Wed 05:33] =>  0:00
  :END:
[2021-12-22 Wed 05:32]
* A saga about running own mailserver and heads up!          :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-12-28 Tue 12:16]--[2021-12-28 Tue 12:17] =>  0:01
  :END:
[2021-12-28 Tue 12:16]
[[https://computer.rip/2021-12-26-diy-mail.html][https://computer.rip/2021-12-26-diy-mail.html]]
* Python CheatSheet                                          :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2021-08-31 Tue 13:32]--[2021-08-31 Tue 13:32] =>  0:00
  :END:
[2021-08-31 Tue 13:32]
[[http://bruxy.regnet.cz/web/programming/EN/python-cheatsheet/][http://bruxy.regnet.cz/web/programming/EN/python-cheatsheet/]]
* Turn off superscript and subscript in org buffer           :Technical:NOTE:
  org-toggle-pretty-entities
  :LOGBOOK:
  CLOCK: [2022-01-14 Fri 13:54]--[2022-01-14 Fri 13:56] =>  0:02
  :END:
[2022-01-14 Fri 13:54]
* Permanent highlight in doc with code example               :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2022-01-15 Sat 16:20]--[2022-01-15 Sat 16:20] =>  0:00
  :END:
[2022-01-15 Sat 16:20]
[[https://kitchingroup.cheme.cmu.edu/blog/2015/07/28/A-highlight-annotation-mode-for-Emacs-using-font-lock/  ][https://kitchingroup.cheme.cmu.edu/blog/2015/07/28/A-highlight-annotation-mode-for-Emacs-using-font-lock/]]
* Emerge default opts in make.conf                           :Technical:NOTE:
  EMERGE_DEFAULT_OPTS="--alphabetical --keep-going --load-average=8 --autounmask=y
  --autounmask-write=n --verbose-conflicts --backtrack=100" and MAKEOPTS="-j9
  -l8" in my make.conf if you're curious.
  :LOGBOOK:
  CLOCK: [2022-01-16 Sun 05:34]--[2022-01-16 Sun 05:35] =>  0:01
  :END:
[2022-01-16 Sun 05:34]
* Vim plugin Scratchbuffer                                   :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2022-01-23 Sun 14:14]--[2022-01-23 Sun 14:15] =>  0:01
  :END:
[2022-01-23 Sun 14:14]
[[https://vimways.org/2019/writing-vim-plugin/][https://vimways.org/2019/writing-vim-plugin/]]
* Apache Mod Rewrite Flags                                   :Technical:NOTE:
 [R]	 Redirect you can add an =301 or =302 to change the type.

[F]	 Forces the url to be forbidden. 403 header

[G]	 Forces the url to be gone 401 header
[L]	 Last rule. (You should use this on all your rules that don't link together)
[N]	 Next round. Rerun the rules again from the start

[C]	 Chains a rewrite rule together with the next rule.
[T]	 use T=MIME-type to force the file to be a mime type
[NS]	 Use if no sub request is requested
[NC]	 Makes the rule case INsensitive
[QSA]	 Query String Append use to add to an existing query string
[NE]	 Turns of normal escapes that are default in the rewriterule
[PT]	 Pass through to the handler (together with mod alias)
[S]	 Skip the next rule S=3 skips the next 3 rules
[E]	 E=var sets an enviromental variable that can be ca
  :LOGBOOK:
  CLOCK: [2022-02-04 Fri 10:32]--[2022-02-04 Fri 10:32] =>  0:00
  :END:
[2022-02-04 Fri 10:31]
* Ecomplete Emacs contact manager                            :Technical:NOTE:
  (setq message-mail-alias-type 'ecomplete)

Alternatively, if you prefer to use the standard completion-at-point instead of
ecomplete's handicrafted UI (works well with Orderless or flex completion
styles) you can use the following:

(ecomplete-setup)
(add-hook 'message-sent-hook 'message-put-addresses-in-ecomplete)
(setq message-mail-alias-type nil
      message-expand-name-standard-ui t)

  :LOGBOOK:
  CLOCK: [2022-02-05 Sat 16:29]--[2022-02-05 Sat 16:30] =>  0:01
  :END:
[2022-02-05 Sat 16:29]
* Org-mode-tweak                                             :Technical:NOTE:
     ;; Improve org mode looks
    (setq org-startup-indented t
          org-pretty-entities t
          org-hide-emphasis-markers t
          org-startup-with-inline-images t
          org-image-actual-width '(300))
  :LOGBOOK:
  CLOCK: [2022-02-09 Wed 16:48]--[2022-02-09 Wed 16:49] =>  0:01
  :END:
[2022-02-09 Wed 16:48]
[[file:~/.emacs.d/OrgFiles/task.org::*My Emacs config file : \[\[file://home/bhaskar/.emacs\]\[.emacs\]\]][My Emacs config file : .emacs]]
* Bash special variables                                     :Technical:NOTE:
$0 - The name of the Bash script.
$1 - $9 - The first 9 arguments to the Bash script.
$# - How many arguments were passed to the Bash script.
$@ - All the arguments supplied to the Bash script.
$? - The exit status of the most recently run process.
$$ - The process ID of the current script.
$USER - The username of the user running the script.
$HOSTNAME - The hostname of the machine the script is running on.
$SECONDS - The number of seconds since the script was started.
$RANDOM - Returns a different random number each time is it referred to.
$LINENO - Returns the current line number in the Bash script.
  :LOGBOOK:
  CLOCK: [2022-02-16 Wed 04:40]--[2022-02-16 Wed 04:41] =>  0:01
  :END:
[2022-02-16 Wed 04:40]
* DRBD Aka Distributed Replicated Storage System             :Technical:NOTE:
  CLOCK: [2022-03-08 Tue 19:44]--[2022-03-08 Tue 19:45] =>  0:01
  :END:
[2022-03-08 Tue 19:44]
[[https://linbit.com/drbd/][https://linbit.com/drbd/]]
[[https://linbit.com/wp-content/uploads/2020/03/DRBD-Diagram-1536x1230.jpg][DRBD Cocept Picture]]
* Linux Process Trace                                        :Technical:NOTE:
  https://idea.popcount.org/2012-12-11-linux-process-states/
  :LOGBOOK:
  CLOCK: [2022-03-30 Wed 07:23]--[2022-03-30 Wed 07:23] =>  0:00
  :END:
[2022-03-30 Wed 07:23]

* How staticaly linked program run in Linux                  :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2022-03-30 Wed 08:04]--[2022-03-30 Wed 08:04] =>  0:00
  :END:
[2022-03-30 Wed 08:04]
[[https://eli.thegreenplace.net/2012/08/13/how-statically-linked-programs-run-on-linux/ ][https://eli.thegreenplace.net/2012/08/13/how-statically-linked-programs-run-on-linux/]]
* The Linux Schedular: A decade of wasted core               :Technical:NOTE:
  :LOGBOOK:
  CLOCK: [2022-03-30 Wed 08:30]--[2022-03-30 Wed 08:32] =>  0:02
  :END:
[2022-03-30 Wed 08:30]
[[https://blog.acolyer.org/2016/04/26/the-linux-scheduler-a-decade-of-wasted-cores/][https://blog.acolyer.org/2016/04/26/the-linux-scheduler-a-decade-of-wasted-cores/]]

* Linux boot parameters to boot fast                         :Technical:NOTE:
  noibrs noibpb nopti nospectre_v2 nospectre_v1 l1tf=off nospec_store_bypass_disable
  no_stf_barrier mds=off tsx=on tsx_async_abort=off mitigations=off
* VimScript Pseudo Variables                                 :Technical:NOTE:
Prefix	           Meaning
& varname	A Vim option (local option if defined, otherwise global)
&l: varname	A local Vim option
&g: varname     A global Vim option
@ varname	A Vim register
$ varname	An environment variable
  :LOGBOOK:
  CLOCK: [2022-04-01 Fri 13:34]--[2022-04-01 Fri 13:35] =>  0:01
  :END:
[2022-04-01 Fri 13:34]
[[file:~/.emacs.d/OrgFiles/journal.org::*2021][2021]]
* VimScript Variable Scoping                                 :Technical:NOTE:
Prefix	        Meaning
g: varname	The variable is global
s: varname	The variable is local to the current script file
w: varname	The variable is local to the current editor window
t: varname	The variable is local to the current editor tab
b: varname	The variable is local to the current editor buffer
l: varname	The variable is local to the current function
a: varname	The variable is a parameter of the current function
v: varname	The variable is one that Vim predefines
  :LOGBOOK:
  CLOCK: [2022-04-01 Fri 13:41]--[2022-04-01 Fri 13:42] =>  0:01
  :END:
[2022-04-01 Fri 13:41]
[[file:~/.emacs.d/OrgFiles/journal.org::*2021][2021]]
* Linux kernel debugging with GDB                            :Technical:NOTE:
The current task is saved in per-cpu space for x86-64 and is accessed through
the gs register at current_task offset as
#+begin_src C
        mov %gs:0xd440,%rdx

	(gdb) p/x &current_task $63 = 0xd440

	(gdb) p/x __per_cpu_offset[0] $64 = 0xffff88001fc00000

        (gdb) x/gx 0xffff88001fc00000+0xd440 0xffff88001fc0d440: 0xffff88001dea6a00

	(gdb) p/d ((struct task_struct*)0xffff88001dea6a00)->pid $67 = 243

	(gdb) p/x((struct task_struct*)0xffff88001dea6a00)->mm $69 = 0xffff88001d1bc800

	(gdb) p/x((struct task_struct*)0xffff88001dea6a00)->active_mm $70 = 0xffff88001d1bc800
(gdb) p/x __per_cpu_offset[2] $73 = 0xffff88001fd00000

	(gdb) x/gx0xffff88001fd00000+0xd440 0xffff88001fd0d440: 0xffff88001f240000

	(gdb) p/x((struct task_struct*)0xffff88001f240000)->pid $74 = 0x1

	(gdb) lx-ps
#+end_src

Ref :

[[https://elinux.org/Debugging_The_Linux_Kernel_Using_Gdb][Linux Kernel Debug With GDB]]
[[https://www.kernel.org/doc/html/v4.13/dev-tools/gdb-kernel-debugging.html][Kernel Documentation For Debugging With GDB]]
[[https://pnx9.github.io/thehive/Debugging-Linux-Kernel.html][Debugging The Linux Kernel with Qemu and GDB]]
[[https://blogs.oracle.com/linux/post/live-kernel-debugging-1][Oracle Blog For Linux Kernel Debugging -Part-I]]
[[https://blogs.oracle.com/linux/post/live-kernel-debugging-2][Oracle Blog Linux Kernel Debugging Part -II]]
[[https://blogs.oracle.com/linux/post/live-kernel-debugging-3][Oracle Blog Linux Kernel Debugging Part -III]]
[[https://www.linuxjournal.com/content/linux-kernel-testing-and-debugging][Linux Journal Paper on Linux Debugging And Testing]]

Printk related stuff :

KERN_EMERG (0) : Emergency condition such as system is hang.
KERN_ALERT (1) : Problem that requires immediate attention
KERN_CRIT (2) : Critical condition
KERN_ERR (3) : Error
KERN_WARNING (4) : Warning
KERN_NOTICE (5) : Normal
KERN_INFO (6) : Informational message
KERN_DEBUG (7) : Debugging message
KERN_DEFAULT (d) : Default kernel loglevel
KERN_CONT : Continued line of printout

addr2line translates addresses and returns human friendly details such as file
name and line number

addr2line -f -e vmlinux 0xffffffff85077934

KGDB patch enables gdb to debug kernel remotely with serial cable. To enable
KGDB on your kernel, enable below configuration

CONFIG_FRAME_POINTER=y
CONFIG_KGDB=y
CONFIG_KGDB_SERIAL_CONSOLE=y
CONFIG_KGDB_KDB=y
CONFIG_KDB_KEYBOARD=y

kmemcheck :

kmemcheck is a dynamic checking tool that detects and warns about some
uses of uninitialized memory. It serves the same function as Valgrind's memcheck
which is a userspace memory checker, whereas kmemcheck checks kernel
memory. CONFIG_KMEMCHECK kernel configuration option enables the kmemcheck
debugging feature. Please read the Documentation/kmemcheck.txt for information
on how to configure and use this feature, and how to interpret the reported
results.

kmemleak :

kmemleak can be used to detect possible kernel memory leaks
in a way similar to a tracing garbage collector. The difference between the
tracing garbage collector and kmemleak is that the latter doesn't free orphan
objects, instead it reports them in /sys/kernel/debug/kmemleak. A similar method
of reporting and not freeing is used by the Valgrind's memcheck --leak-check to
detect memory leaks in user-space applications. CONFIG_DEBUG_KMEMLEAK kernel
configuration option enables the kmemleak debugging feature. Please read the
Documentation/kmemleak.txt for information on how to configure and use this
feature, and how to interpret the reported results.


Once CONFIG_DYNAMIC_DEBUG is enabled in the kernel, dynamic debug feature enables
a fine grain enable/disable of debug
messages. /sys/kernel/debug/dynamic_debug/control is used to specify which pr_*
messages are enabled.

Enable dynamic debug feature in a module to persist across
reboots:

module.dyndbg="+plmft" as a module option as a kernel boot parameter.

Example:

echo 'file suspend.c line 340 +p' > /sys/kernel/debug/dynamic_debug/control


:LOGBOOK:
CLOCK: [2022-04-10 Sun 17:35]--[2022-04-10 Sun 17:38] =>  0:03
:END:
[2022-04-10 Sun 17:35]
[[file:~/.emacs.d/OrgFiles/task.org][file:~/.emacs.d/OrgFiles/task.org]]
* How to extract and disassemble Linux Kernel Image          :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-04-11 Mon 15:57]--[2022-04-11 Mon 15:57] =>  0:00
:END:
[2022-04-11 Mon 15:57]
[[https://blog.packagecloud.io/how-to-extract-and-disassmble-a-linux-kernel-image-vmlinuz/ ][https://blog.packagecloud.io/how-to-extract-and-disassmble-a-linux-kernel-image-vmlinuz/]]
* Vim Advance Stuff                                          :Technical:NOTE:

# Change sentence and put you in insert mode

c)

# Delete chanracter before cursor
X

# Delete current and previous line
d-

# CTRL-W is super useful

 CTRL-W o --> kill the other buffer after saving it

 CTRL-W x  --> Exchange Buffer position

 CTRL-W | --> Make One particular buffer big

 CTRL-W = --> Make all the buffer EQUAL size

 CTRL-Y in Insert mode does a "vertical copy." That is, it copies the character in the same column from the line immediately above the cursor. For example, a CTRL-Y in the following situation would insert an "m" at the cursor:

Glib jocks quiz nymph to vex dwarf
Glib jocks quiz ny_

# Searching from the command line

 CTRL+g and CTRL+t to go through every match without leaving COMMAND-LINE mode.

# Copying from buffer to the command line

 CTRL+r CTRL+f - Copy the filename under the buffer’s cursor.

 CTRL+r CTRL+w - Copy the word under the buffer’s cursor.

 CTRL+r CTRL+a - Copy the WORD under the buffer’s cursor.

 CTRL+r CTRL+l - Copy the line under the buffer’s cursor.

# Fill in quickfix window
 :cex [] - Empty the current quickfix list.

 :cex system("<cmd>") - Populate your quickfix list with any shell command

# How to evaluate vim script expression or shell command output from insert mode

 C-r = put you to command mode then you need to pass what you want to evaluate and the resulting content gets pasted on the buffer.

# Inserting and Deleting

 CTRL+a - Insert the last content inserted.

 CTRL+@ - Insert the last content inserted and quit INSERT mode.

 CTRL+h - Delete the character before the cursor.

 CTRL+w - Delete the word under the cursor.

 CTRL+u - Delete everything before the cursor.

 CTRL+t - Add one indentation.

 CTRL+d - Delete one indentation.

# Completion in Insert mode

 _Vim needs to be compiled with the +insert_expand feature for these keystrokes to work._

CTRL+x CTRL+y - Scroll up

 CTRL+x CTRL+e - Scroll down

 CTRL+x CTRL-l - Complete a whole line from the content of one of your buffer.

 CTRL+x CTRL-f - Complete the filepath under the cursor. It expands environment variables if it contains a filepath too.

 CTRL+x s - Complete with spelling suggestions.

 CTRL+x CTRL+v - Complete with the command line history.

 CTRL+x CTRL+i - Complete with the keywords in the current and included files. These files are in the option path.

 # Restrict viminfo information in viminfo file
 :set viminfo=!,'100,<50,s100.
 that means , set the global variable max 100 files ,a maximum of 50 lines per register and 100kib for for each item.
 :oldfiles or :ol - Display all marked files stored in the viminfo file.
 :rviminfo or :rv - Read the viminfo file.
 :wviminfo or :wv - Write the viminfo file.

# Diagraph

- CTRL+K ->: →
- CTRL+K TM: ™
- CTRL+K Co: ©
- CTRL+K Rg: ®
- CTRL+K Eu: €
- CTRL+K +-: ±

# Checking existing functions

 :function or :fu - List all declared function.
 function - Keyword to declare a function. You can add a bang (function!) to overwrite a previously declared function with the same name.

# You can also use these useful keystrokes in NORMAL mode:

 & - Repeat the last substitute, without its range and its flags.
 g& - Repeat the last substitute with the same flags but without the same range (it’s global), and replace its pattern with the last search pattern.

# Vim Special strings
 % - Relative path of the current file.

 <cword> - Word under the cursor.

 <cWORD> - WORD under the cursor.

 <cfile> - Filepath under the cursor.

 <afile> - File open in the buffer when executing autocommands.

 <sfile> - Filename of sourced file when used with command :source.

**You can also use the following with %:

 :p - Output the absolute path instead of the relative one. Also expand the tilda ~ to the home directory.
 :. - Make the file path relative to the working directory.
 :~ - Make the file path relative to the home directory (if possible).
 :h - Keep the head of the file path (remove the last element).
 :t - Keep the tail of the file path (remove everything except the last element).
 :r - Keep the root of the file name (remove its extension).
 :e - Remove everything except the extension of the filename.
 :s?pat?sub? - Substitute the first occurrence of “pat” with “sub”.
 :gs?pat?sub? - Substitute all occurrences of “pat” with “sub”.

expand(<special_string>) to expand these placeholders

:echom expand("%")
:echom expand("%:p")
:echom expand("<cword>")

# Important options
 NORC - Don’t load any vimrc but load your plugins.

 NONE - Don’t load any vimrc nor plugins.

# Special arguments

 <silent> - Doesn’t output the mapping in the Vim command-line. If you want to also drop the output of the command linked to the mapping, add the command :silent.

 <buffer> - The mapping’s scope is reduced to the current buffer only. These mappings have the priority on the global ones.

 <expr> - The mapping executes a Vimscript expression instead of a Vim command.

 <unique> - The mapping fails if it already exists. It’s useful if you don’t want to override any mapping defined previously.

 <Cmd> - The mapping can run a command without quitting the current mode you’re in.

# Difference between localist window and quickfix

 * Local list is local and that means you can have local window per buffer

 * quickfix window is global, that means you can only have one quickfix window for entire session

 *  A location list is similar to a quickfix list, except that the first is local to a window and the second is global to your Vim instance. In other words, you can have multiple location lists available at the same time (one per window open), but you can only have access to one quickfix list

# Typers of Registers

 The unnamed register (") - Contain the last deleted, changed, or yanked content, even if one register was specified.
 The numbered registers (from 0 to 9)
 0 contains the content of the last yank.
 1 to 9 is a stack containing the content you’ve deleted or changed.
 Each time you delete or change some content, it will be added to the register 1.
 The previous content of the register 1 will be assigned to register 2, the previoius content of 2 to 3…
 When something is added to the register 1, the content of the register 9 is lost.
 None of these registers are written if you’ve specified one before with the keystroke ".
 The small delete register (-) Contains any deleted or changed content smaller than one line.

 nt’s not written if you specified a register with ".
 The named registers (range from a to z)
 nim will never write to them if you don’t specify them with the keystroke ".
 You can use the uppercase name of each register to append to it (instead of overwriting it).
 The read only registers (., % and :)
 . contains the last inserted text.
 % contains the name of the current file.
 : contains the most recent command line executed.
 The alternate buffer register (#) - Contain the alternate buffer for the current window.
 The expression register (=) - Store the result of an expression. More about this register below.
 The selection registers (+ and *)
 + is synchronized with the system clipboard.
 * is synchronized with the selection clipboard (only on *nix systems).
 The black hole register (_) - Everything written in there will disappear forever.
 The last search pattern register (/) - This register contains your last search.
 CTRL+R % in INSERT mode, you’ll put the content of the register % in your current buffer.

# Pattern replacement

 :s/pattern/replacement/ - Substitute the first occurrence of pattern on the current line with replacement.
 :s#pattern#replacement# - Equivalent substitution to the one just above. Handy if you have some URLs in your pattern or your replacement.
 :s/pattern/ - delete the first occurrence of pattern on the current line.
 :s/pattern/replacement/g - Substitute every occurrence of pattern on the current line.
 You can also add a range as prefix and a count as suffix:

 :%s/pattern/replacement/ - Substitute every first occurrence of pattern on each line of the current buffer.
 :%s/pattern/replacement/g - Substitute every occurrence of pattern on each line of the current buffer.
 :1,10s/pattern/replacement/ - Substitute every first occurrence of pattern on the first ten lines of the current buffer.
 :s/pattern/replacement/ 10 - Substitute every first occurrence of pattern for the current line and the 10 next lines.
 :1,10s/pattern/replacement/ 5 - Substitute every first occurrence of pattern on the first ten lines and on the five lines below the last line of the range.
 :s g 10 - Repeat the last substitution without its flag, and add a new flag g. It will affect the 10 lines after the last line of the last substitute command.
 :&& - Repeat the last substitute with its flags.
 :~ - Repeat the last substitute command with the same replacement, but with the last used search pattern.

# Redirections

 :redir > <file> - Write every command’s output to the file <file>.
 Use :redir! (with a bang !) to overwrite the file.
 Use >> instead of > to append to the file.
 :redir @<reg> - Write every command’s output to the register <reg>.
 :redir @<reg>>> - Append every command’s output to the register <reg>.
 :redir => <var> - Write every command’s output to the variable <var>.
 :redir END - End the redirection.
 :redir @A  --> Appending

# Filtering

 :filter /content/ buffers - Only output the buffers with part of the filepath matching content.
 :filter /archives/ oldfiles - Only output the marked files with part of the filepath matching archives.


# Vim Pseudo Variables

 Prefix	           Meaning

 & varname	A Vim option (local option if defined, otherwise global)

 &l: varname	A local Vim option

 &g: varname	A global Vim option

 @ varname	A Vim register

 $ varname	An environment variable

# Vim Variable scope

 Prefix	          Meaning

 g: varname	The variable is global

 s: varname	The variable is local to the current script file

 w: varname	The variable is local to the current editor window

 t: varname	The variable is local to the current editor tab

 b: varname	The variable is local to the current editor buffer

 l: varname	The variable is local to the current function

 a: varname	The variable is a parameter of the current function

 v: varname	The variable is one that Vim predefines


# Remote file access read/write

 e scp:user@remoteip/path/to/file   from inside vim session

 vim ftp://user@remotesystem/path/to/file     from command line

  vim scp://sk@192.168.225.22//home/sk/Documents/info.txt  for absolute path user //

:LOGBOOK:
CLOCK: [2022-04-11 Mon 16:27]--[2022-04-11 Mon 16:30] =>  0:03
:END:
[2022-04-11 Mon 16:27]
* Ftrace internals                                           :Technical:NOTE:
Ftrace introduces a new form of printk() called trace_printk(). It can be used
just like printk(), and can also be used in any context (interrupt code, NMI
code, and scheduler code). What is nice about trace_printk() is that it does not
output to the console. Instead it writes to the Ftrace ring buffer and can be
read via the trace file.

"+" that is there is an annotation marker. When the duration is greater than 10
microseconds, a "+" is shown. If the duration is greater than 100 microseconds a
"!" will be displayed.

Congi parameters to be enables :

CONFIG_FUNCTION_TRACER
    CONFIG_FUNCTION_GRAPH_TRACER
    CONFIG_STACK_TRACER
    CONFIG_DYNAMIC_FTRACE

 annotation of "{" to start a function and "}" at the end. Leaf functions, which
 do not call other functions, simply end with a ";".

 disable the Ftrace ring buffer from recording --> echo 0 > tracing_on

 Enable trace buffe --> echo 1 > tracing_on

 it is very important that you have a space between the number and the greater
 than sign ">"

 To help synchronize between the actions in user space and kernel space, the
 trace_marker file was created

  tracing_on() and tracing_off()

  Having Ftrace configured and enabling ftrace_dump_on_oops in the kernel boot
  parameters, or by echoing a "1" into /proc/sys/kernel/ftrace_dump_on_oops, will
  enable Ftrace to dump to the console the entire trace buffer in ASCII format
  on oops or panic. Having the console output to a serial log makes debugging
  crashes much easier. You can now trace back the events that led up to the
  crash.

  To enable the stack tracer, echo 1 into
  /proc/sys/kernel/stack_tracer_enabled. To see the max stack size during boot up,
  add "stacktrace" to the kernel boot parameters.


:LOGBOOK:
CLOCK: [2022-05-18 Wed 15:52]--[2022-05-18 Wed 17:36] =>  1:44
:END:
[2022-05-18 Wed 15:52]

* How to get youtube channel content                         :Technical:NOTE:
  use youtube-dl to dump jason  output of playlist
  and pipe into jq  to extract title and url to save as org link.

#+NAME: noweb-block
#+begin_src sh
youtube-dl -j --flat-playlist "${url}" | jq -rj \
'"[[https://youtu.be/\(.id)][\(.title)]]\n"'

#+end_src

#+NAME:
#+HEADER:  :var url=""
#+BEGIN_SRC sh  :async t  :results output list :noweb yes :wrap
<<noweb-block>>
#+end_src
* Emacs for writing                                          :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-06-02 Thu 06:27]--[2022-06-02 Thu 06:27] =>  0:00

:END:
[2022-06-02 Thu 06:27]
[[https://jacmoes.wordpress.com/2019/09/24/creative-writing-with-emacs/][https://jacmoes.wordpress.com/2019/09/24/creative-writing-with-emacs/]]
* How pipes work in Linux                                    :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-06-03 Fri 14:00]--[2022-06-03 Fri 14:00] =>  0:00
:END:
[2022-06-03 Fri 14:00]
[[https://mazzo.li/posts/fast-pipes.html][https://mazzo.li/posts/fast-pipes.html]]
* Researcher workout with Emacs                              :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-06-10 Fri 09:22]--[2022-06-10 Fri 09:26] =>  0:04
:END:
[2022-06-10 Fri 09:22]
[[https://www.labri.fr/perso/nrougier/GTD/index.html][https://www.labri.fr/perso/nrougier/GTD/index.html]]
* GCC Internals                                              :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-06-14 Tue 14:28]--[2022-06-14 Tue 14:29] =>  0:01
:END:
[2022-06-14 Tue 14:28]
[[https://gcc-newbies-guide.readthedocs.io/en/latest/diving-into-gcc-internals.html][https://gcc-newbies-guide.readthedocs.io/en/latest/diving-into-gcc-internals.html]]
:LOGBOOK:
* Operating System Development Related WebSite               :Technical:NOTE:
CLOCK: [2022-07-13 Wed 11:56]--[2022-07-13 Wed 12:14] =>  0:18
:END:
[2022-07-13 Wed 11:56]
[[https://wiki.osdev.org/Expanded_Main_Page][https://wiki.osdev.org/Expanded_Main_Page]]
* Spectre Variant 2 inner working and CPU features           :Technical:NOTE:
This is all about Spectre variant 2, where the CPU can be tricked into
mispredicting the target of an indirect branch. And I'm specifically
looking at what we can do on *current* hardware, where we're limited to
the hacks they can manage to add in the microcode.

The new microcode from Intel and AMD adds three new features.

One new feature (IBPB) is a complete barrier for branch prediction.
After frobbing this, no branch targets learned earlier are going to be
used. It's kind of expensive (order of magnitude ~4000 cycles).

The second (STIBP) protects a hyperthread sibling from following branch
predictions which were learned on another sibling. You *might* want
this when running unrelated processes in userspace, for example. Or
different VM guests running on HT siblings.

The third feature (IBRS) is more complicated. It's designed to be
set when you enter a more privileged execution mode (i.e. the kernel).
It prevents branch targets learned in a less-privileged execution mode,
BEFORE IT WAS MOST RECENTLY SET, from taking effect. But it's not just
a 'set-and-forget' feature, it also has barrier-like semantics and
needs to be set on *each* entry into the kernel (from userspace or a VM
guest). It's *also* expensive. And a vile hack, but for a while it was
the only option we had.

Even with IBRS, the CPU cannot tell the difference between different
userspace processes, and between different VM guests. So in addition to
IBRS to protect the kernel, we need the full IBPB barrier on context
switch and vmexit. And maybe STIBP while they're running.

Then there's Skylake, and that generation of CPU cores. For complicated
reasons they actually end up being vulnerable not just on indirect
branches, but also on a 'ret' in some circumstances (such as 16+ CALLs
in a deep chain).
:LOGBOOK:
CLOCK: [2022-07-19 Tue 03:17]--[2022-07-19 Tue 03:24] =>  0:07
:END:
[2022-07-19 Tue 03:17]
* Linux Kernel Debugging Techniques                          :Technical:NOTE:

1) printk() works even the lock is held.

2) printk() accepts /log levels/

3) There are 8 different levels of log

4) Default log level is WARNING

5) Examples:
• Printk(KERN_WARNING “This is a warning message!\n”); or you can use
• Printk(“<4> This is a warning message!\n);
• Printk(KERN_ALERT “DEBUG: %s %d \n”, __SOMEVAR1__, __SOMEVAR2)

6) Loglevels are defined in <linux/kernel.h>

7) Kernel messages are stored in LOG-BUF-LEN ,compile time configurator Also
   CONFIG-LOG-BUF-SHIFT

8) *Klog* is a daemon that retrieves the kernel messages from log buffer

9) *Syslogd* provides two utilities to see logs /dmesg/ and "/var/log//messages" file

10) Limitation of *GDB*  :

    a) user space debugger peek into the address of running kernel
    b) you can examine the content of the kernel space
    c) can not set breakpoint
    d) can not step through the kernel code

11) Debugging with QEMU

12) qemu-kvm –hda yourimage.qcow –m 512 redir tcp::[your_assigned_port]:22

    then on another terminal run

    ssh -p[your-port] -l [username] localhost

13) s and S option freezes the CPU in the beginning of boot process ,to resume
    press c

14) copy the /vmlinux/ binary from qemu booted system to host machine

15) Start gdb like this :

    $gdb vmlinux

16) Try setting breakpoint

17) /add-symbol-file/ command in gdb allow to load a symbol table related to
    module

18) ADDR should be the agrument of the previous command , which is starting
    address of the file

19) /proc filesystem has module path

:LOGBOOK:
CLOCK: [2022-08-26 Fri 09:49]--[2022-08-26 Fri 10:16] =>  0:27
:END:
[2022-08-26 Fri 09:49]
* Emacs Keyboard Shortcuts                                   :Technical:NOTE:
https://github.com/VernonGrant/emacs-keyboard-shortcuts

:LOGBOOK:
CLOCK: [2022-09-03 Sat 08:14]--[2022-09-03 Sat 08:16] =>  0:02
:END:
[2022-09-03 Sat 08:14]
* Make Variables                                             :Technical:NOTE:


    $@: the target filename.
    $*: the target filename without the file extension.
    $<: the first prerequisite filename.
    $^: the filenames of all the prerequisites, separated by spaces, discard duplicates.
    $+: similar to $^, but includes duplicates.
    $?: the names of all prerequisites that are newer than the target, separated by spaces.
    $@ mute the command itself
    $(info the message need to be printed)

:LOGBOOK:
CLOCK: [2022-09-24 Sat 16:07]--[2022-09-24 Sat 16:08] =>  0:01
:END:
[2022-09-24 Sat 16:07]
[[file:~/.emacs.d/OrgFiles/task.org:::PROPERTIES:]]
* Lex and Yacc notes                                         :Technical:NOTE:

Extracted from : https://begriffs.com/posts/2021-11-28-practical-parsing.html

References :

1) https://www.cs.virginia.edu/~cr4bd/flex-manual/

2) https://www.csee.umbc.edu/~chang/cs431/Lex_Manual.pdf

3) https://www.man7.org/linux/man-pages/man1/lex.1p.html

4) https://www.gnu.org/software/bison/manual/

5) https://en.wikipedia.org/wiki/Yacc

6) https://www.man7.org/linux/man-pages/man1/yacc.1p.html

7) https://edoras.sdsu.edu/doc/yacc-intro.pdf

8) [[file:/data/pdf_docs/Steve_Johnson_Yacc_papaer.pdf][Steve Johnson's Paper On Yacc  [pdf]]]

9) [[https://www.cs.utexas.edu/users/novak/yaccpaper.htm][Stephen C. Johnson's paper on Yacc [website]​]]

a) A lex input file is composed of three possible sections: definitions, rules,
and helper functions.The sections are delimited by %%. Lex transforms its input
file into a plain C file that can be built using an ordinary C compiler.

b) Definition : it runs user-supplied C code blocks for regular expression
matches. It reads a list of regexes and constructs a giant state machine which
attempts to match them all “simultaneously.”

c) Lex files have .l (dot el) extension with C syntax

d) To build lex , and turn the input file into an intermidiate C file,you need to run like this :
        lex -t lexfile.l > lexfile.c

    then compile it like this :

    gcc -o whatevertheoutputfilename lexfile.c -ll

e) Lex creates a function called yylex(), and inserts the code blocks verbatim
into a switch statement. When using lex with a parser, the parser will call
yylex() to retrieve tokens, named by integers.

f) EOF in lex is 0, which is different from the EOF macro in the C standard
library

g) Lex and yacc are designed to create standalone programs, with user-defined
code blocks stuck inside. When classic lex and yacc work together, they use a
bunch of global variables.

h) Flex and Bison, on the other hand, can generate thread-safe functions with
uniquely prefixed names that can be safely linked into larger programs

i) a parser can act on the tokens using recursive rules. Yacc/byacc/bison are
LALR (look-ahead left recursive) parsers, and Bison supports more powerful modes
if desired.

j) LR parsers build bottom-up toward a goal, shifting tokens onto a stack and
combining (“reducing”) them according to rules

k) Initially there’s no lookahead token, so yacc calls yylex() to get one.

l) yacc is a powerful tool to transform a grammar into a state machine,

m) While matching tokens, parsers typically build a user-defined value in memory
to represent features of the input.

n) lex generates a yylex() function, and yacc generates yyparse() that calls
yylex() repeatedly to get new token identifiers. Lex actions copy semantic
values to yylval which Yacc copies into $-variables accessible in parser rule
actions.

* Linux Daemon Process Explained                             :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-10-29 Sat 07:52]--[2022-10-29 Sat 07:53] =>  0:01
:END:
[2022-10-29 Sat 07:52]
[[https://man7.org/linux/man-pages/man7/daemon.7.html][https://man7.org/linux/man-pages/man7/daemon.7.html]]

* Linux kernel development setting code                      :Technical:NOTE:
:LOGBOOK:
:END:
(defun c-lineup-arglist-tabs-only (ignored)
  "Line up argument lists by tabs, not spaces"
  (let* ((anchor (c-langelem-pos c-syntactic-element))
		 (column (c-langelem-2nd-pos c-syntactic-element))
		 (offset (- (1+ column) anchor))
		 (steps (floor offset c-basic-offset)))
    (* (max steps 1)
       c-basic-offset)))

(c-add-style "linux-tabs-only"
             '("linux" (c-offsets-alist
                        (arglist-cont-nonempty
                         c-lineup-gcc-asm-reg
                         c-lineup-arglist-tabs-only))))

(defun m/kernel-source-hook ()
  (let ((filename (buffer-file-name)))
    ;; Enable kernel mode for the appropriate files
    (if (and filename
               (or (string-match (expand-file-name "//home/bhaskar/git-linux/linux/h/.*/kernel")
                                 filename)
                   (string-match "//home/bhaskar//git-linux/linux//"
                                 filename)
                   (locate-dominating-file filename "Kbuild")
                   (locate-dominating-file filename "Kconfig")
                   (save-excursion (goto-char 0)
                                   (search-forward-regexp "^#include <linux/\\(module\\|kernel\\)\\.h>$" nil t))))
        (progn
          (setq indent-tabs-mode t)
          (setq tab-width 8)
          (setq c-basic-offset 8)
          (message "Setting up indentation for the linux kernel")
          (c-set-style "linux"))
      (c-set-style "k&r"))))

(add-hook 'c-mode-hook 'm/kernel-source-hook)
* LIbrary linking breakage and fixing                        :Technical:NOTE:
:LOGBOOK:
CLOCK: [2022-12-07 Wed 11:25]--[2022-12-07 Wed 11:29] =>  0:04
:END:
[2022-12-07 Wed 11:25] [[https://rosshemsley.co.uk/posts/linking/][https://rosshemsley.co.uk/posts/linking]]

Points:

Dynamic or shared libraries are loaded up by your program at runtime. They
contain lookup tables that map symbols to shared executable code. If you give
someone a binary that links a dynamic library that they don’t already have, the
OS will complain about missing libraries when they try to run it.

Dynamic or “shared” libraries have names that start with lib and finish with
.so. Unless you’re on a Mac, where they end with .dylib.1

Dynamic libraries themselves can link other dynamic libraries. These are known
as transitive dependencies. All dependencies will need to be found to
successfully run your binary.

If you want to move a binary from one machine (where it was compiled) to
another, you’ll almost certainly find that at least some of the shared libraries
needed by your binary are no longer found. This is usually the first sign of
trouble…

Linux knows how to find libraries because it has a list of known locations for
shared libraries in /etc/ld.so.conf. Each time you run ldconfig, the OS updates
its cache of known libraries by going through directories in this file and
reading the libraries it finds. OS X works differently… see dyld and friends.

Use ldd (linux) or otool -L (OS X) to query your binary for the missing
libraries. Beware that it is not safe to do this on a binary you suspect may be
malicious 😞.

You can safely copy dynamic libraries from one machine to another. As long as
the environments are similar enough…2 . In a perfect world (on linux), you could
just copy the library you want to use into /usr/local/lib (the recommended place
for unstable libraries) and then run ldconfig to make your OS reload its library
cache.

Of course, on OS X things work totally differently. Dynamic libraries have an
install name which contains the absolute path. This path is baked into your
binary at compile time. You can use install_name_tool to change it. Good luck!

On linux, Adding libraries to /usr/local/lib makes them visible to everything,
so you may want to copy your library somewhere else so that only your binary
knows how to find it. One way to do this is using rpath…

You can set the rpath attribute of your binary to contain a directory hint for
your OS to look in for libraries. This hint can be relative to your binary. This
is especially useful if you always ship libraries in a relative directory to
your binary. You can use @origin as a placeholder for the path of the binary
itself, so an rpath of @origin/lib causes the OS to always look in <path to your
binary>/lib for shared libraries at runtime. This can be used on both OS X and
linux, and is one of the most useful tools to actually getting things working in
practice.

If your OS isn’t finding a dynamic library that you know exists, you can try
helping your OS by setting the environment variable LD_LIBRARY_PATH to the
directory containing it - your OS will look there first before default system
paths. Beware, this is considered bad practice, but it might unblock you at a
pinch. OS X has DYLD_LIBRARY_PATH, which is similar, and also
DYLD_FALLBACK_LIBRARY_PATH, which is similar, but different (sorry).

Dynamic libraries also have a thing called a soname, which is the name of the
library, plus version information. You have seen this if you’ve seen
libfoo.so.3.1 or similar. This allows us to use different versions of the same
library on the same OS, and to make non backwards-compatible changes to
libraries. The soname is also baked into the library itself.

Often, your OS will have multiple symlinks to a single library in the same
directory, just with different paths containing version information,
e.g. libfoo.so.3, libfoo.so.3.1. This is to allow programs to find compatible
libraries with slightly different versions. Everything starts to get rather
messy here… if you really need to get into the weeds, this article will
help. You probably only need to understand this if you are distributing
libraries to users and need to support compatibility across versions.

Of course, even if your binary only depends on a single symbol in a dynamic
library, it must still link that library. Now consider that the dependency
itself may also link other unused transitive dependencies. Accidentally
“catching a dependency” can cause your list of shared library dependencies to
grow out of control, so that your simple hello world binary ends up depending on
hundreds of megabytes of totally unused shared libraries 😞.

One solution to avoiding “dependency explosions” is to statically link symbols
directly into your binary, so let’s start to look at static linking!

Static libraries (.a files) contain symbol lookup table, similarly to dynamic
libraries. However, they are much more dumb and also a total PITA to use
correctly.

If you compile your binary and link in only static dependencies, you will end up
with a static binary. This binary will not need to load any dependencies at
runtime and thus much easier to share with others!

People On The Internet will recommend that you do not not distribute static
binaries, because it makes it hard to patch security flaws. With dynamic
libraries, you just have to patch a single library e.g. libssl.so, instead of
re-compiling everything on your machine that may have linked the broken library
without your knowledge (i.e. everything).

People who build production systems at companies recommend static libraries
because it’s wayyyy the hell easier to just deploy a single binary with zero
dependencies that can basically run anywhere. No one cares about how big
binaries are these days anyway.

Still more people on the internet remind you that only one copy of a dynamic
library is loaded into memory by the OS even when it is used by multiple
processes, saving on memory pressure.

The static library people remind you that modern computers have plenty of memory
and library size is hardly the thing killing us right now.

The OS X people point out that OS X strongly discourages the use of statically
linked binaries.

Static libraries can’t declare any kinds of library dependencies. This means it
is your responsibility to ensure all symbols are all baked correctly into your
binary at link time - otherwise your linker will fail. This can make linking
static libraries painfully error-prone.

If you get symbol not found errors but literally swear that you linked every
damn thing, you probably linked a static library, and forgot a transitive
dependency that is needed by it. This pretty much sucks as it’s basically
impossible to figure out where that library comes from. Try having a guess by
looking at the error messages. Or something?

Oh, and you must ensure that you link your static libraries in the correct
order, otherwise you can still get symbol not found errors.

If you are starting to think it might be hard to keep track of static libraries,
you are following along correctly. There are tools that can help you here, such
as pkgconfig, CMake, autotools… or bazel. It’s quite easy to get going, and
achieve deterministic platform-independent static builds with no dynamic
dependencies… Said no one ever 😓.

One classic way to screw up, is to compile a static library without using the
-fPIC flag (for “position independent code”). If you do not do this, you will be
able to use the static library in a binary, but you will not be able to link it
into a dynamic library. This is especially frustrating if you were provided with
a static library that was compiled without this flag and you can’t easily
recompile it.

Beware that -fpic is not the same as -fPIC. Apparently, -fPIC always works but
may result in a few nanoseconds of slowdown, or something. Probably you should
use -fPIC and try not to think about it too much.

Your compiler toolchain (e.g. CMake) usually has a one-liner way to link a bunch
of static libraries into a single dynamic library with no dependencies of its
own. However, should you want to link a bunch of static libraries into another
static library… well I’ve never successfully found a reliable way to do this
😞. Why do this you may ask? Mostly for cffi - when I want to build a single
static library from C++ and then link it into e.g. a go binary.

Beware that your compiler/linker is not smart! Just because the header files
declare a function and your linker manages to find symbols for it in your
library, doesn’t mean that the function is remotely the same. You will discover
this when you get undefined behavior at runtime.

Oh, and if the library you are linking was compiled with a #define switch set,
but when you include the library’s headers, you do not set the define to the
same value, welcome again to runtime undefined behavior land! This is the same
problem as the one above, where the symbols end up being incompatible.

If you are trying to ship C++, another thing that can bite you is that the C++
standard library uses dynamic linking. This means that even the most basic hello
world program cannot be distributed to others unless they have a compatible
version of libstdc++. Very often you’ll end up compiling with a shiny new
version of this library, only to find that your target is using an older,
incompatible version.

One way to get around libstdc++ problems is to statically link it into your
binary. However, if you create a static library that statically links libstdc++,
and your library uses C++ types in its public interface… welcome again to
undefined behavior land ☠️.

Another piece of classic advice is to statically link everything in your binary
apart from core system libraries, such as glibc - which is basically a thin
wrapper around syscalls. A practical goal I usually aim for is to statically
link everything apart from libc and (preferably an older version of)
libstdc++. This seems to be the safest approach.

Ultimately, my rule of thumb for building distributed systems is to statically
link everything apart from libc and (an older version of) libstdc++. You can
then put this library / binary into a Debian package, or an extremely
lightweight Docker container that will run virtually anywhere. Setting up the
static linking is a pain, but IMO worth the effort - the main benefits of
dynamic libraries generally do not apply anymore when you are putting the binary
in a container anyway.

Finally, for ultimate peace of mind, use a language that has a less insane build
toolchain than C++. For example, Go builds everything statically by default and
can link in both dynamic or static libraries if needed, using cgo. Rust also
seems to work this way. Static binaries have started becoming fashionable
* When CPU start                                             :Technical:NOTE:
:LOGBOOK:
CLOCK: [2023-01-11 Wed 10:09]--[2023-01-11 Wed 10:09] =>  0:00
:END:
[2023-01-11 Wed 10:09]
[[https://lateblt.tripod.com/bit68.txt][https://lateblt.tripod.com/bit68.txt]]
* Biggest commit in Linux Kernel history                     :Technical:NOTE:
:LOGBOOK:
CLOCK: [2023-01-24 Tue 12:54]--[2023-01-24 Tue 12:55] =>  0:01
:END:
[2023-01-24 Tue 12:54]
[[https://www.destroyallsoftware.com/blog/2017/the-biggest-and-weirdest-commits-in-linux-kernel-git-history][https://www.destroyallsoftware.com/blog/2017/the-biggest-and-weirdest-commits-in-linux-kernel-git-history]]
* Nice Git tuto                                              :Technical:NOTE:
:LOGBOOK:
CLOCK: [2023-01-24 Tue 12:56]--[2023-01-24 Tue 12:56] =>  0:00
:END:
[2023-01-24 Tue 12:56]
[[https://myme.no/posts/2023-01-22-git-commands-you-do-not-need.html][https://myme.no/posts/2023-01-22-git-commands-you-do-not-need.html]]
* C lesser known tricks, quirks and features                 :Technical:NOTE:
:LOGBOOK:
CLOCK: [2023-02-20 Mon 14:36]--[2023-02-20 Mon 14:37] =>  0:01
:END:
[2023-02-20 Mon 14:36]
[[https://blog.joren.ga/less-known-c][https://blog.joren.ga/less-known-c]]
* Clock and Causality Ordering Events in Distributed Systems :Technical:NOTE:
:LOGBOOK:
CLOCK: [2023-04-02 Sun 07:11]--[2023-04-02 Sun 07:12] =>  0:01
:END:
[2023-04-02 Sun 07:11]
[[https://www.exhypothesi.com/clocks-and-causality/][https://www.exhypothesi.com/clocks-and-causality/]]
* How computer memory hiccup every 7.8 microseconds          :Technical:NOTE:
:LOGBOOK:
CLOCK: [2023-04-03 Mon 18:13]--[2023-04-03 Mon 18:15] =>  0:02
:END:
[2023-04-03 Mon 18:13]
[[https://blog.cloudflare.com/every-7-8us-your-computers-memory-has-a-hiccup/][https://blog.cloudflare.com/every-7-8us-your-computers-memory-has-a-hiccup/]]
* Debian unattended update and upgrade                       :Technical:NOTE:
# File to look into for the change /etc/apt/apt.conf.d/20auto-upgrades and
# this /etc/apt/apt.conf.d/50unattended-upgrades

#OR create a new file /etc/apt/apt.conf.d/51myunattended-upgrades

// Enable the update/upgrade script (0=disable)
APT::Periodic::Enable "1";

// Do "apt-get update" automatically every n-days (0=disable)
APT::Periodic::Update-Package-Lists "1";

// Do "apt-get upgrade --download-only" every n-days (0=disable)
APT::Periodic::Download-Upgradeable-Packages "1";

// Do "apt-get autoclean" every n-days (0=disable)
APT::Periodic::AutocleanInterval "7";

// Send report mail to root
//     0:  no report             (or null string)
//     1:  progress report       (actually any string)
//     2:  + command outputs     (remove -qq, remove 2>/dev/null, add -d)
//     3:  + trace on    APT::Periodic::Verbose "2";
APT::Periodic::Unattended-Upgrade "1";

// Automatically upgrade packages from these
Unattended-Upgrade::Origins-Pattern {
      "o=Debian,a=stable";
      "o=Debian,a=stable-updates";
      "origin=Debian,codename=${distro_codename},label=Debian-Security";
};

// You can specify your own packages to NOT automatically upgrade here
Unattended-Upgrade::Package-Blacklist {
};

// Run dpkg --force-confold --configure -a if a unclean dpkg state is detected to true to ensure that updates get installed even when the system got interrupted during a previous run
Unattended-Upgrade::AutoFixInterruptedDpkg "true";

//Perform the upgrade when the machine is running because we wont be shutting our server down often
Unattended-Upgrade::InstallOnShutdown "false";

// Send an email to this address with information about the packages upgraded.
Unattended-Upgrade::Mail "root";

// Always send an e-mail
Unattended-Upgrade::MailOnlyOnError "false";

// Remove all unused dependencies after the upgrade has finished
Unattended-Upgrade::Remove-Unused-Dependencies "true";

// Remove any new unused dependencies after the upgrade has finished
Unattended-Upgrade::Remove-New-Unused-Dependencies "true";

// Automatically reboot WITHOUT CONFIRMATION if the file /var/run/reboot-required is found after the upgrade.
Unattended-Upgrade::Automatic-Reboot "true";

// Automatically reboot even if users are logged in.
Unattended-Upgrade::Automatic-Reboot-WithUsers "true";

:LOGBOOK:
CLOCK: [2023-09-01 Fri 14:49]--[2023-09-01 Fri 14:50] =>  0:01
:END:
[2023-09-01 Fri 14:48]
* Regular Expression in Perl, Pythin and Emacs by John.D.Cook :Technical:NOTE:

Comparing regular expressions in Perl, Python, and Emacs

Regular expressions are very handy, but unfortunately they are implemented at
least slightly differently everywhere you use them. These notes survey the
implementation of regular expressions in Perl, Python, and Emacs. They are not
exhaustive. If I went into every minute detail, I’d have a book rather than a
page of notes.

When I say “Python” here, I mean Python’s re module as implemented at the time
of writing at the end of 2015. There are other options in Python more compatible
with Perl, and rumor has it that these will eventually be merged into the
standard Python distribution.

Also, these notes only concern regular expression syntax, not how regular
expressions are used. See these notes for a comparison of how to use regular
expressions for common tasks like searching and replacing in Perl and Python.

Common features across Emacs, Python, Perl

The most basic regex features are the same in nearly every implementation: wild
character ., quantifiers *, +, and ?, anchors ^ and $, character classes inside
[], and back references \1, \2, \3 etc.

Recent versions of Emacs support the metacharacters \b for word boundaries, and
\B for non-word boundaries, \w for word characters, and \W for non-word
characters.

Generally Emacs supports only the oldest features of regular expressions, but it
does support the relatively recent innovation of non-greedy quantifiers *?, +?,
and ??.  Common features requiring backslashes in Emacs

Alternation is denoted | in Perl and Python, but requires \| in
Emacs. Similarly, grouping parentheses must be escaped in Emacs: \( and
\). Also, denoting how many times a pattern should match inside curly braces
requires extra backslashes in Emacs: \{ and \}.

Another exception to Emacs’ lack of recent features is non-grouping
parentheses. However, these parentheses also require backslashes: \(?: … \).
Features unique to Emacs Syntax classes

Syntax classes begin with \s in Emacs. And as is conventional in regular
expressions, the negation of the class uses the corresponding capital \S.

For example, \s. denotes any punctuation character, and \S. denotes any
non-punctuation character. Emacs uses \s( for opening delimiters and \s) for
closing delimiters, \s< for opening comment characters, \s> for closing comment
characters, and has other syntax classes.

Note that since Emacs uses \s and \S to begin syntax classes, it does not use
these for whitespace and non-whitespace the way Perl and Python do. Whitespace
in Emacs is denoted \s- and non-whitespace \S-.  Character classes

Character classes are similar to syntax classes but begin with \c for positive
and \C for negative. These are similar to Unicode properties \p{} and \P{} in
Perl.

For example, \cg stands for any Greek character and \Cg for any non-Greek
character. Run M-x describe-categories to see more information on character
classes.  Features in Python and Perl, not in Emacs

Perl and Python will let you modify a regular expression with (?aimsx). For
example, (?i) makes an expression case-insensitive. These modifiers have the
same meaning on both languages. Also, both languages let you introduce a comment
in a regular expression with (?# … ).

Perl and Python support positive and negative look-around with the same syntax:
(?=), (?!), (?<=), and (?<!).

Both languages support the anchors \A and \Z, and the character classes \d and \D, \s and \S.

Both languages let you name a capture with (?P<name>) and reference it with
(?P=name). Perl has its own syntax for this in addition to supporting the syntax
of Python.  Features in Perl, not in Python

The biggest feature in Perl regular expressions not in Python (i.e. not in
Python’s re module at the time of writing) is Unicode character classes \p{} and
their negation \P{}.

Perl’s \X is a sort of variation on . for Unicode. Programming Perl describes it
this way:

    The point is that \X matches one user-visible character (grapheme), even if
    it takes several programmer-visible characters (code-points) to do so.

A few other features unique to Perl are quoting with \Q and \E, making
characters lowercase or title case with \l and \u, or making a sequence of
characters lower or uppercase beginning with \L or \U and ending with \E.

There are many other regular expression features unique to Perl, but I’ve
highlighted the ones I’m most likely to want to use.

:LOGBOOK:
CLOCK: [2023-09-05 Tue 01:51]--[2023-09-05 Tue 01:52] =>  0:01
:END:
[2023-09-05 Tue 01:50]
* Glibc installation dependencies                            :Technical:NOTE:
Bash: sh
Binutils: ar, as, ld, ranlib, readelf
Diffutils: cmp
Fileutils: chmod, cp, install, ln, mknod, mv, mkdir, rm, touch
Gcc: cc, cc1, collect2, cpp, gcc
Grep: egrep, grep
Gzip: gzip
Make: make
Gawk: gawk
Sed: sed
Sh-utils: date, expr, hostname, pwd, uname
Texinfo: install-info, makeinfo
Textutils: cat, cut, sort, tr

:LOGBOOK:
CLOCK: [2023-09-05 Tue 15:59]--[2023-09-05 Tue 16:01] =>  0:02
:END:
[2023-09-05 Tue 15:59]
* LD_LIBRARY_PATH Trouble and Solutions                      :Technical:NOTE:

Pointer : https://www.hpc.dtu.dk/?page_id=1180

This little note is about one of the most “misused” environment variables on
Unix systems: LD_LIBRARY_PATH . If used right, it can be very useful, but very
often – not to say, most of the time – people apply it in the wrong way, and
that is were they are calling for trouble.  So, what does it do?

LD_LIBRARY_PATH tells the dynamic link loader (ld. so – this little program that
starts all your applications) where to search for the dynamic shared libraries
an application was linked against. Multiple directories can be listed, separated
by a colon (:), and this list is then searched before the compiled-in search
path(s), and the standard locations (typically /lib, /usr/lib, …).

This can be used for

    testing new versions of a shared library against an already compiled
    application re-locating shared libraries, e.g. to preserve old versions
    creating a self-contained, relocatable(!) environment for larger
    applications, such that they do not depend on (changing) system libraries –
    many software vendors use that approach.

Sounds very useful, where is the problem?

Yes, it is useful – if you apply it in the way it was invented for, like the
three cases above. However, very often it is used as a crutch to fix a problem
that could have been avoided by other means (see below). It is even getting
worse, if this crutch is applied globally into an user’s (or the system’s!)
environment: applications compiled with those settings get dependent on this
crutch – and if it is eventually taken away, they start to stumble (i.e. fail to
run).

There are other implications as well:

    Security: Remember that the directories specified in LD_LIBRARY_PATH get
    searched before(!) the standard locations? In that way, a nasty person could
    get your application to load a version of a shared library that contains
    malicious code! That’s one reason why setuid/setgid executables do neglect
    that variable!  Performance: The link loader has to search all the
    directories specified, until it finds the directory where the shared library
    resides – for ALL shared libraries the application is linked against! This
    means a lot of system calls to open(), that will fail with “ENOENT (No such
    file or directory)”! If the path contains many directories, the number of
    failed calls will increase linearly, and you can tell that from the start-up
    time of the application. If some (or all) of the directories are in an NFS
    environment, the start-up time of your applications can really get long –
    and it can slow down the whole system!  Inconsistency: This is the most
    common problem. LD_LIBRARY_PATH forces an application to load a shared
    library it wasn’t linked against, and that is quite likely not compatible
    with the original version. This can either be very obvious, i.e. the
    application crashes, or it can lead to wrong results, if the picked up
    library not quite does what the original version would have done. Especially
    the latter is sometimes hard to debug.

How can I check which dynamic libraries are loaded?

There is the ldd command, that shows you which libraries are needed by a
dynamically linked executable, e.g.

$ ldd /usr/bin/file
        linux-vdso.so.1 =>  (0x00007fff9646c000)
        libmagic.so.1 => /usr/lib64/libmagic.so.1 (0x00000030f9a00000)
        libz.so.1 => /lib64/libz.so.1 (0x00000030f8e00000)
        libc.so.6 => /lib64/libc.so.6 (0x00000030f8200000)
        /lib64/ld-linux-x86-64.so.2 (0x00000030f7a00000)

This is a ‘static’ view, since ldd doesn’t resolve dependencies and libraries
that will get loaded at runtime, e.g. by a library that depends on others. To
get an overview of libraries loaded at runtime, you can use the pldd command:

$ ldd /bin/bash
        linux-vdso.so.1 =>  (0x00007ffff63ff000)
        libtinfo.so.5 => /lib64/libtinfo.so.5 (0x0000003108a00000)
        libdl.so.2 => /lib64/libdl.so.2 (0x00000030f8600000)
        libc.so.6 => /lib64/libc.so.6 (0x00000030f8200000)
        /lib64/ld-linux-x86-64.so.2 (0x00000030f7a00000)
$ pldd 24362
24362:  -bash
/lib64/ld-2.12.so
/lib64/libc-2.12.so
/lib64/libdl-2.12.so
/lib64/libtinfo.so.5.7
/usr/lib64/gconv/ISO8859-1.so
/lib64/libnss_files-2.12.so

As you can see, there are two more .so-files loaded at runtime, that weren’t on
the ‘static’ list.

Note: pldd is originally a Solaris command, that usually is not available on
Linux. However, there is a Perl-script available (and installed on our machines)
that extracts this information from the /proc/<PID>/maps file.  How to avoid
those problems with LD_LIBRARY_PATH?

A very simplistic answer would be: “just don’t use LD_LIBRARY_PATH!” The more
realistic answer is, “the less you use it, the better off you will be”.

Below comes a list of ways how to avoid LD_LIBRARY_PATH, inspired by reference
[1] below. The best solution is on the top, going down to the last resort.

    If you compile your application(s) yourself, you can solve the problem by
    specifying the correct location of the shared libraries and tell the linker
    to add those to the runpath of your executable, specifying the path in the
    ‘-rpath’ linker option:

    cc -o myprog obj1.o ... objn.o -Wl,-rpath=/path/to/lib \
       -L/path/to/lib -lmylib

    The linker also reads the LD_RUN_PATH environment variable, if set, and thus
    you can specify more than one path in an easy way, without having to use the
    above linker option:

    export LD_RUN_PATH=/path/to/lib1:/path/to/lib2:/path/to/lib3
    cc -o myprog obj1.o ... objn.o -L/path/to/lib1 -lmylib1 \
       -L/path/to/lib2 -lmylib2 ...

    In both cases, you can check with ldd, that your executable will find the
    right libraries at start-up (see above). If there is a ‘not found’ message
    in the ldd output, you have done something wrong and should review your
    Makefile and/or your LD_RUN_PATH settings.  There are tools around, to
    fix/change the runpath in a binary executable, e.g. chrpath under Linux. The
    problem with this method is, that the space in the executable that contains
    this information (i.e. the string defining the path) cannot be extended,
    i.e. you cannot add additional information – only overwrite an existing
    path. Furthermore, if no runpath exists in the executable, there is no way
    to change it. Read the man page for chrpath for more information.  If you
    can’t fix the executable, create a wrapper script that calls the executable
    with the right LD_LIBRARY_PATH setting. In that way, the setting gets
    exposed to this application, only – and the applications that get started by
    that. The latter can lead to the inconsistency problem above, though.

    #!/bin/sh
    LD_LIBRARY_PATH=/path/to/lib1:/path/to/lib2:/path/to/lib3
    export LD_LIBRARY_PATH
    exec /path/to/bin/myprog $@

    Testing a LD_LIBRARY_PATH from the command line:

    $ env LD_LIBRARY_PATH=/path/to/lib1:/path/to/lib2:/path/to/lib3 ./myprog

    This sets LD_LIBRARY_PATH for this command only. Do NOT do:

    $ export LD_LIBRARY_PATH=/path/to/lib1:/path/to/lib2:/path/to/lib3
    $ ./myprog

    since this will pollute the shell environment for all consecutive commands!
    Never put LD_LIBRARY_PATH in your login profiles! In that way you will
    expose all the applications you start to this – probably problematic – path!

Unfortunately, some ISVs ship software, that puts global LD_LIBRARY_PATH
settings into the system profiles during the installation, or they ask the user
to add those settings to their profiles. Just say no! Try if you can solve the
problem by other means, e.g. by creating a wrapper script, or tell the vendor to
fix this problem.

:LOGBOOK:
CLOCK: [2023-09-05 Tue 16:06]--[2023-09-05 Tue 16:09] =>  0:03
:END:
[2023-09-05 Tue 16:06]
* How does Linux nat a ping                                  :Technical:NOTE:
https://devnonsense.com/posts/how-does-linux-nat-a-ping/
:LOGBOOK:
CLOCK: [2023-09-13 Wed 04:11]--[2023-09-13 Wed 04:12] =>  0:01
:END:
[2023-09-13 Wed 04:11]
* Symbolic link mess in UNIX file system hierachies :Technical:info:laptop:important:NOTE:
:LOGBOOK:
CLOCK: [2023-10-28 Sat 13:39]--[2023-10-28 Sat 13:42] =>  0:03
:END:
[2023-10-28 Sat 13:39]

Check out the new USENIX Web site.

	Home 		About USENIX 		Events 		Membership 		Publications 		Students
2000 USENIX Annual Technical Conference    [Technical Index]

Pp. 85–92 of the Proceedings
Lexical File Names in Plan 9
or
Getting Dot-Dot Right

    Rob Pike
    rob@plan9.bell-labs.com
    Bell Laboratories, Murray Hill, NJ, 07974

    ABSTRACT


    Symbolic links make the Unix file system non-hierarchical, resulting in multiple valid path names for a given file. This ambiguity is a source of confusion, especially since some shells work overtime to present a consistent view from programs such as pwd, while other programs and the kernel itself do nothing about the problem.

    Plan 9 has no symbolic links but it does have other mechanisms that produce the same difficulty. Moreover, Plan 9 is founded on the ability to control a program's environment by manipulating its name space. Ambiguous names muddle the result of operations such as copying a name space across the network.

    To address these problems, the Plan 9 kernel has been modified to maintain an accurate path name for every active file (open file, working directory, mount table entry) in the system. The definition of 'accurate' is that the path name for a file is guaranteed to be the rooted, absolute name the program used to acquire it. These names are maintained by an efficient method that combines lexical processing—such as evaluating .. by just removing the last path name element of a directory—with local operations within the file system to maintain a consistently, easily understood view of the name system. Ambiguous situations are resolved by examining the lexically maintained names themselves.

    A new kernel call, fd2path, returns the file name associated with an open file, permitting the use of reliable names to improve system services ranging from pwd to debugging. Although this work was done in Plan 9, Unix systems could also benefit from the addition of a method to recover the accurate name of an open file or the current directory.

Motivation


Consider the following unedited transcript of a session running the Bourne shell on a modern Unix system:

    % echo $HOME
    /home/rob
    % cd $HOME
    % pwd
    /n/bopp/v7/rob
    % cd /home/rob
    % cd /home/ken
    % cd ../rob
    ../rob: bad directory
    %

(The same output results from running tcsh; we'll discuss ksh in a moment.) To a neophyte being schooled in the delights of a hierarchical file name space, this behavior must be baffling. It is, of course, the consequence of a series of symbolic links intended to give users the illusion they share a disk, when in fact their files are scattered over several devices:

    % ls -ld /home/rob /home/ken
    lrwxr-xr-x  1 root  sys   14 Dec 26  1998 /home/ken -> /n/bopp/v6/ken
    lrwxr-xr-x  1 root  sys   14 Dec 23  1998 /home/rob -> /n/bopp/v7/rob
    %

The introduction of symbolic links has changed the Unix file system from a true hierarchy into a directed graph, rendering .. ambiguous and sowing confusion.

Unix popularized hierarchical naming, but the introduction of symbolic links made its naming irregular. Worse, the pwd command, through the underlying getwd library routine, uses a tricky, expensive algorithm that often delivers the wrong answer. Starting from the current directory, getwd opens the parent, .., and searches it for an entry whose i-number matches the current directory; the matching entry is the final path element of the ultimate result. Applying this process iteratively, getwd works back towards the root. Since getwd knows nothing about symbolic links, it will recover surprising names for directories reached by them, as illustrated by the example; the backward paths getwd traverses will not backtrack across the links.

Partly for efficiency and partly to make cd and pwd more predictable, the Korn shell ksh [Korn94] implements pwd as a builtin. (The cd command must be a builtin in any shell, since the current directory is unique to each process.) Ksh maintains its own private view of the file system to try to disguise symbolic links; in particular, cd and pwd involve some lexical processing (somewhat like the cleanname function discussed later in this paper), augmented by heuristics such as examining the environment for names like $HOME and $PWD to assist initialization of the state of the private view. [Korn00]

This transcript begins with a Bourne shell running:

    % cd /home/rob
    % pwd
    /n/bopp/v7/rob
    % ksh
    $ pwd
    /home/rob
    $

This result is encouraging. Another example, again starting from a Bourne shell:

    % cd /home/rob
    % cd ../ken
    ../ken: bad directory
    % ksh
    $ pwd
    /home/rob
    $ cd ../ken
    $ pwd
    /home/ken
    $

By doing extra work, the Korn shell is providing more sensible behavior, but it is easy to defeat:

    % cd /home/rob
    % pwd
    /n/bopp/v7/rob
    % cd bin
    % pwd
    /n/bopp/v7/rob/bin
    % ksh
    $ pwd
    /n/bopp/v7/rob/bin
    $ exit
    % cd /home/ken
    % pwd
    /n/bopp/v6/ken
    % ksh
    $ pwd
    /n/bopp/v6/ken
    $

In these examples, ksh's built-in pwd failed to produce the results /home/rob/bin ( and /home/ken) that the previous example might have led us to expect. The Korn shell is hiding the problem, not solving it, and in fact is not even hiding it very well.

A deeper question is whether the shell should even be trying to make pwd and cd do a better job. If it does, then the getwd library call and every program that uses it will behave differently from the shell, a situation that is sure to confuse. Moreover, the ability to change directory to ../ken with the Korn shell's cd command but not with the chdir system call is a symptom of a diseased system, not a healthy shell.

The operating system should provide names that work and make sense. Symbolic links, though, are here to stay, so we need a way to provide sensible, unambiguous names in the face of a non-hierarchical name space. This paper shows how the challenge was met on Plan 9, an operating system with Unix-like naming.
Names in Plan 9


Except for some details involved with bootstrapping, file names in Plan 9 have the same syntax as in Unix. Plan 9 has no symbolic links, but its name space construction operators, bind and mount, make it possible to build the same sort of non-hierarchical structures created by symbolically linking directories on Unix.

Plan 9's mount system call takes a file descriptor and attaches to the local name space the file system service it represents:

    mount(fd, "/dir", flags)

Here fd is a file descriptor to a communications port such as a pipe or network connection; at the other end of the port is a service, such as file server, that talks 9P, the Plan 9 file system protocol. After the call succeeds, the root directory of the service will be visible at the mount point /dir, much as with the mount call of Unix. The flag argument specifies the nature of the attachment: MREPL says that the contents of the root directory (appear to) replace the current contents of /dir; MAFTER says that the current contents of dir remain visible, with the mounted directory's contents appearing after any existing files; and MBEFORE says that the contents remain visible, with the mounted directory's contents appearing before any existing files. These multicomponent directories are called union directories and are somewhat different from union directories in 4.4BSD-Lite [PeMc95], because only the top-level directory itself is unioned, not its descendents, recursively. (Plan 9's union directories are used differently from 4.4BSD-Lite's, as will become apparent.)

For example, to bootstrap a diskless computer the system builds a local name space containing only the root directory, /, then uses the network to open a connection to the main file server. It then executes

    mount(rootfd, "/", MREPL);

After this call, the entire file server's tree is visible, starting from the root of the local machine.

While mount connects a new service to the local name space, bind rearranges the existing name space:

    bind("tofile", "fromfile", flags)

causes subsequent mention of the fromfile (which may be a plain file or a directory) to behave as though tofile had been mentioned instead, somewhat like a symbolic link. (Note, however, that the arguments are in the opposite order compared to ln -s). The flags argument is the same as with mount.

As an example, a sequence something like the following is done at bootstrap time to assemble, under the single directory /bin, all of the binaries suitable for this architecture, represented by (say) the string sparc:

    bind("/sparc/bin", "/bin", MREPL);
    bind("/usr/rob/sparc/bin", "/bin", MAFTER);

This sequence of binds causes /bin to contain first the standard binaries, then the contents of rob's private SPARC binaries. The ability to build such union directories obviates the need for a shell $PATH variable while providing opportunities for managing heterogeneity. If the system were a Power PC, the same sequence would be run with power textually substituted for sparc to place the Power PC binaries in /bin rather than the SPARC binaries.

Trouble is already brewing. After these bindings are set up, where does

    % cd /bin
    % cd ..

set the current working directory, to / or /sparc or /usr/rob/sparc? We will return to this issue.

There are some important differences between binds and symbolic links. First, symbolic links are a static part of the file system, while Plan 9 bindings are created at run time, are stored in the kernel, and endure only as long as the system maintains them; they are temporary. Since they are known to the kernel but not the file system, they must be set up each time the kernel boots or a user logs in; permanent bindings are created by editing system initialization scripts and user profiles rather than by building them in the file system itself.

The Plan 9 kernel records what bindings are active for a process, whereas symbolic links, being held on the Unix file server, may strike whenever the process evaluates a file name. Also, symbolic links apply to all processes that evaluate the affected file, whereas bind has a local scope, applying only to the process that executes it and possibly some of its peers, as discussed in the next section. Symbolic links cannot construct the sort of /bin directory built here; it is possible to have multiple directories point to /bin but not the other way around.

Finally, symbolic links are symbolic, like macros: they evaluate the associated names each time they are accessed. Bindings, on the other hand, are evaluated only once, when the bind is executed; after the binding is set up, the kernel associates the underlying files, rather than their names. In fact, the kernel's representation of a bind is identical to its representation of a mount; in effect, a bind is a mount of the tofile upon the fromfile. The binds and mounts coexist in a single mount table, the subject of the next section.
The Mount Table


Unix has a single global mount table for all processes in the system, but Plan 9's mount tables are local to each process. By default it is inherited when a process forks, so mounts and binds made by one process affect the other, but a process may instead inherit a copy, so modifications it makes will be invisible to other processes. The convention is that related processes, such as processes running in a single window, share a mount table, while sets of processes in different windows have distinct mount tables. In practice, the name spaces of the two windows will appear largely the same, but the possibility for different processes to see different files (hence services) under the same name is fundamental to the system, affecting the design of key programs such as the window system [Pike91].

The Plan 9 mount table is little more than an ordered list of pairs, mapping the fromfiles to the tofiles. For mounts, the tofile will be an item called a Channel, similar to a Unix vnode, pointing to the root of the file service, while for a bind it will be the Channel pointing to the tofile mentioned in the bind call. In both cases, the fromfile entry in the table will be a Channel pointing to the fromfile itself.

The evaluation of a file name proceeds as follows. If the name begins with a slash, start with the Channel for the root; otherwise start with the Channel for the current directory of the process. For each path element in the name, such as usr in /usr/rob, try to 'walk' the Channel to that element [Pike93]. If the walk succeeds, look to see if the resulting Channel is the same as any fromfile in the mount table, and if so, replace it by the corresponding tofile. Advance to the next element and continue.

There are a couple of nuances. If the directory being walked is a union directory, the walk is attempted in the elements of the union, in order, until a walk succeeds. If none succeed, the operation fails. Also, when the destination of a walk is a directory for a purpose such as the chdir system call or the fromfile in a bind, once the final walk of the sequence has completed the operation stops; the final check through the mount table is not done. Among other things, this simplifies the management of union directories; for example, subsequent bind calls will append to the union associated with the underlying fromfile instead of what is bound upon it.
A Definition of Dot-Dot


The ability to construct union directories and other intricate naming structures introduces some thorny problems: as with symbolic links, the name space is no longer hierarchical, files and directories can have multiple names, and the meaning of .., the parent directory, can be ambiguous.

The meaning of .. is straightforward if the directory is in a locally hierarchical part of the name space, but if we ask what .. should identify when the current directory is a mount point or union directory or multiply symlinked spot (which we will henceforth call just a mount point, for brevity), there is no obvious answer. Name spaces have been part of Plan 9 from the beginning, but the definition of .. has changed several times as we grappled with this issue. In fact, several attempts to clarify the meaning of .. by clever coding resulted in definitions that could charitably be summarized as 'what the implementation gives.'

Frustrated by this situation, and eager to have better-defined names for some of the applications described later in this paper, we recently proposed the following definition for ..:

    The parent of a directory X, X/.., is the same directory that would obtain if we instead accessed the directory named by stripping away the last path name element of X.



For example, if we are in the directory /a/b/c and chdir to .., the result is exactly as if we had executed a chdir to /a/b.

This definition is easy to understand and seems natural. It is, however, a purely lexical definition that flatly ignores evaluated file names, mount tables, and other kernel-resident data structures. Our challenge is to implement it efficiently. One obvious (and correct) implementation is to rewrite path names lexically to fold out .., and then evaluate the file name forward from the root, but this is expensive and unappealing. We want to be able to use local operations to evaluate file names, but maintain the global, lexical definition of dot-dot. It isn't too hard.
The Implementation


To operate lexically on file names, we associate a name with each open file in the kernel, that is, with each Channel data structure. The first step is therefore to store a char* with each Channel in the system, called its Cname, that records the absolute rooted file name for the Channel. Cnames are stored as full text strings, shared copy-on-write for efficiency. The task is to maintain each Cname as an accurate absolute name using only local operations.

When a file is opened, the file name argument in the open (or chdir or bind or ...) call is recorded in the Cname of the resulting Channel. When the file name begins with a slash, the name is stored as is, subject to a cleanup pass described in the next section. Otherwise, it is a local name, and the file name must be made absolute by prefixing it with the Cname of the current directory, followed by a slash. For example, if we are in /home/rob and chdir to bin, the Cname of the resulting Channel will be the string /home/rob/bin.

This assumes, of course, that the local file name contains no .. elements. If it does, instead of storing for example /home/rob/.. we delete the last element of the existing name and set the Cname to /home. To maintain the lexical naming property we must guarantee that the resulting Cname, if it were to be evaluated, would yield the identical directory to the one we actually do get by the local .. operation.

If the current directory is not a mount point, it is easy to maintain the lexical property. If it is a mount point, though, it is still possible to maintain it on Plan 9 because the mount table, a kernel-resident data structure, contains all the information about the non-hierarchical connectivity of the name space. (On Unix, by contrast, symbolic links are stored on the file server rather than in the kernel.) Moreover, the presence of a full file name for each Channel in the mount table provides the information necessary to resolve ambiguities.

The mount table is examined in the from->to direction when evaluating a name, but .. points backwards in the hierarchy, so to evaluate .. the table must be examined in the to->from direction. ("How did we get here?")

The value of .. is ambiguous when there are multiple bindings (mount points) that point to the directories involved in the evaluation of ... For example, return to our original script with /n/bopp/v6 (containing a home directory for ken) and /n/bopp/v7 (containing a home directory for rob) unioned into /home. This is represented by two entries in the mount table, from=/home, to=/n/bopp/v6 and from=/home, to=/n/bopp/v7. If we have set our current directory to /home/rob (which has landed us in the physical location /n/bopp/v7/rob) our current directory is not a mount point but its parent is. The value of .. is ambiguous: it could be /home, /n/bopp/v7, or maybe even /n/bopp/v6, and the ambiguity is caused by two tofiles bound to the same fromfile. By our definition, if we now evaluate .., we should acquire the directory /home; otherwise ../ken could not possibly result in ken's home directory, which it should. On the other hand, if we had originally gone to /n/bopp/v7/rob, the name ../ken should not evaluate to ken's home directory because there is no directory /n/bopp/v7/ken (ken's home directory is on v6). The problem is that by using local file operations, it is impossible to distinguish these cases: regardless of whether we got here using the name /home/rob or /n/bopp/v7/rob, the resulting directory is the same. Moreover, the mount table does not itself have enough information to disambiguate: when we do a local operation to evaluate .. and land in /n/bopp/v7, we discover that the directory is a tofile in the mount table; should we step back through the table to /home or not?

The solution comes from the Cnames themselves. Whether to step back through the mount point from=/home, to=/n/bopp/v7 when evaluating .. in rob's directory is trivially resolved by asking the question, Does the Cname for the directory begin /home? If it does, then the path that was evaluated to get us to the current directory must have gone through this mount point, and we should back up through it to evaluate ..; if not, then this mount table entry is irrelevant.

More precisely, both before and after each .. element in the path name is evaluated, if the directory is a tofile in the mount table, the corresponding fromfile is taken instead, provided the Cname of the corresponding fromfile is the prefix of the Cname of the original directory. Since we always know the full name of the directory we are evaluating, we can always compare it against all the entries in the mount table that point to it, thereby resolving ambiguous situations and maintaining the lexical property of ... This check also guarantees we don't follow a misleading mount point, such as the entry pointing to /home when we are really in /n/bopp/v7/rob. Keeping the full names with the Channels makes it easy to use the mount table to decide how we got here and, therefore, how to get back.

In summary, the algorithm is as follows. Use the usual file system operations to walk to ..; call the resulting directory d. Lexically remove the last element of the initial file name. Examine all entries in the mount table whose tofile is d and whose fromfile has a Cname identical to the truncated name. If one exists, that fromfile is the correct result; by construction, it also has the right Cname. In our example, evaluating .. in /home/rob (really /n/bopp/v7/rob) will set d to /n/bopp/v7; that is a tofile whose fromfile is /home. Removing the /rob from the original Cname, we find the name /home, which matches that of the fromfile, so the result is the fromfile, /home.

Since this implementation uses only local operations to maintain its names, it is possible to confuse it by external changes to the file system. Deleting or renaming directories and files that are part of a Cname, or modifying the mount table, can introduce errors. With more implementation work, such mistakes could probably be caught, but in a networked environment, with machines sharing a remote file server, renamings and deletions made by one machine may go unnoticed by others. These problems, however, are minor, uncommon and, most important, easy to understand. The method maintains the lexical property of file names unless an external agent changes the name surreptitiously; within a stable file system, it is always maintained and pwd is always right.

To recapitulate, maintaining the Channel's absolute file names lexically and using the names to disambiguate the mount table entries when evaluating .. at a mount point combine to maintain the lexical definition of .. efficiently.
Cleaning names


The lexical processing can generate names that are messy or redundant, ones with extra slashes or embedded ../ or ./ elements and other extraneous artifacts. As part of the kernel's implementation, we wrote a procedure, cleanname, that rewrites a name in place to canonicalize its appearance. The procedure is useful enough that it is now part of the Plan 9 C library and is employed by many programs to make sure they always present clean file names.

Cleanname is analogous to the URL-cleaning rules defined in RFC 1808 [Field95], although the rules are slightly different. Cleanname iteratively does the following until no further processing can be done:

    1. Reduce multiple slashes to a single slash.
    2. Eliminate . path name elements (the current directory).
    3. Eliminate .. path name elements (the parent directory) and the . non- .., non- element that precedes them.
    4. Eliminate .. elements that begin a rooted path, that is, replace /.. by / at the beginning of a path.
    5. Leave intact .. elements that begin a non-rooted path.



If the result of this process is a null string, cleanname returns the string ".", representing the current directory.
The fd2path system call


Plan 9 has a new system call, fd2path, to enable programs to extract the Cname associated with an open file descriptor. It takes three arguments: a file descriptor, a buffer, and the size of the buffer:

    int fd2path(int fd, char *buf, int nbuf)

It returns an error if the file descriptor is invalid; otherwise it fills the buffer with the name associated with fd. (If the name is too long, it is truncated; perhaps this condition should also draw an error.) The fd2path system call is very cheap, since all it does is copy the Cname string to user space.

The Plan 9 implementation of getwd uses fd2path rather than the tricky algorithm necessary in Unix:

    char*
    getwd(char *buf, int nbuf)
    {
    	int n, fd;

    	fd = open(".", OREAD);
    	if(fd < 0)
    		return NULL;
    	n = fd2path(fd, buf, nbuf);
    	close(fd);
    	if(n < 0)
    		return NULL;
    	return buf;
    }

(The Unix specification of getwd does not include a count argument.) This version of getwd is not only straightforward, it is very efficient, reducing the performance advantage of a built-in pwd command while guaranteeing that all commands, not just pwd, see sensible directory names.

Here is a routine that prints the file name associated with each of its open file descriptors; it is useful for tracking down file descriptors left open by network listeners, text editors that spawn commands, and the like:

    void
    openfiles(void)
    {
    	int i;
    	char buf[256];

    	for(i=0; i<NFD; i++)
    		if(fd2path(i, buf, sizeof buf) >= 0)
    			print("%d: %s\n", i, buf);
    }

Uses of good names


Although pwd was the motivation for getting names right, good file names are useful in many contexts and have become a key part of the Plan 9 programming environment. The compilers record in the symbol table the full name of the source file, which makes it easy to track down the source of buggy, old software and also permits the implementation of a program, src, to automate tracking it down. Given the name of a program, src reads its symbol table, extracts the file information, and triggers the editor to open a window on the program's source for its main routine. No guesswork, no heuristics.

The openfiles routine was the inspiration for a new file in the /proc file system [Kill84]. For process n, the file /proc/n/fd is a list of all its open files, including its working directory, with associated information including its open status, I/O offset, unique id (analogous to i-number) and file name. Here is the contents of the fd file for a process in the window system on the machine being used to write this paper:

    % cat /proc/125099/fd
    /usr/rob
      0 r  M 5141 00000001.00000000        0 /mnt/term/dev/cons
      1 w  M 5141 00000001.00000000       51 /mnt/term/dev/cons
      2 w  M 5141 00000001.00000000       51 /mnt/term/dev/cons
      3 r  M 5141 0000000b.00000000     1166 /dev/snarf
      4 rw M 5141 0ffffffc.00000000      288 /dev/draw/new
      5 rw M 5141 00000036.00000000  4266337 /dev/draw/3/data
      6 r  M 5141 00000037.00000000        0 /dev/draw/3/refresh
      7 r  c    0 00000004.00000000  6199848 /dev/bintime
    %

(The Linux implementation of /proc provides a related service by giving a directory in which each file-descriptor-numbered file is a symbolic link to the file itself.) When debugging errant systems software, such information can be valuable.

Another motivation for getting names right was the need to extract from the system an accurate description of the mount table, so that a process's name space could be recreated on another machine, in order to move (or simulate) a computing environment across the network. One program that does this is Plan 9's cpu command, which recreates the local name space on a remote machine, typically a large fast multiprocessor. Without accurate names, it was impossible to do the job right; now /proc provides a description of the name space of each process, /proc/n/ns:

    % cat /proc/125099/ns
    bind  / /
    mount -aC #s/boot /
    bind  #c /dev
    bind  #d /fd
    bind -c #e /env
    bind  #p /proc
    bind -c #s /srv
    bind  /386/bin /bin
    bind -a /rc/bin /bin
    bind  /net /net
    bind -a #l /net
    mount -a #s/cs /net
    mount -a #s/dns /net
    bind -a #D /net
    mount -c #s/boot /n/emelie
    bind -c /n/emelie/mail /mail
    mount -c /net/il/134/data /mnt/term
    bind -a /usr/rob/bin/rc /bin
    bind -a /usr/rob/bin/386 /bin
    mount  #s/boot /n/emelieother other
    bind -c /n/emelieother/rob /tmp
    mount  #s/boot /n/dump dump
    bind  /mnt/term/dev/cons /dev/cons
    ...
    cd /usr/rob
    %

(The # notation identifies raw device drivers so they may be attached to the name space.) The last line of the file gives the working directory of the process. The format of this file is that used by a library routine, newns, which reads a textual description like this and reconstructs a name space. Except for the need to quote # characters, the output is also a shell script that invokes the user-level commands bind and mount, which are just interfaces to the underlying system calls. However, files like /net/il/134/data represent network connections; to find out where they point, so that the corresponding calls can be reestablished for another process, they must be examined in more detail using the network device files [PrWi93]. Another program, ns, does this; it reads the /proc/n/ns file, decodes the information, and interprets it, translating the network addresses and quoting the names when required:

    ...
    mount -a '#s/dns' /net
    ...
    mount -c il!135.104.3.100!12884 /mnt/term
    ...

These tools make it possible to capture an accurate description of a process's name space and recreate it elsewhere. And like the open file descriptor table, they are a boon to debugging; it is always helpful to know exactly what resources a program is using.
Adapting to Unix


This work was done for the Plan 9 operating system, which has the advantage that the non-hierarchical aspects of the name space are all known to the kernel. It should be possible, though, to adapt it to a Unix system. The problem is that Unix has nothing corresponding precisely to a Channel, which in Plan 9 represents the unique result of evaluating a name. The vnode structure is a shared structure that may represent a file known by several names, while the file structure refers only to open files, but for example the current working directory of a process is not open. Possibilities to address this discrepancy include introducing a Channel-like structure that connects a name and a vnode, or maintaining a separate per-process table that maps names to vnodes, disambiguating using the techniques described here. If it could be done the result would be an implementation of .. that reduces the need for a built-in pwd in the shell and offers a consistent, sensible interpretation of the 'parent directory'.

We have not done this adaptation, but we recommend that the Unix community try it.
Conclusions


It should be easy to discover a well-defined, absolute path name for every open file and directory in the system, even in the face of symbolic links and other non-hierarchical elements of the file name space. In earlier versions of Plan 9, and all current versions of Unix, names can instead be inconsistent and confusing.

The Plan 9 operating system now maintains an accurate name for each file, using inexpensive lexical operations coupled with local file system actions. Ambiguities are resolved by examining the names themselves; since they reflect the path that was used to reach the file, they also reflect the path back, permitting a dependable answer to be recovered even when stepping backwards through a multiply-named directory.

Names make sense again: they are sensible and consistent. Now that dependable names are available, system services can depend on them, and recent work in Plan 9 is doing just that. We—the community of Unix and Unix-like systems—should have done this work a long time ago.
Acknowledgements


Phil Winterbottom devised the ns command and the fd and ns files in /proc, based on an earlier implementation of path name management that the work in this paper replaces. Russ Cox wrote the final version of cleanname and helped debug the code for reversing the mount table. Ken Thompson, Dave Presotto, and Jim McKie offered encouragement and consultation.
References


[Field95] R. Fielding, "Relative Uniform Resource Locators", Network Working Group Request for Comments: 1808, June, 1995.

[Kill84] T. J. Killian, "Processes as Files", Proceedings of the Summer 1984 USENIX Conference, Salt Lake City, 1984, pp. 203-207.

[Korn94] David G. Korn, "ksh: An Extensible High Level Language", Proceedings of the USENIX Very High Level Languages Symposium, Santa Fe, 1994, pp. 129-146.

[Korn00] David G. Korn, personal communication.

[PeMc95] Jan-Simon Pendry and Marshall Kirk McKusick, "Union Mounts in 4.4BSD-Lite", Proceedings of the 1995 USENIX Conference, New Orleans, 1995.

[Pike91] Rob Pike, "8½, the Plan 9 Window System", Proceedings of the Summer 1991 USENIX Conference, Nashville, 1991, pp. 257-265.

[Pike93] Rob Pike, Dave Presotto, Ken Thompson, Howard Trickey, and Phil Winterbottom, "The Use of Name Spaces in Plan 9", Operating Systems Review, 27, 2, April 1993, pp. 72-76.

[PrWi93] Dave Presotto and Phil Winterbottom, "The Organization of Networks in Plan 9", Proceedings of the Winter 1993 USENIX Conference, San Diego, 1993, pp. 43-50.

Copyright © 2000 Lucent Technologies Inc. All rights reserved.
This paper was originally published in the Proceedings of the 2000 USENIX Annual Technical Conference, June 18-23, 2000, San Diego, California, USA
Last changed: 7 Feb 2002 ml

Technical Program
Conference Index Home
USENIX home
* Linux coredump internals                                   :Technical:NOTE:
What's Inside a Linux Kernel Core Dump
February 8, 2024 | 35 minute read
Stephen Brennan

Linux kernel core dumps are often critical for diagnosing and fixing problems with the OS. We’ve published several blogs related to kernel core dumps, including how to generate them, how to estimate their size, how to analyze them with Drgn, and even how to manually extract stack function arguments from them. But have you ever wondered what’s really in a core dump? In this blog, we’ll answer that question. We’ll start by discussing the different software which can actually produce a vmcore (there are more than you might think). Next, we’ll discuss the contents of vmcores, including what important metadata needs to be present in order to make analysis possible. We’ll then dive into the details of a few of the most prominent vmcore formats and variations on them, before finishing with a quick overview of some tools that can be used to analyze them.

This topic has a lot of history and so much variation, that there’s no hope of covering it all. Instead, we’ll focus on the sources and formats most commonly found with Oracle Linux, which should cover much of the modern desktop and server Linux landscape. This blog isn’t intended to be a step-by-step guide to achieving a particular task; it’s just a reference and introduction to a field that’s not frequently discussed.
Core dump sources

When the Linux kernel encounters an unrecoverable error (a “panic”) or a hang, it’s incredibly useful to save the state of memory, registers, etc. into a file for later analysis & debugging. In these situations, the system may be in a dire state, so the process which creates the core dump must be reliable. Further, downtime and disk space can be expensive, so the process must also be reasonably quick, and should avoid capturing unnecessary information. This is a tall order, and so there are multiple ways a core dump can be created, each with different trade-offs.

For distributions like Oracle Linux, the most common way to create a vmcore is by using kexec(8), generally with makedumpfile. However, it can also be done by a hypervisor or firmware level crash dump system. We’ll explain and discuss the different possibilities in this section.
Kexec crash kernels

On a system which is configured for kexec crash dumps, some memory is reserved at boot time for a second Linux kernel (the “kdump kernel”). On startup, the system uses kexec_load(2) to load a kernel image into this reserved memory region. If a panic occurs, all CPUs are halted and control is transferred to the kdump kernel. The kdump kernel boots up and represents the memory image of the previous kernel as the file /proc/vmcore - thus the name “vmcore”. Normally, the kdump kernel is configured to execute a tool (typically makedumpfile) which will then save this file to a disk or network location, and then reboot.

The /proc/vmcore file is thus one of the most common sources for kernel core dumps. The data is represented in ELF (Executable and Linkable Format), which we will discuss a bit more later on. However, if you were to go searching on your Linux machine for a /proc/vmcore file, you probably wouldn’t find it, because it only appears when you’re running within a kdump kernel. (More precisely, only when the elfcorehdr command-line option points at a valid ELF header created by the original kernel). But there is also another ELF formatted core image which the kernel provides: /proc/kcore.
Linux running kernels

Unlike /proc/vmcore, the file /proc/kcore is always available. Rather than showing the memory image of the previously crashed kernel, it shows the live memory image of the currently running kernel. It’s common for people to run live debugging tools like crash or drgn against /proc/kcore, since it’s always present and easy to access, assuming that security features such as lockdown=confidentiality are not active.

You could also create a copy of /proc/kcore much like you could of /proc/vmcore, however this isn’t terribly common. Typical use of /proc/kcore is for live debugging, while /proc/vmcore is normally saved for later inspection.
Makedumpfile

Both /proc/vmcore and /proc/kcore are direct representations of the memory space of a kernel, using the ELF format. This means that the files take up roughly the same amount of space as the physical memory in use by the kernel. On a laptop with 8GiB or 16GiB of memory, that may not be too bad, but for servers with 100s of GiBs or even TiBs of memory, that’s just not acceptable.

The makedumpfile tool is used to create a much smaller dump file, using two main strategies. First, it can omit data in memory that may not be useful for debugging the kernel (e.g. memory filled with zero’s, free memory, userspace memory, etc). Second, for the data that is included, it can compress each page of memory, if the output format supports it. Incidentally, it also supports the ability to “filter out” data for particular symbols or data structures, but this doesn’t reduce the data size: it simply redacts the data.

In a typical configuration, the kdump kernel is configured to use makedumpfile to save the /proc/vmcore file to disk or the network. You might expect this to take longer than simply copying the file to disk, but usually it’s faster: the slow part is writing the file to disk, and by omitting and compressing data, the time spent doing I/O is greatly decreased. So the end result is a vmcore which is quicker to generate, and takes up much less space than the original /proc/vmcore file would have. It’s less common, but perfectly valid to use makedumpfile to create a compressed dump of your currently-running kernel too, by running it on /proc/kcore instead. Makedumpfile also supports running against already-created core dump files, in order to re-filter them or convert the format.
Hypervisors

So far, all of the core dumps we’ve discussed came from Linux’s /proc/kcore or /proc/vmcore, with a possible intermediate step through makedumpfile. However, the Linux kernel is frequently run as a virtual machine guest, and in those cases, a hypervisor is responsible for managing the VM’s memory. It’s entirely possible for a hypervisor to create a core dump itself, by pausing the execution of the VM (to ensure consistency), and then saving a copy of that memory and the CPU state. This may be necessary in rare cases where the VM guest is unresponsive for some reason. If the normal means of triggering a panic & kdump within the guest OS are unsuccessful, a hypervisor core dump could provide you the necessary information to resolve the issue.

A current common example of a hypervisor creating a core dump would be QEMU, which supports a dump-guest-memory QMP/HMP command, which is frequently used via libvirt’s virsh dump command. You can also create memory dumps with Hyper-V, Xen, and other hypervisors.

Some hypervisors, such as Hyper-V, have their own custom dump formats. Others, like QEMU, support a range of formats. And naturally, each hypervisor tends to create its core dump format using its own implementation and quirks. For example, ELF vmcores generated by QEMU will look different than the ELF /proc/vmcore, which itself even looks different than the /proc/kcore file. As a result, it’s not always enough to simply know what format a vmcore is in: you may also need to know what created it.
Others

While /proc/vmcore, /proc/kcore, and hypervisors are certainly the most common sources of core dumps we see, this list is by no means exhaustive. Some server vendors provide firmware-based diagnostics and dumping mechanisms, which are similar in principle to hypervisor core dumps: the firmware takes the place of the hypervisor, halting the machine and saving elements of physical memory. These vendor-specific solutions will vary widely in capability, quality, and their output formats. And even outside of these solutions, there are other application-specific solutions (e.g. those which are better suited for embedded devices). And of course, there are historical systems & formats which are mostly unused today.

For the purpose of this article, we can’t cover all of those, so we’ll stick to the world of Linux ELF vmcores, makedumpfile, and hypervisors. These are the ones we see most commonly with Oracle Linux.
Data contained in core dumps

As we’ve seen, there’s quite a diversity of tools which can be used to create a kernel core dump. But they’re all trying to achieve the same end goal: provide enough data that an analysis tool can understand the dump, and allow a user to analyze the information. The end goal is to allow that user to debug whatever issue led to the core dump being created in the first place.

The main information provided by the core dump is, of course, the contents of memory. However, on its own, the memory contents are not enough information for tools to interpret meaningful information such as variable values, stack traces, log buffer contents, etc. Tools will need additional information to properly interpret a core dump:

    Probably the most important information is the exact kernel release as reported by uname -r, for example: 5.15.0-104.119.4.2.el9uek.x86_64. This string usually identifies a specific kernel build released by your distribution, and it is generally enough to identify the specific *-debuginfo package which applies to that kernel. Increasingly, debuggers are relying on a special value called the “build ID” which is unique to each build and can also be used to find debugging information, so the build ID may be an important metadata as well. The debugging information typically contains an ELF symbol table as well as DWARF information that describes variables, types, and much more, allowing debuggers to introspect data structures.
    Another fundamental requirement is to know the architecture that the core dump came from. This obviously includes the architecture name (x86_64, aarch64, etc.), but it may also include details such as page size, word size, or endianness that could be left unspecified by the architecture.
    The kernel is responsible for managing memory, and that includes maintaining the page tables. The vast majority of the kernel works with virtual addresses, and so debugging tools need to understand those virtual address mappings. Some core dump formats can represent the virtual address space as part of their memory encoding. In other cases, the core dump simply represents physical memory addresses, and leaves the debugger to find and interpret the page tables via other metadata. In Linux, the symbol swapper_pg_dir refers to the virtual address of the root page table. With some architecture specific metadata to translate that virtual address to its corresponding physical address, and with page table support for the architecture, a debugger can traverse these tables and understand the virtual address mappings.
    Most kernel configurations randomize at boot time the physical base address at which the kernel is loaded, as well as the virtual memory addresses where the kernel is mapped. These are different forms of so-called KASLR, or Kernel Address Space Layout Randomization. While it’s possible to search for well-known data in a core dump in order to “break” the KASLR, this takes a long time and can be error-prone, so it’s important for the dump to contain these KASLR offsets.
    Finally, the values of the registers for each CPU are very important. These include the program counter & stack pointer, which are crucial for creating an accurate stack trace for the code executing on a particular CPU. These are typically represented in an architecture-specific ELF note called NT_PRSTATUS.

These are just the low-level metadata that a debugger would need in order to interpret variables and data structures in memory, as well as unwind the stacks of active tasks. However, there are other kinds of metadata which need to be considered. For example, makedumpfile, which we’ve already discussed, has the ability to omit certain kinds of memory pages from its output. In order to do that, makedumpfile needs to be able to understand the kernel’s array of page frames. It can use debuginfo for this, but in practice, debuginfo is rarely available at runtime. So makedumpfile can use additional metadata that declares the sizes and member offsets of certain structures, as well as addresses of certain essential symbols. Other tools directly use the vmcore (without debuginfo) to extract the kernel log buffer, and so metadata for symbols and types related to the log buffer are also frequently needed.

All told, there’s quite a bit of metadata that’s necessary to interpret the kernel core dump. Some of this metadata is represented by the dump format intrinsically (e.g. ELF contains the architecture information in its header, and can hold virtual address mappings in the program headers). However, many of the Linux-specific details, such as page table locations, KASLR offsets, kernel release, and more are contained in a special piece of data called the “vmcoreinfo”.

The vmcoreinfo is text formatted, key=value block of data. Normally, a page of memory is allocated at startup by the Linux kernel, and the data is formatted and written into this page. The page is never overwritten or deallocated, so it’s always available – if you can find it. In the event of a panic, the kernel includes the already-created vmcoreinfo into the ELF /proc/kcore and /proc/vmcore files as an ELF note. This makes it easy to explore on a running system: simply run eu-readelf -n /proc/kcore as root and look for the “VMCOREINFO” note and its data. Much of the above metadata is represented as keys in this text, for example:

    OSRELEASE - the kernel release
    BUILD-ID - the build ID of the kernel (vmlinux)
    PAGESIZE - physical page frame size
    KERNELOFFSET - this is x86_64 specific, but it contains the KASLR offset
    SYMBOL(swapper_pg_dir) - the root page table

The strength of the vmcoreinfo is that it is text-based, so it’s easy to extend with new information that wasn’t anticipated, unlike binary file formats. Another benefit is that it’s included in kernel memory anyway, so it’s almost always present somewhere in a core dump, if you’re willing to search for it. The downside is that core dump formats need to be aware of it and either include a copy, or include a pointer to it. If that copy or pointer is lost, or if the tool which created the dump never knew where it was (e.g. hypervisors), then it can be difficult or impossible to use with tools like Crash and Drgn.
Core dump formats

Now that we know the diversity of core dump producers, and that there is a large amount of metadata that needs to be well represented in a core dump, we’re ready to tackle an incomplete list of the more common core dump formats. In each section, we’ll show how to identify the format (frequently, by using the file utility, but sometimes using a hex dumper like xxd). We’ll also describe the benefits and drawbacks, along with the common producers and consumers.
ELF

The Executable and Linkable Format (ELF) is ubiquitous in the Linux world. Most programs are in ELF, as are the intermediate outputs of the compiler, and also the core dumps of userspace programs. In order to suit that diversity of use cases, ELF has to be very flexible.

ELF is essentially composed of four parts:

    The ELF header
    The section headers (optional)
    The program headers (optional)
    Data

The ELF header points to section & program headers, and gives some basic information about the architecture. The section & program headers are optional (but at least one needs to be included), and they define regions of the data and provide metadata about them. The section headers are intended to be more useful for a compiler or linker: they define a series of “sections” of data in the file, all of which are named and have different types and some flags. The program headers, on the other hand, define “segments”, which are intended to be used while executing a program. Each entry describes which parts of the file should be loaded into memory at what address, with what permissions. It’s like a recipe for creating a process image in memory.

ELF is typically used to represent core dumps using the program headers. Rather than describing how a loader should create the program in memory, a core dump contains the memory contents of the program, and the program headers describe where the contents were mapped at the time of the crash. The Linux Kernel creates userspace core dumps using the ELF format in this manner, and so it’s not surprising that /proc/kcore and /proc/vmcore are also represented as ELF files with memory regions described by program headers.

But how is the metadata handled? Some of it is included in the ELF header, especially the architecture. The remainder is usually stored in ELF “notes”. Program headers and section headers can both declare a section of the file as containing specially-formatted “note” data that can have arbitrary contents. The following notes are contained in vmcore/kcore files generated by Linux:

    Notes of type PRSTATUS, which contain the registers for every CPU. (The /proc/kcore file only contains the PRSTATUS for the running CPU. Getting the data for other CPUs would require sending inter-processor-interrupts to collect the data, and by the time it was returned to userspace, the data would be stale anyway.)
    A note of type PRPSINFO can contain the kernel command line
    A note of type VMCOREINFO will contain the vmcoreinfo note with the metadata described above.

Detecting ELF Core Dumps

You can tell a file is an ELF core dump via the file command. For example:
Copy code snippet
Copied to Clipboard
Error: Could not Copy
Copied to Clipboard
Error: Could not Copy

$ sudo file /proc/kcore
/proc/kcore: ELF 64-bit LSB core file, x86-64, version 1 (SYSV), SVR4-style, from 'BOOT_IMAGE=(hd0,gpt3)/vmlinuz-5.14.0-284.25.1.0.1.el9_2.x86_64 root=/dev/mapper'

You can also view the program headers that define the core dump memory via eu-readelf -l FILE, and you can see the notes (with contents, if possible) via eu-readelf -n FILE. For example, here are the program headers for a /proc/kcore and a /proc/vmcore respectively:
Copy code snippet
Copied to Clipboard
Error: Could not Copy
Copied to Clipboard
Error: Could not Copy

sh-5.1# readelf -l /proc/kcore

Elf file type is CORE (Core file)
Entry point 0x0
There are 9 program headers, starting at offset 64

Program Headers:
  Type           Offset             VirtAddr           PhysAddr
                 FileSiz            MemSiz              Flags  Align
  NOTE           0x0000000000000238 0x0000000000000000 0x0000000000000000
                 0x0000000000001b60 0x0000000000000000         0x0
  LOAD           0x00007fffff602000 0xffffffffff600000 0xffffffffffffffff
                 0x0000000000001000 0x0000000000001000  RWE    0x1000
  LOAD           0x00007fff81002000 0xffffffff81000000 0x000000007d600000
                 0x0000000001630000 0x0000000001630000  RWE    0x1000
  LOAD           0x0000490000002000 0xffffc90000000000 0xffffffffffffffff
                 0x00001fffffffffff 0x00001fffffffffff  RWE    0x1000
  LOAD           0x00007fffc0002000 0xffffffffc0000000 0xffffffffffffffff
                 0x000000003f000000 0x000000003f000000  RWE    0x1000
  LOAD           0x0000088000003000 0xffff888000001000 0x0000000000001000
                 0x000000000009e000 0x000000000009e000  RWE    0x1000
  LOAD           0x00006a0000002000 0xffffea0000000000 0xffffffffffffffff
                 0x0000000000003000 0x0000000000003000  RWE    0x1000
  LOAD           0x000008806f003000 0xffff88806f001000 0x000000006f001000
                 0x000000000ffff000 0x000000000ffff000  RWE    0x1000
  LOAD           0x00006a0001bc2000 0xffffea0001bc0000 0xffffffffffffffff
                 0x0000000000400000 0x0000000000400000  RWE    0x1000
sh-5.1# readelf -l /proc/vmcore

Elf file type is CORE (Core file)
Entry point 0x0
There are 4 program headers, starting at offset 64

Program Headers:
  Type           Offset             VirtAddr           PhysAddr
                 FileSiz            MemSiz              Flags  Align
  NOTE           0x0000000000001000 0x0000000000000000 0x0000000000000000
                 0x0000000000001700 0x0000000000001700         0x0
  LOAD           0x0000000000003000 0xffffffff88600000 0x0000000056200000
                 0x0000000001630000 0x0000000001630000  RWE    0x0
  LOAD           0x0000000001633000 0xffff9a7ec0100000 0x0000000000100000
                 0x000000006ef00000 0x000000006ef00000  RWE    0x0
  LOAD           0x0000000070533000 0xffff9a7f3f000000 0x000000007f000000
                 0x0000000000fe0000 0x0000000000fe0000  RWE    0x0

You can see the /proc/kcore file contains many program header entries: this is because it is representing several virtual address regions, like the kernel’s vmalloc and vmemmap regions, in addition to the kernel’s mapping of code, and the kernel’s direct mapping of physical memory.

By comparison, /proc/vmcore contains far fewer headers. It doesn’t represent vmalloc or vmemmap regions, because the currently running kernel doesn’t have enough information to find them.
Benefits & Drawbacks of ELF Core Dumps

Since ELF is such a widespread, enduring standard, there are very many tools that support it. As we’ve seen above, readelf and its elfutils-based variant, eu-readelf can both show detailed information related to the file format, without the need for a user to read or write code to analyze it. Further, ELF is one of the only formats that “normal debuggers” (i.e. those which don’t specialize on the Linux kernel) can use.

However, ELF has a few drawbacks. Much of the data in a kernel core dump is not useful for debugging, and can or should be omitted. The ELF program header, which is used to define the memory of the core dump, can be used to help omit this data: segments can be split up so that the gaps between them exclude unneeded data. Unfortunately, this comes at a cost: each additional segment requires an entry of 56 bytes (for a 64-bit ELF file), and the number of program headers is limited by the size of the header field e_phnum, a 16-bit integer whose max value is 65,535, which is a limit that could conceivably be hit by systems with large amounts of memory. This limit can be exceeded (search for PN_XNUM in elf(5)), but it is a bit clunky. Despite these oddities, makedumpfile -E does support outputting ELF-formatted vmcores with pages excluded in exactly this way. It’s worth trying this out for yourself, if you want to explore: makedumpfile -E -d 31 /proc/kcore my_dump.elf would create such a file.

The far more important limitation is the ELF program header does not define any field or flag to indicate compression for a segment. So there is no broadly compatible way for an ELF vmcore to include compressed data. Of course, it is possible to compress the entire ELF file, but this may not be practical due to memory or CPU constraints.
Variant: QEMU ELF

Not all ELF kernel core dumps are created by the kernel or makedumpfile, though. QEMU (and thus, virsh dump) has the option to create an ELF-formatted vmcore. It uses mostly the same format as the Linux kernel, but it does include some extra QEMU-specific data for each CPU, in addition to the PRSTATUS note. It also includes a nearly empty section header table containing no information of value.

The main concern with QEMU’s ELF core dumps is whether they contain the vmcoreinfo note. Hypervisors may have access to the guest memory, but they don’t have any general-purpose way to see the vmcoreinfo data that the kernel has prepared at boot time. This means that, unless you’ve explicitly configured it, vmcores generated by QEMU won’t have a vmcoreinfo note. Thankfully, QEMU implements a virtualized “vmcoreinfo” device. The guest Linux kernel can detect its presence at boot and write its vmcoreinfo data into this device once it is ready. The QEMU hypervisor can then store this data alongside the virtual machine, in case it must later create a vmcore. If you’ve run QEMU with -device vmcoreinfo and you have a properly configured kernel, then QEMU will include that data into its core dumps.

It’s worth noting that, even if the ELF core dump doesn’t contain the vmcoreinfo as an ELF note, the data is still there buried in the core dump. With the right tools, you can search through the memory contents and find it.
More Variation: Virtual Addresses in ELF

As we’ve seen, ELF program headers are a quite flexible way of storing memory metadata. They allow each memory segment to be assigned a virtual and physical memory address. This means that an ELF core dump can represent the kernel’s virtual address space as well as its physical address space. For /proc/kcore and /proc/vmcore, the kernel does exactly that, since it already knows its own virtual address mappings. When creating ELF vmcores, makedumpfile will also populate the virtual address field according to the kernel’s virtual address mappings.

However, hypervisor or firmware level vmcores tend not to include the virtual address information in core dumps. While the memory mapping information is available to a hypervisor, it may not be easily accessible. For example, KVM-based hypervisors delegate page table management to processor virtualization extensions wherever possible, so the hypervisor itself may not even know the current guest page tables. Parsing the page tables from the guest memory would be expensive and could result in a denial of service if a malicious guest crafted a very large set of page tables. So hypervisors tend to avoid that complexity, and they’ll only include the physical addresses for memory. QEMU does have the support for including virtual address information, but it must be explicitly enabled (e.g. dump-guest-memory -p).

When hypervisors don’t have virtual address information available, the behavior is not well-defined. Some, like QEMU, include a fake virtual address (the same value as the physical address). This can make it difficult for a debugger to detect whether a vmcore really has accurate virtual memory mappings.
Kdump-compressed Format

While ELF may be a ubiquitous file format for programs, libraries, and userspace core dumps, the kdump-compressed format is by far the most common format used by our customers for kernel core dumps. A default Oracle Linux installation with kdump enabled will use makedumpfile to create core dumps in kdump-compressed format, which offers several advantages over ELF (the main one being compression, which reduces file size significantly). However, this format wasn’t always ubiquitous, nor did it originate with makedumpfile.

In the dark days prior to kexec being a viable way to make crash dumps, there were a few projects that contained out-of-tree Linux kernel patches to enable core dumps, as well as utilities to use these dumps. Your author was not of an age to pay attention at the time that these systems were at their zenith, so you’ll be spared the history lesson. One such project was lkdump, in whose diskdumputils-0.1.5.tar.bz2 distribution you can see an early definition of a file format called diskdump, in dumpheader.h. While this project seems to have died off, the format seems to be the predecessor to the one currently in use by makedumpfile.

For the purposes of this description, we’ll call the dump format “kdump-compressed”, though in truth, it goes by many names. This is because there is no single standard, except what’s in makedumpfile’s source code. You would be forgiven for calling it a modified “diskdump” format, as that is what makedumpfile’s own code seems to refer to it as. However, the file utility, as well as makedumpfile(8), refer to it as kdump-compressed, so we’ll try to use that name too. You can see how recent versions of file recognize the format below:
Copy code snippet
Copied to Clipboard
Error: Could not Copy
Copied to Clipboard
Error: Could not Copy

$ file vmcore
vmcore: Kdump compressed dump v6, system Linux, node stepbren-ol9.local, release 5.14.0-284.25.1.0.1.el9_2.x86_64, version #1 SMP PREEMPT_DYNAMIC Fri Aug 4 09:00:16 PDT 2023, machine x86_64, domain (none)

Leaving aside matters of naming and provenance, the kdump-compressed format is primarily output by makedumpfile (though QEMU can output a variant of the format), and it is primarily read by crash, although the libkdumpfile library is a powerful option for consuming it as well. The strength of the format is in its ability include or omit any page, as well as compress each page. A typical kdump-compressed vmcore can be just a fraction of the size of physical memory, though it will depend on the memory usage of the system, as well as the dump level and compression arguments passed to makedumpfile. This makes it ideal for use when a vmcore needs to be sent to remote engineers for analysis.

The downside of the kdump-compressed format, of course, is that it is quite niche. Any kernel-specific debugger, such as Crash and Drgn, can understand it, so in the common case, there aren’t many compatibility issues. However, in case that you’re having trouble opening a kdump-compressed vmcore (e.g. due to implementation bugs, corruption, etc), you’ll find fewer tools available to help you. There is no readelf equivalent for kdump-compressed files. You’ll likely find yourself writing your own tools to help in diagnosing these issues, possibly with the help of libkdumpfile. In fact, the examples directory in the libkdumpfile project contains some genuinely useful programs for investigating issues.

As shown above, recent versions of the file tool do a good job of identifying this format. However, older versions may unhelpfully report only: data. You can manually look for the file header using xxd:
Copy code snippet
Copied to Clipboard
Error: Could not Copy
Copied to Clipboard
Error: Could not Copy

$ xxd ./vmcore | head -n 4
00000000: 4b44 554d 5020 2020 0600 0000 4c69 6e75  KDUMP   ....Linu
00000010: 7800 0000 0000 0000 0000 0000 0000 0000  x...............
00000020: 0000 0000 0000 0000 0000 0000 0000 0000  ................
00000030: 0000 0000 0000 0000 0000 0000 0000 0000  ................

If the first 8 bytes are "KDUMP " (that is, KDUMP followed by three spaces), then you can be confident that you’re looking at a kdump-compressed vmcore. If the first 8 bytes are "DISKDUMP", then you have a quite old core dump from the old diskdump utilities. If, however, the first 12 bytes are "makedumpfile", then you should continue reading, as this is a “flattened” kdump file, discussed below.
Kdump Format Details

The modern kdump-compressed format manages to achieve impressive compactness through a few optimizations. First, it uses a bitmap to represent the availability of any given page. This is quite efficient for the common cases, where there is a mix of pages included and excluded. For the common page size of 4096 bytes, each gigabyte of physical memory requires 32KiB of bitmap space in the output file, regardless of whether any of that memory is actually included in the output file.

In addition to this bitmap, the format maintains an array of 24-byte descriptors, one for each page included in the dump. The descriptor contains information about compression, size, and location of the data in the dump. It also reserves 8 bytes within this descriptor for a field called page_flags, which seems intended to be populated from the corresponding struct page in the kernel. From quick inspection, neither crash nor libkdumpfile, the two major consumers of the format, use this field. So there is always room for improvement.

To store metadata, the format uses an interesting trick. Rather than define its own format to store PRSTATUS or VMCOREINFO notes, the latest version (v6) of the format simply reuses the ELF Note structures & formats. This is a major advantage, since it allows any defined ELF note type (or custom notes) to be inserted into the vmcore, and it allows sharing code in systems which may output either ELF or kdump-compressed files.

Finally, the kdump-compressed format is not designed with any particular mechanism for encoding the kernel’s virtual address mappings. Instead, consumers like crash and kdump are expected to use architecture-specific code and VMCOREINFO metadata to find and interpret the page tables.
Variant: flattened format

To properly write the kdump-compressed format, the core dump creator (typically makedumpfile) needs to seek between two locations in the output file: the header area, which contains the bitmaps and page descriptors, and the data area, where the actual compressed page contents are written out. Since the compressed size of pages cannot be known ahead of time, the descriptors can’t be written before the pages. The descriptors need to be written near the beginning of the file to serve as an index for the variable-size page data. While seeking between these locations is no problem for conventional files, it is impossible to do this if you would like to output the core dump on stdout, or transmit it via a network socket: pipes and sockets do not support lseek(2).

To resolve this issue, makedumpfile introduced the “flattened” variant of the kdump-compressed format. Instead of using lseek(fd, SEEK_SET, offset) to go to a particular offset, and then write(fd, data, size), makedumpfile simply writes offset and size out, followed by the data. Before reading the data, a “reassembly” phase is necessary, which simply reads each offset + size record, and follows the instructions to create the final output file.

This format has a header which starts the 12 bytes: “makedumpfile”, so it can be recognized quite easily with xxd as shown above.

There is only one advantage to this format: it’s the most practical way to output the kdump-compressed format to a socket or pipe. However, once the core dump is saved, there is no point in using the flattened format! Flattened vmcores can be “reassembled” into a normal kdump-compressed vmcore using the command makedumpfile -R, or makedumpfile-R.pl.

For convenience, some analysis tools support directly reading the flattened variant of the format: Crash, starting with version 5.1.2; libkdumpfile, starting with 0.5.3; and Drgn, starting with 0.0.25 (alongside capable libkdumpfile). However, this support comes at a cost, which is frequently not advertised to the user. In order to directly read a flattened vmcore, these tools must first build an index of what the reassembled file should look like. This process is time-consuming for large vmcores, and as of this writing, no implementation saves the resulting index, so it needs to be recomputed each time the file is opened. Frequently, tools like Crash don’t inform the user about what the indexing process is, nor do they inform the user that they could avoid the indexing phase by simply using makedumpfile -R one time only to generate a standard vmcore.

To add even more confusion to the situation, QEMU’s default “kdump” output setting creates a flattened vmcore, not a standard kdump-compressed vmcore. This was done for much the same reason as makedumpfile: to allow outputting core dumps to pipe file descriptors. But unlike makedumpfile, which falls back to the flattened format only when lseek() fails, QEMU versions prior to 8.2 simply only implement the flattened format. Since QEMU 8.2, a new output setting called “kdump-raw” been added, which corresponds to the standard kdump format. Since it is opt-in, users are forced to know the difference between the flattened and standard formats. Users who don’t know it may end up with a vmcore incompatible with their tools, or which is painfully slow to analyze.

What’s worse, it’s possible for QEMU to omit the VMCOREINFO note from its vmcores, as we’ve discussed already. In the unhappy case where a flattened vmcore is produced without a VMCOREINFO note, the resulting file is nearly impossible to load in Crash, but Crash will spend a good deal of time trying to open it prior to failing. Your author has witnessed many well-qualified engineers give up on these sorts of core dumps, unaware of the subtleties of these formats, and unaware that none of the obstacles are insurmountable.
Xen ELF

While we have already covered ELF formatted vmcores, the ELF format used by the Xen hypervisor really deserves its own category. A core dump created by running xm dump-core from the dom0 (hypervisor) results in an ELF file, on which file will cheerfully report:
Copy code snippet
Copied to Clipboard
Error: Could not Copy
Copied to Clipboard
Error: Could not Copy

$ file xendump
xendump: ELF 64-bit LSB core file, x86-64, version 1 (SYSV), no program header

This all seems fine, except for that last bit: “no program header”. As we saw previously, the ELF format for vmcores used by Linux and QEMU use the program headers to describe the memory segments and where they belong in memory. ELF section headers don’t have fields to represent this sort of information. Let’s take a look at the ELF sections that are present:
Copy code snippet
Copied to Clipboard
Error: Could not Copy
Copied to Clipboard
Error: Could not Copy

$ readelf -S xendump
There are 7 section headers, starting at offset 0x40:

Section Headers:
  [Nr] Name              Type             Address           Offset
       Size              EntSize          Flags  Link  Info  Align
  [ 0]                   NULL             0000000000000000  00000000
       0000000000000000  0000000000000000           0     0     0
  [ 1] .shstrtab         STRTAB           0000000000000000  100bfdfc0
       0000000000000048  0000000000000000           0     0     0
  [ 2] .note.Xen         NOTE             0000000000000000  00000200
       0000000000000568  0000000000000000           0     0     0
  [ 3] .xen_prstatus     PROGBITS         0000000000000000  00000768
       0000000000002860  0000000000001430           0     0     8
  [ 4] .xen_shared_info  PROGBITS         0000000000000000  00002fc8
       0000000000001000  0000000000001000           0     0     8
  [ 5] .xen_pages        PROGBITS         0000000000000000  00004000
       00000001003f8000  0000000000001000           0     0     4096
  [ 6] .xen_pfn          PROGBITS         0000000000000000  1003fc000
       0000000000801fc0  0000000000000008           0     0     8

There are some immediately recognizable items here. .note.Xen is a NOTE section, so it probably contains some metadata about the hypervisor. The .xen_prstatus section shares a name with the PRSTATUS notes we saw in previous core dumps, so it almost certainly contains the register state of each CPU. For the remainder, we can refer to a greatly appreciated documentation file turned up by a web search. To quote:
Copy code snippet
Copied to Clipboard
Error: Could not Copy
Copied to Clipboard
Error: Could not Copy

".xen_pfn" section
        name            ".xen_pfn"
        type            SHT_PROGBITS
        structure       array of uint64_t
        description
                This elements represents the frame number of the page
                in .xen_pages section.
                The size of arrays is stored in xch_nr_pages member of header
                note descriptor in .note.Xen note section.
                The entries are stored in ascending order.
                The value, ~(uint64_t)0, means invalid pfn and the
                corresponding page has zero. There might exist invalid
                pfn's at the end part of this array.
                This section must exist when the domain is auto translated
                physmap mode. Currently x86 full virtualized domain and
                ia64 domain.
[...]
".xen_pages" section
        name            ".xen_pages"
        type            SHT_PROGBITS
        structure       array of page where page is page size byte array
        description
                This section includes the contents of pages.
                The corresponding address is described in .xen_p2m section
                or .xen_pfn section.
                The page size is stored in xch_page_size member of header note
                descriptor in .note.Xen section.
                The array size is stored in xch_nr_pages member of header note
                descriptor in .note.Xen section.
                This section must exist.

So, the .xen_pages section contains the actual memory data (which makes sense, as it is the largest), and the .xen_pfn section provides the PFN (roughly the same as the address) of each page. This also gives some info on the note contents: they are providing some of that critical metadata like page sizes.

All this is to say that, while the Xen core dump may be in the ELF format, it is not at all similar to the ELF vmcores we’ve seen before. This is both a testament to, and a consequence of, ELF’s incredible flexibility. It’s worth noting that, thanks to the .xen_pfn section, it seems like it’s possible for this format to exclude pages from the vmcore, much like makedumpfile does with the kdump-compressed format. However, it’s unlikely that Xen actually uses this capability, since deciding which pages should be excluded is difficult for a hypervisor. On the other hand, unlike the kdump-compressed format, this format still cannot support per-page compression: the metadata is designed with the expectation that each page in the .xen_pages takes up the full PAGE_SIZE bytes.

Without more experience with the format, it’s difficult to say what its advantages are (beyond being the format available if you’re using Xen). It could be that the various metadata can provide insight into the hypervisor state as well. However, the disadvantages here are clear. Though ELF tools will analyze it, the non-standard use of application-specific sections instead of program headers precludes most standard debugging tools from understanding it. Of course, this does not apply to Crash, which does fully support Xen vmcores.
Other formats

Like we’ve already said, this listing of kernel core dump formats is mostly incomplete. Simply browsing the code in Crash which is responsible for identifying the core dump format is overwhelming, as it reveals several others which aren’t even mentioned here:

    A few additional Xen-specific formats
    A “kvmdump” format which seems to be obsolete now
    An “sadump” format which seems related to a BIOS dump capability on some Fujitsu servers
    Dump formats for the netdump and LKCD projects which predate kdump
    Some formats for VMWare, as well as one related to Mission Critical Linux.

And these are just the formats Crash supports! Your author has also had the pleasure of using Hyper-V to create a virtual machine snapshot, and then using the vm2core tool provided by azure-linux-utils to create an ELF file which was marginally useful (with some tweaking) with Crash. Surely there are even more exotic formats yet to be found.
Core dump consumers

Throughout this article, we’ve mentioned a few tools which can be used to analyze Linux vmcores, namely: GDB, Crash, libkdumpfile, and drgn. In this section, we’ll briefly give some pros and cons of each tool, especially as they relate to their supported input formats and analysis capabilities.
GDB

GDB needs no introduction, as the GNU project’s very well-known general purpose debugger. GDB is of course capable of debugging running processes, but can also support ELF formatted core dumps, typically by running:
Copy code snippet
Copied to Clipboard
Error: Could not Copy
Copied to Clipboard
Error: Could not Copy

$ gdb EXECUTABLE
...
(gdb) core-file CORE

It is capable of debugging some Linux core dumps or /proc/kcore, and in fact the Linux kernel contains a set of scripts which could be used for debugging the kernel with GDB. However, this solution is limited in a number of ways. GDB only supports standard ELF core dumps, and it relies on the virtual addresses in the program headers to do address translations. If the program headers are missing for certain ranges, or if the ELF vmcore was from a source that didn’t include the virtual address translations, then GDB won’t be able to understand the core.

It also seems that GDB may not support kernels with KASLR enabled, but further research is needed to confirm that this is still a relevant concern. Needless to say, while GDB is a quite powerful debugger, it’s not designed for the kernel, and so it’s not regularly used for it.
Crash

The Crash utility can be thought of as a successor to the kernel’s GDB scripts. It wraps a GDB process and implements support for a broad variety of core dump formats, as well as details like page table translation for various architectures. Users can run common GDB commands or several quite useful kernel-specific helpers. The PyKdump framework can be used to further extend this system with Python scripting.

Crash is able to combine the power of GDB with support for almost every core dump format under the sun, which makes it a quite impressive debugging tool. It sets the standard for interactive kernel core dump debuggers.
libkdumpfile

Unlike Crash and GDB, libkdumpfile is not a debugger, per-se. Instead, it is a library which implements the ability to read many different types of core dumps, including ELF and kdump-compressed formats. Its bundled library, libaddrxlat, implements the details of address translation for a variety of architectures. You can write simple applications to read data out of a vmcore, and by using this library, you’ll find it shocking how many core dump formats and architectures your tools will support.
drgn

Drgn is a kernel (and userspace) debugger as a Python library. Rather than using debugger commands, it allows users to write Python code that treats the kernel’s data structures like regular Python objects. It supports standard ELF core dumps (and the running kernel) natively, and it relies on libkdumpfile to understand other formats, like kdump-compressed files. Its support for more esoteric core dumps (e.g. hypervisor core dumps which may be missing metadata) is behind alternatives like Crash, but it is improving.
Other Tools

When presented with a core dump that is difficult to understand, you may want to revisit some of the tools used throughout this article to better understand it:

    file is a good first step! Be sure to use the most recent possible version, as its detection is constantly changing.
    readelf and eu-readelf (from binutils and elfutils, respectively) provide critical tools for examining ELF files:
        -l for viewing program headers
        -S for viewing section headers
        -n for viewing notes. Prefer eu-readelf here, because it supports printing the contents of some note types, like build IDs and VMCOREINFO.
    xxd is quite useful for viewing data headers, though any hex dumper will do.
    Some more specific needs may require some scripts, or even a libkdumpfile based tool. You can find some example tools in the libkdumpfile examples directory, as well as some of my own tools here.
    Finally, some of the best diagnostic information when analyzing a core dump can come from reading the code of tools designed to generate or read them. To that end, here are some links to a few relevant portions of several important projects:
        Linux: kcore.c implements /proc/kcore
        Makedumpfile: diskdump_mod.h contains the definition of kdump-compressed format
        QEMU: dump.c contains QEMU’s implementation of creating ELF and kdump-compressed vmcores.
        Crash: diskdump.c contains the implementation of reading kdump-compressed files. However, there are lots of other relevant files for different formats and variants.
        libkdumpfile: diskdump.c contains implementations related to kdump-compressed vmcores, and elfdump.c contains implementations related to ELF

Conclusion

Kernel core dumps are complex. They are not simply copies of system memory; they contain plenty of extra metadata which is critical to understanding their contents. And like any other type of data, the design of the file formats can enable lots of flexibility and power. However, due to the broad variety of tools out there, the diversity of dump formats is overwhelming, and the lack of documentation or specifications compounds the problem. While the ecosystem, and especially high quality tools like Crash, makedumpfile, libkdumpfile, and drgn, generally work very well together, there are still compatibility issues that can be difficult to work around. Hopefully this guide can provide the first step in understanding these issues, so that you can be better equipped to fix your vmcores in the future.

If you’ve made it this far in this article, then your interest in debugging the kernel is quite impressive. Your feedback, experiences, questions, and contributions related to core dump formats and kernel debuggers are wanted! Please join us on the linux-debuggers kernel mailing list and share your experiences.

Link: https://blogs.oracle.com/linux/post/whats-inside-a-linux-kernel-core-dump
:LOGBOOK:
CLOCK: [2024-02-09 Fri 14:11]--[2024-02-09 Fri 14:14] =>  0:03
:END:
[2024-02-09 Fri 14:11]
* LLVM Highlights                                            :Technical:NOTE:

LLVM IR

Front-end developers only need to understand LLVM IR, its workings, and
invariants, making it easy to create new front ends for LLVM. Unlike other
compilers like GCC, LLVM IR is self-contained, eliminating the need to
manipulate complex data structures and global variables from other parts of the
compiler.

Modular Library-Based Design

The LLVM infrastructure consists of loosely coupled libraries instead of a
monolith, including the optimizer, allowing developers to choose and order
optimization passes for their specific needs. Only the necessary optimization
passes are linked into the final application, optimizing compile times and
avoiding unnecessary bloat

Retargetable Code Generator

The LLVM code generator transforms LLVM IR into target specific machine code. It
employs a modular approach with individual passes for instruction selection,
register allocation, scheduling, code layout optimization, and assembly
emission. This flexibility enables target-specific optimizations, such as
register pressure reduction for x86 and latency optimization for PowerPC,
without requiring a complete code generator rewrite

:LOGBOOK:
CLOCK: [2024-02-14 Wed 16:27]--[2024-02-14 Wed 16:34] =>  0:07
:END:
[2024-02-14 Wed 16:27]
* C Assembly Programming Paradigm Stanford University :Researching:Technical:NOTE:
:LOGBOOK:
CLOCK: [2024-05-05 Sun 08:22]--[2024-05-05 Sun 08:35] =>  0:13
:END:
[2024-05-05 Sun 08:22]
[[https://youtu.be/Ps8jOj7diA0?si=hPrnFI-DayQULebq][C,Assembly,C++,Concurrentprogramming Part-1]]
[[https://youtu.be/jTSvthW34GU?si=rxOvLjslOSgp6xCt][C,Assembly,C++,ConcurrentProgramming -Part-2]]
[[https://youtu.be/H4MQXBF6FN4?si=ESY-gAMtfsWhrJZo][C,Assembly,C++,ConcurrentProgramming -Part-3]]
[[https://youtu.be/_eR4rxnM7Lc?si=6VuFmOIrjvmpSqaR][C,Assembly,C++,ConcurrentProgramming --Part 4]]
[[https://youtu.be/73Z7gaAvovQ?si=vWMu9cpV1fWQ0Z2R][C,Assembly,C++,ConcurrentProgramming --Part 5]]
[[https://youtu.be/iyLNYXcEtWE?si=baex4dPaRrSNw0Sr][C,Assembly,C++,ConcurrentProgramming -Part 6]]
[[https://youtu.be/Yr1YnOVG-4g?si=M8Olbd5saLuetsyU][C,Assembly,C++,ConcurrentProgramming -Part 7]]
[[https://youtu.be/1nYDflSL0Mg?si=eiYROJUN1ZyT6Wo2][C,Assembly,C++,ConcurrentProgramming -Part 8]]
[[https://youtu.be/arjo2-JQeaY?si=wnZbexp-4S7dIXw][C,Assembly,C++,ConcurrentProgramming -Part 9]]
[[https://youtu.be/FvpxXmEG1F8?si=rNmwaQxAHzzCEfil][C,Assembly,C++,ConcurrentProgramming -Part 10]]
[[https://youtu.be/DwTXMjVkIUY?si=ccnW8TKN0f9KyhWM][C,Assembly,C++,ConcurrentProgramming -Part 11]]
[[https://youtu.be/0rXjvLa2NSs?si=No0Wsw2uXiVcOx-n][C,Assembly,C++,ConcurrentProgramming -Part 12]]
[[https://youtu.be/ucQI5HpiFrI?si=cNQtnC8b1fb1jZBO][C,Assembly,C++,ConcurrentProgramming -Part 13 ]]
[[https://youtu.be/TRfbJIsDBIM?si=Dhvaq0TXFFNXsJqG][C,Assembly,C++,ConcurrentProgramming -Part 14]]
[[https://youtu.be/omE3YYpHhLo?si=Sb3nuSFxE93tGVj6][C,Assembly,C++,ConcurrentProgramming -Part 15]]
[[https://youtu.be/OGHN_zVTMMo?si=yDd5QAKJpNPQaGV0][C,Assembly,C++,ConcurrentProgramming -Part 16]]
[[https://youtu.be/kF3eSQTFagQ?si=8klTSSG-z4AhWJDQ][C,Assembly,C++,ConcurrentProgramming -Part 17 --->Done]]
[[https://youtu.be/ynwh5O3jVRM?si=H8WpmDhqrsaKld10][C,Assembly,C++,ConcurrentProgramming -Part 18]]
[[https://youtu.be/_cV8NWQCxnE?si=XvPRY4FdfZFzeKnq][C,Assembly,C++,ConcurrentProgramming -Part 19]]
[[https://youtu.be/onKR7ICXacQ?si=EanYRToM_ZQEJdW2][C,Assembly,C++,ConcurrentProgramming -Part 20]]
[[https://youtu.be/omzSd3En5g4?si=Cj-eILFyPeA0z2go][C,Assembly,C++,ConcurrentProgramming -Part 21]]
[[https://youtu.be/3LeCydausnk?si=zUUIDQnfyNuAyvei][C,Assembly,C++,ConcurrentProgramming -Part 22]]
[[https://youtu.be/TJkH1CSHg44?si=7cvks0RW0WN3UHGA][C,Assembly,C++,ConcurrentProgramming -Part 23]]
[[https://youtu.be/_9XAlLofYwU?si=sEEkn4TmqDI9o2-k][C,Assembly,C++,ConcurrentProgramming -Part 24]]
[[https://youtu.be/V-5DCBQdErM?si=-b3MONCfzWY0JvPb][C,Assembly,C++,ConcurrentProgramming -Part 25]]
[[https://youtu.be/PrnRTwCaWz8?si=N3Z3QZ-No0kBf8Ho][C,Assembly,C++,ConcurrentProgramming -Part 26]]
[[https://youtu.be/cXY4fSA7DnM?si=Uziys9shqAfzD5HY][C,Assembly,C++,ConcurrentProgramming -Part 27]]
[[https://youtu.be/aIjM-UE1JDQ?si=fpbMxCMmdFAvM5bz][C,Assembly,C++,ConcurrentProgramming -Part 27-II]]
[[https://youtu.be/Kfc5O8hVzLQ?si=YtZAtqGsjkGBVkwY][C,Assembly,C++,ConcurrentProgramming -Part 28]]
* SPF,DKIM and DMARC explained                               :Technical:NOTE:

** SPF (Sender Policy Framework)

SPF: It's like a list of friends who can send emails for you. The SPF Record is
this list. If an email says it's from you but it's not sent by a friend on your
list, it's probably not really from you.

As the owner of a domain, you can use SPF to create a list of 'email friends' -
these are the mail servers that are allowed to send emails on your behalf. This
helps stop people who aren't your 'email friends' from pretending to be you. The
SPF Record, a DNS TXT record, is where you keep this list of 'email friends'.

The DNS TXT record for an SPF your 'email friends' typically looks like this:

v=spf1 ip4:123.123.123.123 ~all

Here's the command I usually run to fetch that:

dig TXT example.com

** DKIM (DomainKeys Identified Mail)

DKIM: It's like a secret note inside your emails. When you send an email, you
put a secret note inside. This note is made using a special secret code only you
know. When your email arrives, the receiver checks the secret note using a
public code that everyone knows. This public code is stored in a place called
the DKIM Record. If the secret note matches the public code, the email is really
from you and hasn't been changed. This helps stop bad people from pretending to
send emails from you or changing your emails.

This public code, also known as a public key, is stored in a DNS TXT record
known as the DKIM Record, which is accessible to everyone. It's like the decoder
for your secret code.

The DNS TXT record, where the public code (or public key) for DKIM is stored, typically looks like this:

v=DKIM1; k=rsa; p=NICfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDBolTXCqbxwoRBffyg2efs+Dtlc+CjxKz9grZGBaISRvN7EOZNoGDTyjbDIG8CnEK479niIL4rPAVriT54MhUZfC5UU4OFXTvOW8FWzk6++a0JzYu+FAwYnOQE9R8npKNOl2iDK/kheneVcD4IKCK7IhuWf8w4lnR6QEW3hpTsawIDAQ0B"

Here's the command I usually run to fetch that:

dig TXT selector1._domainkey.yourdomain.com

Note: Replace selector1 with your actual selector, and yourdomain.com with your
actual domain. This command will fetch the DNS TXT record where your public code
is stored.  DMARC (Domain-based Message Authentication, Reporting & Conformance)

** DMARC:

It's like the boss of SPF and DKIM. It takes the rules from SPF and DKIM
and makes a big rule book. This rule book tells everyone what to do if an email
from your domain doesn't follow the rules. For example, one rule could be to
send a report if an email doesn't pass the checks. The DMARC Record, a place
everyone can see, holds this rule book.

If an email passes the SPF and DKIM checks, the receiver then looks at the DMARC
rule book to decide what to do with the email. They might follow the rule to
send a report, or they might follow another rule depending on what your rule
book says.

DMARC allows domain owners to declare their rules in the rule book. This rule
book, stored in the DMARC Record, a DNS TXT record, specifies your DMARC
policies and how receivers should handle mail that violates these rules. If both
SPF and DKIM checks pass, the receiver then checks the DMARC rule book to decide
what to do with the email.

The DNS TXT record for DMARC 'rule book' typically looks like this:

v=DMARC1; p=none; rua=mailto:postmaster@example.com

Here's the command I usually run to fetch that:

dig _dmarc.example.com TXT

:LOGBOOK:
CLOCK: [2024-06-17 Mon 22:46]--[2024-06-17 Mon 22:47] =>  0:01
:END:
[2024-06-17 Mon 22:46]
* Vim Tips                                                   :Technical:NOTE:


Best of Vim Tips
David Rayner

__BEGIN__
------------------------------------------------------------------------------
" new items marked *N* , corrected items marked *C*
" searching
/joe/e                      : cursor set to End of match
3/joe/e+1                   : find 3rd joe cursor set to End of match plus 1 *C*
/joe/s-2                    : cursor set to Start of match minus 2
/joe/+3                     : find joe move cursor 3 lines down
/^joe.*fred.*bill/          : find joe AND fred AND Bill (Joe at start of line)
/^[A-J]/                    : search for lines beginning with one or more A-J
/begin\_.*end               : search over possible multiple lines
/fred\_s*joe/               : any whitespace including newline *C*
/fred\|joe                  : Search for FRED OR JOE
/.*fred\&.*joe              : Search for FRED AND JOE in any ORDER!
/\<fred\>/                  : search for fred but not alfred or frederick *C*
/\<\d\d\d\d\>               : Search for exactly 4 digit numbers
/\D\d\d\d\d\D               : Search for exactly 4 digit numbers
/\<\d\{4}\>                 : same thing
/\([^0-9]\|^\)%.*%          : Search for absence of a digit or beginning of line
" finding empty lines
/^\n\{3}                    : find 3 empty lines
/^str.*\nstr                : find 2 successive lines starting with str
/\(^str.*\n\)\{2}           : find 2 successive lines starting with str
" using rexexp memory in a search
/\(fred\).*\(joe\).*\2.*\1
" Repeating the Regexp (rather than what the Regexp finds)
/^\([^,]*,\)\{8}
" visual searching
:vmap // y/<C-R>"<CR>       : search for visually highlighted text
:vmap <silent> //    y/<C-R>=escape(@", '\\/.*$^~[]')<CR><CR> : with spec chars
" \zs and \ze regex delimiters :h /\zs
/<\zs[^>]*\ze>              : search for tag contents, ignoring chevrons
" zero-width :h /\@=
/<\@<=[^>]*>\@=             : search for tag contents, ignoring chevrons
/<\@<=\_[^>]*>\@=           : search for tags across possible multiple lines
" searching over multiple lines \_ means including newline
/<!--\_p\{-}-->                   : search for multiple line comments
/fred\_s*joe/                     : any whitespace including newline *C*
/bugs\(\_.\)*bunny                : bugs followed by bunny anywhere in file
:h \_                             : help
" search for declaration of subroutine/function under cursor
:nmap gx yiw/^\(sub\<bar>function\)\s\+<C-R>"<CR>
" multiple file search
:bufdo /searchstr/                : use :rewind to recommence search
" multiple file search better but cheating
:bufdo %s/searchstr/&/gic   : say n and then a to stop
" How to search for a URL without backslashing
?http://www.vim.org/        : (first) search BACKWARDS!!! clever huh!
" Specify what you are NOT searching for (vowels)
/\c\v([^aeiou]&\a){4}       : search for 4 consecutive consonants
/\%>20l\%<30lgoat           : Search for goat between lines 20 and 30 *N*
/^.\{-}home.\{-}\zshome/e   : match only the 2nd occurence in a line of "home" *N*
:%s/home.\{-}\zshome/alone  : Substitute only the occurrence of home in any line *N*
" find str but not on lines containing tongue
^\(.*tongue.*\)\@!.*nose.*$
\v^((tongue)@!.)*nose((tongue)@!.)*$
.*nose.*\&^\%(\%(tongue\)\@!.\)*$
:v/tongue/s/nose/&/gic
"----------------------------------------
"substitution
:%s/fred/joe/igc            : general substitute command
:%s//joe/igc                : Substitute what you last searched for *N*
:%s/~/sue/igc               : Substitute your last replacement string *N*
:%s/\r//g                   : Delete DOS returns ^M
" Is your Text File jumbled onto one line? use following
:%s/\r/\r/g                 : Turn DOS returns ^M into real returns
:%s=  *$==                  : delete end of line blanks
:%s= \+$==                  : Same thing
:%s#\s*\r\?$##              : Clean both trailing spaces AND DOS returns
:%s#\s*\r*$##               : same thing
" deleting empty lines
:%s/^\n\{3}//               : delete blocks of 3 empty lines
:%s/^\n\+/\r/               : compressing empty lines
:%s#<[^>]\+>##g             : delete html tags, leave text (non-greedy)
:%s#<\_.\{-1,}>##g          : delete html tags possibly multi-line (non-greedy)
:%s#.*\(\d\+hours\).*#\1#   : Delete all but memorised string (\1) *N*
%s#><\([^/]\)#>\r<\1#g      : split jumbled up XML file into one tag per line *N*
" VIM Power Substitute
:'a,'bg/fred/s/dick/joe/igc : VERY USEFUL
" duplicating columns
:%s= [^ ]\+$=&&=            : duplicate end column
:%s= \f\+$=&&=              : same thing
:%s= \S\+$=&&               : usually the same
" memory
%s#.*\(tbl_\w\+\).*#\1#     : produce a list of all strings tbl_*   *N*
:s/\(.*\):\(.*\)/\2 : \1/   : reverse fields separated by :
:%s/^\(.*\)\n\1$/\1/        : delete duplicate lines
" non-greedy matching \{-}
:%s/^.\{-}pdf/new.pdf/      : delete to 1st occurence of pdf only (non-greedy)
" use of optional atom \?
:%s#\<[zy]\?tbl_[a-z_]\+\>#\L&#gc : lowercase with optional leading characters
" over possibly many lines
:%s/<!--\_.\{-}-->//        : delete possibly multi-line comments
:help /\{-}                 : help non-greedy
" substitute using a register
:s/fred/<c-r>a/g            : sub "fred" with contents of register "a"
:s/fred/<c-r>asome_text<c-r>s/g
:s/fred/\=@a/g              : better alternative as register not displayed
" multiple commands on one line
:%s/\f\+\.gif\>/\r&\r/g | v/\.gif$/d | %s/gif/jpg/
:%s/a/but/gie|:update|:next : then use @: to repeat
" ORing
:%s/goat\|cow/sheep/gc      : ORing (must break pipe)
:'a,'bs#\[\|\]##g           : remove [] from lines between markers a and b *N*
:%s/\v(.*\n){5}/&\r         : insert a blank line every 5 lines *N*
" Calling a VIM function
:s/__date__/\=strftime("%c")/ : insert datestring
" Working with Columns sub any str1 in col3
:%s:\(\(\w\+\s\+\)\{2}\)str1:\1str2:
" Swapping first & last column (4 columns)
:%s:\(\w\+\)\(.*\s\+\)\(\w\+\)$:\3\2\1:
" format a mysql query
:%s#\<from\>\|\<where\>\|\<left join\>\|\<\inner join\>#\r&#g
" filter all form elements into paste register
:redir @*|sil exec 'g#<\(input\|select\|textarea\|/\=form\)\>#p'|redir END
:nmap ,z :redir @*<Bar>sil exec 'g@<\(input\<Bar>select\<Bar>textarea\<Bar>/\=form\)\>@p'<Bar>redir END<CR>
" substitute string in column 30 *N*
:%s/^\(.\{30\}\)xx/\1yy/
" decrement numbers by 3
:%s/\d\+/\=(submatch(0)-3)/
" increment numbers by 6 on certain lines only
:g/loc\|function/s/\d/\=submatch(0)+6/
" better
:%s#txtdev\zs\d#\=submatch(0)+1#g
:h /\zs
" increment only numbers gg\d\d  by 6 (another way)
:%s/\(gg\)\@<=\d\+/\=submatch(0)+6/
:h zero-width
" rename a string with an incrementing number
:let i=10 | 'a,'bg/Abc/s/yy/\=i/ |let i=i+1 # convert yy to 10,11,12 etc
" as above but more precise
:let i=10 | 'a,'bg/Abc/s/xx\zsyy\ze/\=i/ |let i=i+1 # convert xxyy to xx11,xx12,xx13
" find replacement text, put in memory, then use \zs to simplify substitute
:%s/"\([^.]\+\).*\zsxx/\1/
" Pull word under cursor into LHS of a substitute
:nmap <leader>z :%s#\<<c-r>=expand("<cword>")<cr>\>#
" Pull Visually Highlighted text into LHS of a substitute
:vmap <leader>z :<C-U>%s/\<<c-r>*\>/
" substitute singular or plural
:'a,'bs/bucket\(s\)*/bowl\1/gic   *N*
----------------------------------------
" all following performing similar task, substitute within substitution
" Multiple single character substitution in a portion of line only
:%s,\(all/.*\)\@<=/,_,g     : replace all / with _ AFTER "all/"
" Same thing
:s#all/\zs.*#\=substitute(submatch(0), '/', '_', 'g')#
" Substitute by splitting line, then re-joining
:s#all/#&^M#|s#/#_#g|-j!
" Substitute inside substitute
:%s/.*/\='cp '.submatch(0).' all/'.substitute(submatch(0),'/','_','g')/
----------------------------------------
" global command display
:g/gladiolli/#              : display with line numbers (YOU WANT THIS!)
:g/fred.*joe.*dick/         : display all lines fred,joe & dick
:g/\<fred\>/                : display all lines fred but not freddy
:g/^\s*$/d                  : delete all blank lines
:g!/^dd/d                   : delete lines not containing string
:v/^dd/d                    : delete lines not containing string
:g/joe/,/fred/d             : not line based (very powerfull)
:g/fred/,/joe/j             : Join Lines *N*
:g/-------/.-10,.d          : Delete string & 10 previous lines
:g/{/ ,/}/- s/\n\+/\r/g     : Delete empty lines but only between {...}
:v/\S/d                     : Delete empty lines (and blank lines ie whitespace)
:v/./,/./-j                 : compress empty lines
:g/^$/,/./-j                : compress empty lines
:g/<input\|<form/p          : ORing
:g/^/put_                   : double space file (pu = put)
:g/^/m0                     : Reverse file (m = move)
:g/^/m$                     : No effect! *N*
:'a,'bg/^/m'b               : Reverse a section a to b
:g/^/t.                     : duplicate every line
:g/fred/t$                  : copy(transfer) lines matching fred to EOF
:g/stage/t'a                : copy (transfer) lines matching stage to marker a (cannot use .) *C*
:g/^Chapter/t.|s/./-/g      : Automatically underline selecting headings *N*
:g/\(^I[^^I]*\)\{80}/d      : delete all lines containing at least 80 tabs
" perform a substitute on every other line
:g/^/ if line('.')%2|s/^/zz /
" match all lines containing "somestr" between markers a & b
" copy after line containing "otherstr"
:'a,'bg/somestr/co/otherstr/ : co(py) or mo(ve)
" as above but also do a substitution
:'a,'bg/str1/s/str1/&&&/|mo/str2/
:%norm jdd                  : delete every other line
" incrementing numbers (type <c-a> as 5 characters)
:.,$g/^\d/exe "norm! \<c-a>": increment numbers
:'a,'bg/\d\+/norm! ^A       : increment numbers
" storing glob results (note must use APPEND) you need to empty reg a first with qaq.
"save results to a register/paste buffer
:g/fred/y A                 : append all lines fred to register a
:g/fred/y A | :let @*=@a    : put into paste buffer
:let @a=''|g/Barratt/y A |:let @*=@a
" filter lines to a file (file must already exist)
:'a,'bg/^Error/ . w >> errors.txt
" duplicate every line in a file wrap a print '' around each duplicate
:g/./yank|put|-1s/'/"/g|s/.*/Print '&'/
" replace string with contents of a file, -d deletes the "mark"
:g/^MARK$/r tmp.txt | -d
" display prettily
:g/<pattern>/z#.5           : display with context
:g/<pattern>/z#.5|echo "=========="  : display beautifully
" Combining g// with normal mode commands
:g/|/norm 2f|r*                      : replace 2nd | with a star
"send output of previous global command to a new window
:nmap <F3>  :redir @a<CR>:g//<CR>:redir END<CR>:new<CR>:put! a<CR><CR>
"----------------------------------------
" Global combined with substitute (power editing)
:'a,'bg/fred/s/joe/susan/gic :  can use memory to extend matching
:/fred/,/joe/s/fred/joe/gic :  non-line based (ultra)
:/biz/,/any/g/article/s/wheel/bucket/gic:  non-line based *N*
----------------------------------------
" Find fred before beginning search for joe
:/fred/;/joe/-2,/sid/+3s/sally/alley/gIC
"----------------------------------------
" create a new file for each line of file eg 1.txt,2.txt,3,txt etc
:g/^/exe ".w ".line(".").".txt"
"----------------------------------------
" chain an external command
:.g/^/ exe ".!sed 's/N/X/'" | s/I/Q/    *N*
"----------------------------------------
" Operate until string found *N*
d/fred/                                :delete until fred
y/fred/                                :yank until fred
c/fred/e                               :change until fred end
"----------------------------------------
" Summary of editing repeats *N*
.      last edit (magic dot)
:&     last substitute
:%&    last substitute every line
:%&gic last substitute every line confirm
g%     normal mode repeat last substitute
g&     last substitute on all lines
@@     last recording
@:     last command-mode command
:!!    last :! command
:~     last substitute
:help repeating
----------------------------------------
" Summary of repeated searches
;      last f, t, F or T
,      last f, t, F or T in opposite direction
n      last / or ? search
N      last / or ? search in opposite direction
----------------------------------------
" Absolutely essential
----------------------------------------
# g* g#           : find word under cursor (<cword>) (forwards/backwards)
%                   : match brackets {}[]()
.                   : repeat last modification
@:                  : repeat last : command (then @@)
matchit.vim         : % now matches tags <tr><td><script> <?php etc
<C-N><C-P>          : word completion in insert mode
<C-X><C-L>          : Line complete SUPER USEFUL
/<C-R><C-W>         : Pull <cword> onto search/command line
/<C-R><C-A>         : Pull <CWORD> onto search/command line
:set ignorecase     : you nearly always want this
:set smartcase      : overrides ignorecase if uppercase used in search string (cool)
:syntax on          : colour syntax in Perl,HTML,PHP etc
:set syntax=perl    : force syntax (usually taken from file extension)
:h regexp<C-D>      : type control-D and get a list all help topics containing
                      regexp (plus use TAB to Step thru list)
----------------------------------------
" MAKE IT EASY TO UPDATE/RELOAD _vimrc
:nmap ,s :source $VIM/_vimrc
:nmap ,v :e $VIM/_vimrc
:e $MYVIMRC         : edits your _vimrc whereever it might be  *N*
" How to have a variant in your .vimrc for different PCs *N*
if $COMPUTERNAME == "NEWPC"
ab mypc vista
else
ab mypc dell25
endif
----------------------------------------
" splitting windows
:vsplit other.php       # vertically split current file with other.php *N*
----------------------------------------
"VISUAL MODE (easy to add other HTML Tags)
:vmap sb "zdi<b><C-R>z</b><ESC>  : wrap <b></b> around VISUALLY selected Text
:vmap st "zdi<?= <C-R>z ?><ESC>  : wrap <?=   ?> around VISUALLY selected Text
----------------------------------------
"vim 7 tabs
vim -p fred.php joe.php             : open files in tabs
:tabe fred.php                      : open fred.php in a new tab
:tab ball                           : tab open files
" vim 7 forcing use of tabs from .vimrc
:nnoremap gf <C-W>gf
:cab      e  tabe
:tab sball                           : retab all files in buffer (repair) *N*
----------------------------------------
" Exploring
:e .                            : file explorer
:Exp(lore)                      : file explorer note capital Ex
:Sex(plore)                     : file explorer in split window
:browse e                       : windows style browser
:ls                             : list of buffers
:cd ..                          : move to parent directory
:args                           : list of files
:args *.php                     : open list of files (you need this!)
:lcd %:p:h                      : change to directory of current file
:autocmd BufEnter * lcd %:p:h   : change to directory of current file automatically (put in _vimrc)
----------------------------------------
" Changing Case
guu                             : lowercase line
gUU                             : uppercase line
Vu                              : lowercase line
VU                              : uppercase line
g~~                             : flip case line
vEU                             : Upper Case Word
vE~                             : Flip Case Word
ggguG                           : lowercase entire file
" Titlise Visually Selected Text (map for .vimrc)
vmap ,c :s/\<\(.\)\(\k*\)\>/\u\1\L\2/g<CR>
" titlise a line
nmap ,t :s/.*/\L&/<bar>:s/\<./\u&/g<cr>  *N*
" Uppercase first letter of sentences
:%s/[.!?]\_s\+\a/\U&\E/g
----------------------------------------
gf                              : open file name under cursor (SUPER)
:nnoremap gF :view <cfile><cr>  : open file under cursor, create if necessary
ga                              : display hex,ascii value of char under cursor
ggVGg?                          : rot13 whole file
ggg?G                           : rot13 whole file (quicker for large file)
:8 | normal VGg?                : rot13 from line 8
:normal 10GVGg?                 : rot13 from line 8
<C-A>,<C-X>                     : increment,decrement number under cursor
                                  win32 users must remap CNTRL-A
<C-R>=5*5                       : insert 25 into text (mini-calculator)
----------------------------------------
" Make all other tips superfluous
:h 42            : also http://www.google.com/search?q=42
:h holy-grail
:h!
----------------------------------------
" disguise text (watch out) *N*
ggVGg?                          : rot13 whole file (toggles)
:set rl!                        : reverse lines right to left (toggles)
:g/^/m0                         : reverse lines top to bottom (toggles)
----------------------------------------
" Markers & moving about
'.               : jump to last modification line (SUPER)
`.               : jump to exact spot in last modification line
g;               : cycle thru recent changes (oldest first)
g,               : reverse direction
:changes
:h changelist    : help for above
<C-O>            : retrace your movements in file (starting from most recent)
<C-I>            : retrace your movements in file (reverse direction)
:ju(mps)         : list of your movements
:help jump-motions
:history         : list of all your commands
:his c           : commandline history
:his s           : search history
q/               : Search history Window (puts you in full edit mode) (exit CTRL-C)
q:               : commandline history Window (puts you in full edit mode) (exit CTRL-C)
:<C-F>           : history Window (exit CTRL-C)
----------------------------------------
" Abbreviations & Maps
" Following 4 maps enable text transfer between VIM sessions
:map   <f7>   :'a,'bw! c:/aaa/x       : save text to file x
:map   <f8>   :r c:/aaa/x             : retrieve text
:map   <f11>  :.w! c:/aaa/xr<CR>      : store current line
:map   <f12>  :r c:/aaa/xr<CR>        : retrieve current line
:ab php          : list of abbreviations beginning php
:map ,           : list of maps beginning ,
" allow use of F10 for mapping (win32)
set wak=no       : :h winaltkeys
" For use in Maps
<CR>             : carriage Return for maps
<ESC>            : Escape
<LEADER>         : normally \
<BAR>            : | pipe
<BACKSPACE>      : backspace
<SILENT>         : No hanging shell window
#display RGB colour under the cursor eg #445588
:nmap <leader>c :hi Normal guibg=#<c-r>=expand("<cword>")<cr><cr>
map <f2> /price only\\|versus/ :in a map need to backslash the \
# type table,,, to get <table></table>       ### Cool ###
imap ,,, <esc>bdwa<<esc>pa><cr></<esc>pa><esc>kA
----------------------------------------
" Simple PHP debugging display all variables yanked into register a
iab phpdb exit("<hr>Debug <C-R>a  ");
----------------------------------------
" Using a register as a map (preload registers in .vimrc)
:let @m=":'a,'bs/"
:let @s=":%!sort -u"
----------------------------------------
" Useful tricks
"ayy@a           : execute "Vim command" in a text file
yy@"             : same thing using unnamed register
u@.              : execute command JUST typed in
"ddw             : store what you delete in register d *N*
"ccaw            : store what you change in register c *N*
----------------------------------------
" Get output from other commands (requires external programs)
:r!ls -R         : reads in output of ls
:put=glob('**')  : same as above                 *N*
:r !grep "^ebay" file.txt  : grepping in content   *N*
:20,25 !rot13    : rot13 lines 20 to 25   *N*
!!date           : same thing (but replaces/filters current line)
" Sorting with external sort
:%!sort -u       : use an external program to filter content
:'a,'b!sort -u   : use an external program to filter content
!1} sort -u      : sorts paragraph (note normal mode!!)
:g/^$/;,/^$/-1!sort : Sort each block (note the crucial ;)
" Sorting with internal sort
:sort /.*\%2v/   : sort all lines on second column *N*
" number lines
:new | r!nl #                  *N*
----------------------------------------
" Multiple Files Management (Essential)
:bn              : goto next buffer
:bp              : goto previous buffer
:wn              : save file and move to next (super)
:wp              : save file and move to previous
:bd              : remove file from buffer list (super)
:bun             : Buffer unload (remove window but not from list)
:badd file.c     : file from buffer list
:b3              : go to buffer 3 *C*
:b main          : go to buffer with main in name eg main.c (ultra)
:sav php.html    : Save current file as php.html and "move" to php.html
:sav! %<.bak     : Save Current file to alternative extension (old way)
:sav! %:r.cfm    : Save Current file to alternative extension
:sav %:s/fred/joe/           : do a substitute on file name
:sav %:s/fred/joe/:r.bak2    : do a substitute on file name & ext.
:!mv % %:r.bak   : rename current file (DOS use Rename or DEL)
:help filename-modifiers
:e!              : return to unmodified file
:w c:/aaa/%      : save file elsewhere
:e #             : edit alternative file (also cntrl-^)
:rew             : return to beginning of edited files list (:args)
:brew            : buffer rewind
:sp fred.txt     : open fred.txt into a split
:sball,:sb       : Split all buffers (super)
:scrollbind      : in each split window
:map   <F5> :ls<CR>:e # : Pressing F5 lists all buffer, just type number
:set hidden      : Allows to change buffer w/o saving current buffer
----------------------------------------
" Quick jumping between splits
map <C-J> <C-W>j<C-W>_
map <C-K> <C-W>k<C-W>_
----------------------------------------
" Recording (BEST TIP of ALL)
qq  # record to q
your complex series of commands
q   # end recording
@q to execute
@@ to Repeat
5@@ to Repeat 5 times
qQ@qq                             : Make an existing recording q recursive *N*
" editing a register/recording
"qp                               :display contents of register q (normal mode)
<ctrl-R>q                         :display contents of register q (insert mode)
" you can now see recording contents, edit as required
"qdd                              :put changed contacts back into q
@q                                :execute recording/register q
" Operating a Recording on a Visual BLOCK
1) define recording/register
qq:s/ to/ from/g^Mq
2) Define Visual BLOCK
V}
3) hit : and the following appears
:'<,'>
4)Complete as follows
:'<,'>norm @q
----------------------------------------
"combining a recording with a map (to end up in command mode)
:nnoremap ] @q:update<bar>bd
----------------------------------------
" Visual is the newest and usually the most intuitive editing mode
" Visual basics
v                               : enter visual mode
V                               : visual mode whole line
<C-V>                           : enter VISUAL BLOCK mode
gv                              : reselect last visual area (ultra)
o                               : navigate visual area
"*y or "+y                      : yank visual area into paste buffer  *C*
V%                              : visualise what you match
V}J                             : Join Visual block (great)
V}gJ                            : Join Visual block w/o adding spaces
`[v`]                           : Highlight last insert
:%s/\%Vold/new/g                : Do a substitute on last visual area *N*
----------------------------------------
" Delete first 2 characters of 10 successive lines
0<c-v>10j2ld  (use Control Q on win32) *C*
----------------------------------------
" how to copy a set of columns using VISUAL BLOCK
" visual block (AKA columnwise selection) (NOT BY ordinary v command)
<C-V> then select "column(s)" with motion commands (win32 <C-Q>)
then c,d,y,r etc
----------------------------------------
" how to overwrite a visual-block of text with another such block *C*
" move with hjkl etc
Pick the first block: ctrl-v move y
Pick the second block: ctrl-v move P <esc>
----------------------------------------
" text objects :h text-objects                                     *C*
daW                                   : delete contiguous non whitespace
di<   yi<  ci<                        : Delete/Yank/Change HTML tag contents
da<   ya<  ca<                        : Delete/Yank/Change whole HTML tag
dat   dit                             : Delete HTML tag pair
diB   daB                             : Empty a function {}
das                                   : delete a sentence
----------------------------------------
" _vimrc essentials
:set incsearch : jumps to search word as you type (annoying but excellent)
:set wildignore=*.o,*.obj,*.bak,*.exe : tab complete now ignores these
:set shiftwidth=3                     : for shift/tabbing
:set vb t_vb=".                       : set silent (no beep)
:set browsedir=buffer                 : Maki GUI File Open use current directory
----------------------------------------
" launching Win IE
:nmap ,f :update<CR>:silent !start c:\progra~1\intern~1\iexplore.exe file://%:p<CR>
:nmap ,i :update<CR>: !start c:\progra~1\intern~1\iexplore.exe <cWORD><CR>
----------------------------------------
" FTPing from VIM
cmap ,r  :Nread ftp://209.51.134.122/public_html/index.html
cmap ,w  :Nwrite ftp://209.51.134.122/public_html/index.html
gvim ftp://www.somedomain.com/index.html # uses netrw.vim
----------------------------------------
" appending to registers (use CAPITAL)
" yank 5 lines into "a" then add a further 5
"a5yy
10j
"A5yy
----------------------------------------
[I     : show lines matching word under cursor <cword> (super)
----------------------------------------
" Conventional Shifting/Indenting
:'a,'b>>
" visual shifting (builtin-repeat)
:vnoremap < <gv
:vnoremap > >gv
" Block shifting (magic)
>i{
>a{
" also
>% and <%
----------------------------------------
" Redirection & Paste register *
:redir @*                    : redirect commands to paste buffer
:redir END                   : end redirect
:redir >> out.txt            : redirect to a file
" Working with Paste buffer
"*yy                         : yank curent line to paste
"*p                          : insert from paste buffer
" yank to paste buffer (ex mode)
:'a,'by*                     : Yank range into paste
:%y*                         : Yank whole buffer into paste
:.y*                         : Yank Current line to paster
" filter non-printable characters from the paste buffer
" useful when pasting from some gui application
:nmap <leader>p :let @* = substitute(@*,'[^[:print:]]','','g')<cr>"*p
----------------------------------------
" Re-Formatting text
gq}                          : Format a paragraph
gqap                         : Format a paragraph
ggVGgq                       : Reformat entire file
Vgq                          : current line
" break lines at 70 chars, if possible after a ;
:s/.\{,69\};\s*\|.\{,69\}\s\+/&\r/g
----------------------------------------
" Operate command over multiple files
:argdo %s/foo/bar/e          : operate on all files in :args
:bufdo %s/foo/bar/e
:windo %s/foo/bar/e
:argdo exe '%!sort'|w!       : include an external command
:bufdo exe "normal @q" | w   : perform a recording on open files
:silent bufdo !zip proj.zip %:p   : zip all current files
----------------------------------------
" Command line tricks
gvim -h                    : help
ls | gvim -                : edit a stream!!
cat xx | gvim - -c "v/^\d\d\|^[3-9]/d " : filter a stream
gvim -o file1 file2        : open into a split
" execute one command after opening file
gvim.exe -c "/main" joe.c  : Open joe.c & jump to "main"
" execute multiple command on a single file
vim -c "%s/ABC/DEF/ge | update" file1.c
" execute multiple command on a group of files
vim -c "argdo %s/ABC/DEF/ge | update" *.c
" remove blocks of text from a series of files
vim -c "argdo /begin/+1,/end/-1g/^/d | update" *.c
" Automate editing of a file (Ex commands in convert.vim)
vim -s "convert.vim" file.c
#load VIM without .vimrc and plugins (clean VIM)
gvim -u NONE -U NONE -N
" Access paste buffer contents (put in a script/batch file)
gvim -c 'normal ggdG"*p' c:/aaa/xp
" print paste contents to default printer
gvim -c 's/^/\=@*/|hardcopy!|q!'
" gvim's use of external grep (win32 or *nix)
:!grep somestring *.php     : creates a list of all matching files *C*
" use :cn(ext) :cp(rev) to navigate list
:h grep
" Using vimgrep with copen                              *N*
:vimgrep /keywords/ *.php
:copen
----------------------------------------
" GVIM Difference Function (Brilliant)
gvim -d file1 file2        : vimdiff (compare differences)
dp                         : "put" difference under cursor to other file
do                         : "get" difference under cursor from other file
" complex diff parts of same file *N*
:1,2yank a | 7,8yank b
:tabedit | put a | vnew | put b
:windo diffthis
----------------------------------------
" Vim traps
In regular expressions you must backslash + (match 1 or more)
In regular expressions you must backslash | (or)
In regular expressions you must backslash ( (group)
In regular expressions you must backslash { (count)
/fred\+/                   : matches fred/freddy but not free
/\(fred\)\{2,3}/           : note what you have to break
----------------------------------------
" \v or very magic (usually) reduces backslashing
/codes\(\n\|\s\)*where  : normal regexp
/\vcodes(\n|\s)*where   : very magic
----------------------------------------
" pulling objects onto command/search line (SUPER)
<C-R><C-W> : pull word under the cursor into a command line or search
<C-R><C-A> : pull WORD under the cursor into a command line or search
<C-R>-                  : pull small register (also insert mode)
<C-R>[0-9a-z]           : pull named registers (also insert mode)
<C-R>%                  : pull file name (also #) (also insert mode)
<C-R>=somevar           : pull contents of a variable (eg :let sray="ray[0-9]")
----------------------------------------
" List your Registers
:reg             : display contents of all registers
:reg a           : display content of register a
:reg 12a         : display content of registers 1,2 & a *N*
"5p              : retrieve 5th "ring"
"1p....          : retrieve numeric registers one by one
:let @y='yy@"'   : pre-loading registers (put in .vimrc)
qqq              : empty register "q"
qaq              : empty register "a"
:reg .-/%:*"     : the seven special registers *N*
:reg 0           : what you last yanked, not affected by a delete *N*
"_dd             : Delete to blackhole register "_ , don't affect any register *N*
----------------------------------------
" manipulating registers
:let @a=@_              : clear register a
:let @a=""              : clear register a
:let @a=@"              : Save unnamed register *N*
:let @*=@a              : copy register a to paste buffer
:let @*=@:              : copy last command to paste buffer
:let @*=@/              : copy last search to paste buffer
:let @*=@%              : copy current filename to paste buffer
----------------------------------------
" help for help (USE TAB)
:h quickref             : VIM Quick Reference Sheet (ultra)
:h tips                 : Vim's own Tips Help
:h visual<C-D><tab>     : obtain  list of all visual help topics
                        : Then use tab to step thru them
:h ctrl<C-D>            : list help of all control keys
:helpg uganda           : grep HELP Files use :cn, :cp to find next
:helpgrep edit.*director: grep help using regexp
:h :r                   : help for :ex command
:h CTRL-R               : normal mode
:h /\r                  : what's \r in a regexp (matches a <CR>)
:h \\zs                 : double up backslash to find \zs in help
:h i_CTRL-R             : help for say <C-R> in insert mode
:h c_CTRL-R             : help for say <C-R> in command mode
:h v_CTRL-V             : visual mode
:h tutor                : VIM Tutor
<C-[>, <C-T>            : Move back & Forth in HELP History
gvim -h                 : VIM Command Line Help
:cabbrev h tab h        : open help in a tab *N*
----------------------------------------
" where was an option set
:scriptnames            : list all plugins, _vimrcs loaded (super)
:verbose set history?   : reveals value of history and where set
:function               : list functions
:func SearchCompl       : List particular function
----------------------------------------
" making your own VIM help
:helptags /vim/vim64/doc  : rebuild all *.txt help files in /doc
:help add-local-help
----------------------------------------
" running file thru an external program (eg php)
map   <f9>   :w<CR>:!c:/php/php.exe %<CR>
map   <f2>   :w<CR>:!perl -c %<CR>
----------------------------------------
" capturing output of current script in a separate buffer
:new | r!perl #                   : opens new buffer,read other buffer
:new! x.out | r!perl #            : same with named file
:new+read!ls
----------------------------------------
" create a new buffer, paste a register "q" into it, then sort new buffer
:new +put q|%!sort
----------------------------------------
" Inserting DOS Carriage Returns
:%s/$/\<C-V><C-M>&/g          :  that's what you type
:%s/$/\<C-Q><C-M>&/g          :  for Win32
:%s/$/\^M&/g                  :  what you'll see where ^M is ONE character
----------------------------------------
" automatically delete trailing Dos-returns,whitespace
autocmd BufRead * silent! %s/[\r \t]\+$//
autocmd BufEnter *.php :%s/[ \t\r]\+$//e
----------------------------------------
" perform an action on a particular file or file type
autocmd VimEnter c:/intranet/note011.txt normal! ggVGg?
autocmd FileType *.pl exec('set fileformats=unix')
----------------------------------------
" Retrieving last command line command for copy & pasting into text
i<c-r>:
" Retrieving last Search Command for copy & pasting into text
i<c-r>/
----------------------------------------
" more completions
<C-X><C-F>                        :insert name of a file in current directory
----------------------------------------
" Substituting a Visual area
" select visual area as usual (:h visual) then type :s/Emacs/Vim/ etc
:'<,'>s/Emacs/Vim/g               : REMEMBER you dont type the '<.'>
gv                                : Re-select the previous visual area (ULTRA)
----------------------------------------
" inserting line number into file
:g/^/exec "s/^/".strpart(line(".")."    ", 0, 4)
:%s/^/\=strpart(line(".")."     ", 0, 5)
:%s/^/\=line('.'). ' '
----------------------------------------
#numbering lines VIM way
:set number                       : show line numbers
:map <F12> :set number!<CR>       : Show linenumbers flip-flop
:%s/^/\=strpart(line('.')."        ",0,&ts)
#numbering lines (need Perl on PC) starting from arbitrary number
:'a,'b!perl -pne 'BEGIN{$a=223} substr($_,2,0)=$a++'
#Produce a list of numbers
#Type in number on line say 223 in an empty file
qqmnYP`n^Aq                       : in recording q repeat with @q
" increment existing numbers to end of file (type <c-a> as 5 characters)
:.,$g/^\d/exe "normal! \<c-a>"
" advanced incrementing
http://vim.sourceforge.net/tip_view.php?tip_id=150
----------------------------------------
" advanced incrementing (really useful)
" put following in _vimrc
let g:I=0
function! INC(increment)
let g:I =g:I + a:increment
return g:I
endfunction
" eg create list starting from 223 incrementing by 5 between markers a,b
:let I=223
:'a,'bs/^/\=INC(5)/
" create a map for INC
cab viminc :let I=223 \| 'a,'bs/$/\=INC(5)/
----------------------------------------
" generate a list of numbers  23-64
o23<ESC>qqYp<C-A>q40@q
----------------------------------------
" editing/moving within current insert (Really useful)
<C-U>                             : delete all entered
<C-W>                             : delete last word
<HOME><END>                       : beginning/end of line
<C-LEFTARROW><C-RIGHTARROW>       : jump one word backwards/forwards
<C-X><C-E>,<C-X><C-Y>             : scroll while staying put in insert
----------------------------------------
#encryption (use with care: DON'T FORGET your KEY)
:X                                : you will be prompted for a key
:h :X
----------------------------------------
" modeline (make a file readonly etc) must be in first/last 5 lines
// vim:noai:ts=2:sw=4:readonly:
" vim:ft=html:                    : says use HTML Syntax highlighting
:h modeline
----------------------------------------
" Creating your own GUI Toolbar entry
amenu  Modeline.Insert\ a\ VIM\ modeline <Esc><Esc>ggOvim:ff=unix ts=4 ss=4<CR>vim60:fdm=marker<esc>gg
----------------------------------------
" A function to save word under cursor to a file
function! SaveWord()
   normal yiw
   exe ':!echo '.@0.' >> word.txt'
endfunction
map ,p :call SaveWord()
----------------------------------------
" function to delete duplicate lines
function! Del()
 if getline(".") == getline(line(".") - 1)
   norm dd
 endif
endfunction

:g/^/ call Del()
----------------------------------------
" Digraphs (non alpha-numerics)
:digraphs                         : display table
:h dig                            : help
i<C-K>e'                          : enters é
i<C-V>233                         : enters é (Unix)
i<C-Q>233                         : enters é (Win32)
ga                                : View hex value of any character
#Deleting non-ascii characters (some invisible)
:%s/[\x00-\x1f\x80-\xff]/ /g      : type this as you see it
:%s/[<C-V>128-<C-V>255]//gi       : where you have to type the Control-V
:%s/[€-ÿ]//gi                     : Should see a black square & a dotted y
:%s/[<C-V>128-<C-V>255<C-V>01-<C-V>31]//gi : All pesky non-asciis
:exec "norm /[\x00-\x1f\x80-\xff]/"        : same thing
#Pull a non-ascii character onto search bar
yl/<C-R>"                         :
/[^a-zA-Z0-9_[:space:][:punct:]]  : search for all non-ascii
----------------------------------------
" All file completions grouped (for example main_c.c)
:e main_<tab>                     : tab completes
gf                                : open file under cursor  (normal)
main_<C-X><C-F>                   : include NAME of file in text (insert mode)
----------------------------------------
" Complex Vim
" swap two words
:%s/\<\(on\|off\)\>/\=strpart("offon", 3 * ("off" == submatch(0)), 3)/g
" swap two words
:vnoremap <C-X> <Esc>`.``gvP``P
" Swap word with next word
nmap <silent> gw    "_yiw:s/\(\%#\w\+\)\(\_W\+\)\(\w\+\)/\3\2\1/<cr><c-o><c-l> *N*
----------------------------------------
" Convert Text File to HTML
:runtime! syntax/2html.vim        : convert txt to html
:h 2html
----------------------------------------
" VIM has internal grep
:grep some_keyword *.c            : get list of all c-files containing keyword
:cn                               : go to next occurrence
----------------------------------------
" Force Syntax coloring for a file that has no extension .pl
:set syntax=perl
" Remove syntax coloring (useful for all sorts of reasons)
:set syntax off
" change coloring scheme (any file in ~vim/vim??/colors)
:colorscheme blue
" Force HTML Syntax highlighting by using a modeline
# vim:ft=html:
" Force syntax automatically (for a file with non-standard extension)
au BufRead,BufNewFile */Content.IE?/* setfiletype html
----------------------------------------
:set noma (non modifiable)        : Prevents modifications
:set ro (Read Only)               : Protect a file from unintentional writes
----------------------------------------
" Sessions (Open a set of files)
gvim file1.c file2.c lib/lib.h lib/lib2.h : load files for "session"
:mksession                        : Make a Session file (default Session.vim)
:q
gvim -S Session.vim               : Reload all files
----------------------------------------
#tags (jumping to subroutines/functions)
taglist.vim                       : popular plugin
:Tlist                            : display Tags (list of functions)
<C-]>                             : jump to function under cursor
----------------------------------------
" columnise a csv file for display only as may crop wide columns
:let width = 20
:let fill=' ' | while strlen(fill) < width | let fill=fill.fill | endwhile
:%s/\([^;]*\);\=/\=strpart(submatch(1).fill, 0, width)/ge
:%s/\s\+$//ge
" Highlight a particular csv column (put in .vimrc)
function! CSVH(x)
    execute 'match Keyword /^\([^,]*,\)\{'.a:x.'}\zs[^,]*/'
    execute 'normal ^'.a:x.'f,'
endfunction
command! -nargs=1 Csv :call CSVH(<args>)
" call with
:Csv 5                             : highlight fifth column
----------------------------------------
zf1G      : fold everything before this line *N*
" folding : hide sections to allow easier comparisons
zf}                               : fold paragraph using motion
v}zf                              : fold paragraph using visual
zf'a                              : fold to mark
zo                                : open fold
zc                                : re-close fold
:help folding
zfG      : fold everything after this line *N*
----------------------------------------
" displaying "non-asciis"
:set list
:h listchars
----------------------------------------
" How to paste "normal commands" w/o entering insert mode
:norm qqy$jq
----------------------------------------
" manipulating file names
:h filename-modifiers             : help
:w %                              : write to current file name
:w %:r.cfm                        : change file extention to .cfm
:!echo %:p                        : full path & file name
:!echo %:p:h                      : full path only
:!echo %:t                        : filename only
:reg %                            : display filename
<C-R>%                            : insert filename (insert mode)
"%p                               : insert filename (normal mode)
/<C-R>%                           : Search for file name in text
----------------------------------------
" delete without destroying default buffer contents
"_d                               : what you've ALWAYS wanted
"_dw                              : eg delete word (use blackhole)
----------------------------------------
" pull full path name into paste buffer for attachment to email etc
nnoremap <F2> :let @*=expand("%:p")<cr> :unix
nnoremap <F2> :let @*=substitute(expand("%:p"), "/", "\\", "g")<cr> :win32
----------------------------------------
" Simple Shell script to rename files w/o leaving vim
$ vim
:r! ls *.c
:%s/\(.*\).c/mv & \1.bla
:w !sh
:q!
----------------------------------------
" count words/lines in a text file
g<C-G>                                 # counts words
:echo line("'b")-line("'a")            # count lines between markers a and b *N*
:'a,'bs/^//n                           # count lines between markers a and b
:'a,'bs/somestring//gn                 # count occurences of a string
----------------------------------------
" example of setting your own highlighting
:syn match DoubleSpace "  "
:hi def DoubleSpace guibg=#e0e0e0
----------------------------------------
" reproduce previous line word by word
imap ]  @@@<ESC>hhkyWjl?@@@<CR>P/@@@<CR>3s
nmap ] i@@@<ESC>hhkyWjl?@@@<CR>P/@@@<CR>3s
" Programming keys depending on file type
:autocmd bufenter *.tex map <F1> :!latex %<CR>
:autocmd bufenter *.tex map <F2> :!xdvi -hush %<.dvi&<CR>
----------------------------------------
" reading Ms-Word documents, requires antiword
:autocmd BufReadPre *.doc set ro
:autocmd BufReadPre *.doc set hlsearch!
:autocmd BufReadPost *.doc %!antiword "%"
----------------------------------------
" a folding method
vim: filetype=help foldmethod=marker foldmarker=<<<,>>>
A really big section closed with a tag <<<
--- remember folds can be nested ---
Closing tag >>>
----------------------------------------
" Return to last edit position (You want this!) *N*
autocmd BufReadPost *
     \ if line("'\"") > 0 && line("'\"") <= line("$") |
     \   exe "normal! g`\"" |
     \ endif
----------------------------------------
" store text that is to be changed or deleted in register a
"act<                                 :  Change Till < *N*
----------------------------------------
# using gVIM with Cygwin on a Windows PC
if has('win32')
source $VIMRUNTIME/mswin.vim
behave mswin
set shell=c:\\cygwin\\bin\\bash.exe shellcmdflag=-c shellxquote=\"
endif
----------------------------------------
" Just Another Vim Hacker JAVH
vim -c ":%s%s*%Cyrnfr)fcbafbe[Oenz(Zbbyranne%|:%s)[[()])-)Ig|norm Vg?"
----------------------------------------
__END__
----------------------------------------
"Read Vimtips into a new vim buffer (needs w3m.sourceforge.net)
:tabe | :r ! w3m -dump http://zzapper.co.uk/vimtips.html    *N*
----------------------------------------
updated version at http://www.zzapper.co.uk/vimtips.html
----------------------------------------
Please email any errors, tips etc to
david@rayninfo.co.uk
" Information Sources
----------------------------------------
www.vim.org
Vim Wiki *** VERY GOOD *** *N*
Vim Use VIM newsgroup *N*
comp.editors
groups.yahoo.com/group/vim "VIM" specific newsgroup
VIM Webring
Vim Book
Searchable VIM Doc
VimTips PDF Version (PRINTABLE!)
----------------------------------------
" : commands to neutralise < for HTML display and publish
" use yy@" to execute following commands
:w!|sav! vimtips.html|:/^__BEGIN__/,/^__END__/s#<#\<#g|:w!|:!vimtipsftp
----------------------------------------

History & Attributions
:LOGBOOK:
CLOCK: [2024-06-18 Tue 13:07]--[2024-06-18 Tue 13:10] =>  0:03
:END:
[2024-06-18 Tue 13:07]
* Quick notes on Operating System Rings and their implication :Technical:NOTE:

Linux x86 ring usage overview

Understanding how rings are used in Linux will give you a good idea of what they are designed for.

In x86 protected mode, the CPU is always in one of 4 rings. The Linux kernel only uses 0 and 3:

    0 for kernel
    3 for users


Ring 1 and Ring 2

On the other hand, rings 1 and ring 2 offer unique advantages that ring 3
lacks. The OS uses ring 1 to interact with the computer’s hardware. This ring
would need to run commands such as streaming a video through a camera on our
monitor. Instructions that must interact with the system storage, loading, or
saving files are stored in ring 2.

These rights are known as input and output permissions because they involve transferring data into and out of working memory, RAM.

In ring 2, for example, loading an Excel file document from storage. In such a case, ring 3 will be responsible for editing and saving the data.

This is the most hard and fast definition of kernel vs userland.

Why Linux does not use rings 1 and 2: CPU Privilege Rings: Why rings 1 and 2 aren't used?

How is the current ring determined?

The current ring is selected by a combination of:

    global descriptor table: a in-memory table of GDT entries, and each entry has a field Privl which encodes the ring.

    The LGDT instruction sets the address to the current descriptor table.

    See also: http://wiki.osdev.org/Global_Descriptor_Table

    the segment registers CS, DS, etc., which point to the index of an entry in the GDT.

    For example, CS = 0 means the first entry of the GDT is currently active for the executing code.

What can each ring do?

The CPU chip is physically built so that:

    ring 0 can do anything

    ring 3 cannot run several instructions and write to several registers, most notably:

        cannot change its own ring! Otherwise, it could set itself to ring 0 and rings would be useless.

        In other words, cannot modify the current segment descriptor, which determines the current ring.

        cannot modify the page tables: How does x86 paging work?

        In other words, cannot modify the CR3 register, and paging itself prevents modification of the page tables.

        This prevents one process from seeing the memory of other processes for security / ease of programming reasons.

        cannot register interrupt handlers. Those are configured by writing to memory locations, which is also prevented by paging.

        Handlers run in ring 0, and would break the security model.

        In other words, cannot use the LGDT and LIDT instructions.

        cannot do IO instructions like in and out, and thus have arbitrary hardware accesses.

        Otherwise, for example, file permissions would be useless if any program could directly read from disk.

        More precisely thanks to Michael Petch: it is actually possible for the OS to allow IO instructions on ring 3, this is actually controlled by the Task state segment.

        What is not possible is for ring 3 to give itself permission to do so if it didn't have it in the first place.

        Linux always disallows it. See also: Why doesn't Linux use the hardware context switch via the TSS?

How do programs and operating systems transition between rings?

    when the CPU is turned on, it starts running the initial program in ring 0
    (well kind of, but it is a good approximation). You can think this initial
    program as being the kernel (but it is normally a bootloader that then calls
    the kernel still in ring 0).

    when a userland process wants the kernel to do something for it like write
    to a file, it uses an instruction that generates an interrupt such as int
    0x80 or syscall to signal the kernel. x86-64 Linux syscall hello world
    example:

    .data
    hello_world:
        .ascii "hello world\n"
        hello_world_len = . - hello_world
    .text
    .global _start
    _start:
        /* write */
        mov $1, %rax
        mov $1, %rdi
        mov $hello_world, %rsi
        mov $hello_world_len, %rdx
        syscall

        /* exit */
        mov $60, %rax
        mov $0, %rdi
        syscall

    compile and run:

    as -o hello_world.o hello_world.S
    ld -o hello_world.out hello_world.o
    ./hello_world.out

    GitHub upstream.

    When this happens, the CPU calls an interrupt callback handler which the kernel registered at boot time. Here is a concrete baremetal example that registers a handler and uses it.

    This handler runs in ring 0, which decides if the kernel will allow this action, do the action, and restart the userland program in ring 3. x86_64

    when the exec system call is used (or when the kernel will start /init), the
    kernel prepares the registers and memory of the new userland process, then
    it jumps to the entry point and switches the CPU to ring 3

    If the program tries to do something naughty like write to a forbidden register or memory address (because of paging), the CPU also calls some kernel callback handler in ring 0.

    But since the userland was naughty, the kernel might kill the process this time, or give it a warning with a signal.

    When the kernel boots, it setups a hardware clock with some fixed frequency, which generates interrupts periodically.

    This hardware clock generates interrupts that run ring 0, and allow it to schedule which userland processes to wake up.

    This way, scheduling can happen even if the processes are not making any system calls.

What is the point of having multiple rings?

There are two major advantages of separating kernel and userland:

    it is easier to make programs as you are more certain one won't interfere
    with the other. E.g., one userland process does not have to worry about
    overwriting the memory of another program because of paging, nor about
    putting hardware in an invalid state for another process.  it is more
    secure. E.g. file permissions and memory separation could prevent a hacking
    app from reading your bank data. This supposes, of course, that you trust
    the kernel.

How to play around with it?

I've created a bare metal setup that should be a good way to manipulate rings directly: https://github.com/cirosantilli/x86-bare-metal-examples

I didn't have the patience to make a userland example unfortunately, but I did go as far as paging setup, so userland should be feasible. I'd love to see a pull request.

Alternatively, Linux kernel modules run in ring 0, so you can use them to try
out privileged operations, e.g. read the control registers: How to access the
control registers cr0,cr2,cr3 from a program? Getting segmentation fault

Here is a convenient QEMU + Buildroot setup to try it out without killing your host.

The downside of kernel modules is that other kthreads are running and could
interfere with your experiments. But in theory you can take over all interrupt
handlers with your kernel module and own the system, that would be an
interesting project actually.

Negative rings

While negative rings are not actually referenced in the Intel manual, there are
actually CPU modes which have further capabilities than ring 0 itself, and so
are a good fit for the "negative ring" name.

One example is the hypervisor mode used in virtualization.

For further details see:

    https://security.stackexchange.com/questions/129098/what-is-protection-ring-1
    https://security.stackexchange.com/questions/216527/ring-3-exploits-and-existence-of-other-rings

ARM

In ARM, the rings are called Exception Levels instead, but the main ideas remain the same.

There exist 4 exception levels in ARMv8, commonly used as:

    EL0: userland

    EL1: kernel ("supervisor" in ARM terminology).

    Entered with the svc instruction (SuperVisor Call), previously known as swi before unified assembly, which is the instruction used to make Linux system calls. Hello world ARMv8 example:

    hello.S

    .text
    .global _start
    _start:
        /* write */
        mov x0, 1
        ldr x1, =msg
        ldr x2, =len
        mov x8, 64
        svc 0

        /* exit */
        mov x0, 0
        mov x8, 93
        svc 0
    msg:
        .ascii "hello syscall v8\n"
    len = . - msg

    GitHub upstream.

    Test it out with QEMU on Ubuntu 16.04:

    sudo apt-get install qemu-user gcc-arm-linux-gnueabihf
    arm-linux-gnueabihf-as -o hello.o hello.S
    arm-linux-gnueabihf-ld -o hello hello.o
    qemu-arm hello

    Here is a concrete baremetal example that registers an SVC handler and does an SVC call.

    EL2: hypervisors, for example Xen.

    Entered with the hvc instruction (HyperVisor Call).

    A hypervisor is to an OS, what an OS is to userland.

    For example, Xen allows you to run multiple OSes such as Linux or Windows on
    the same system at the same time, and it isolates the OSes from one another
    for security and ease of debug, just like Linux does for userland programs.

    Hypervisors are a key part of today's cloud infrastructure: they allow
    multiple servers to run on a single hardware, keeping hardware usage always
    close to 100% and saving a lot of money.

    AWS for example used Xen until 2017 when its move to KVM made the news.

    EL3: yet another level. TODO example.

    Entered with the smc instruction (Secure Mode Call)

The ARMv8 Architecture Reference Model DDI 0487C.a - Chapter D1 - The AArch64 System Level Programmer's Model - Figure D1-1 illustrates this beautifully:

The ARM situation changed a bit with the advent of ARMv8.1 Virtualization Host Extensions (VHE). This extension allows the kernel to run in EL2 efficiently:

VHE was created because in-Linux-kernel virtualization solutions such as KVM
have gained ground over Xen (see e.g. AWS' move to KVM mentioned above), because
most clients only need Linux VMs, and as you can imagine, being all in a single
project, KVM is simpler and potentially more efficient than Xen. So now the host
Linux kernel acts as the hypervisor in those cases.

Note how ARM, maybe due to the benefit of hindsight, has a better naming
convention for the privilege levels than x86, without the need for negative
levels: 0 being the lower and 3 highest. Higher levels tend to be created more
often than lower ones.

The current EL can be queried with the MRS instruction: what is the current execution mode/exception level, etc?

ARM does not require all exception levels to be present to allow for implementations that don't need the feature to save chip area. ARMv8 "Exception levels" says:

An implementation might not include all of the Exception levels. All implementations must include EL0 and EL1. EL2 and EL3 are optional.

QEMU for example defaults to EL1, but EL2 and EL3 can be enabled with command line options: qemu-system-aarch64 entering el1 when emulating a53 power up

:LOGBOOK:
CLOCK: [2024-07-26 Fri 14:15]--[2024-07-26 Fri 14:17] =>  0:02
:END:
[2024-07-26 Fri 14:15]
* Linux tidbits and pebbble                                  :Technical:NOTE:

1) systemd-analyze outputs, since that only shows units that completed
   activation

2)TPM 2.0 exists and status /sys/class/tpm/tpm0/tpm_version_major say?
/dev/tpmrm0

3) /dev/ttyS* files are controlled by this kernel config
   CONFIG_SERIAL_8250_RUNTIME_UARTS and this can be overridden by this kernel
   command parameter : 8250.nr_uarts=n.

4) On most systems /dev is a special tmpfs — a devtmpfs, a single-instance
   filesystem where the kernel automatically creates device nodes. It doesn't
   exist anywhere on permanent storage

5) An initrd is a real filesystem, packaged up into a file and compressed. But
   an initramfs is just a cpio archive (also compressed). It needs to be
   unpacked into some filesystem. The kernel creates a ramfs for this, unpacks
   the archive into it, then runs init in it

6) shell uses what is called a logical working directory. This essentially
   tracks the symlinks used to reach the current directory. When you run cd ..,
   that simply removes the last component of the logical working directory, even
   when that doesn't lead to the current directory's actual parent directory.

Logical working directories are (mostly) a shell-only thing. Almost everything
else just uses the regular physical working directory instead.

7) very few reasons to use aliases, and a lot of reasons not to. Aliases are
   processed as the shell input is read, not as shell commands are executed,
   which means they work completely differently from most other forms of
   expansion.

8) In order to boot the system the kernel needs two things:

    a root filesystem;
    an init process.

Booting the system is simply a matter of mounting the root filesystem at /, then
running init in it. That's it!

It is possible to have the kernel directly mount a regular filesystem as the
root filesystem, but there are significant limitations if you do that. You can
only use block devices and filesystems whose drivers are built into the kernel,
i.e. not provided as kernel modules. You can't do anything fancy or complicated,
like setting up decryption keys to unlock encrypted filesystems.

So most systems boot into a special memory-only filesystem first instead. This
is the "initramfs". It is, quite literally, just a ramfs filesystem. Part of the
contents of that filesystem are built into the kernel itself. The rest of the
contents are unpacked from a file loaded by the kernel as it is booting. (How
can the kernel do that if it doesn't have any filesystem drivers built in?
Sometimes the file is given to it by the boot loader. Sometimes it can use the
system firmware to access the file instead.)

That memory-only filesystem is where the kernel runs init. What the init process
does after that is entirely up to that init process. Typically its job is to:

    load whatever kernel modules are needed to boot the system;
    get the "real" root filesystem mounted at /newroot;
    pivot into that real root filesystem — this makes /newroot become /, and makes the old / become /oldroot;
    unmount the old initramfs, now at /oldroot;
    execute some init process inside the new root filesystem at /.

(A slight variant uses chroot instead of pivot_root. That's OK, it just means
the original initramfs cannot be unmounted. It can be emptied out so it
effectively uses no memory.)

Where does systemd fit into this? Some systems use systemd within the initramfs
itself, that is the init in the initramfs is systemd. But it still just executes
a completely separate systemd in the real root filesystem after pivoting into
it.

And even among those systems that uses systemd normally, not all of them use
systemd in the initramfs. Many systems just use a shell script in the initramfs.

So to answer your questions specifically:

    I thought initramfs is used to set initial parameters and configurations and after that it starts kernel.

No, the kernel starts first. It loads the initramfs, mounts it at /, and executes init in it.

    But if its destroyed after kernel is started, how some hooks like systemd keep running?

Once init in the initramfs is running the system is "booted", at least as far as
the kernel is concerned. It has mounted a root filesystem, and it has run
init. Done!

From then on everything is just ordinary processes doing ordinary process
things. There is a gentleman's agreement that processes launched from the
initramfs shouldn't survive past the pivot into the "real" root filesystem, but
the kernel itself doesn't care.

If systemd is used in the initramfs, it will explicitly execute a new systemd
after pivoting into the real root filesystem. I'm not quite sure what you mean
by "hooks", but certainly there can be different systemd instances during
different phases of system startup.


9) Polkit configuration is in /etc/polkit-1/rules.d/ for local settings and
   /usr/share/polkit-1/rules.d/ for package-managed settings. See if anything has
   added rules for the various org.freedesktop.login1.* actions.

By default, in the absence of any rules, a local active user — even an
unprivileged user — should be able to power-off or reboot the system without
authentication.

10) this broke is because /dev is a shared mount, so unmounting /mnt/dev/pts
    will unmount /dev/pts as well (and similarly for other mount points under
    /mnt/dev). Read "Shared subtree operations" in mount(8), and the kernel
    documentation it refers to.


11) C strings are arrays of char, not arrays of unsigned char

12) unsigned char array. strcpy requires a char pointer. unsigned char and char
    are not the same type.

13) C doesn't have an exponentiation operator.

14) The primary name for the zone is Etc/UTC. UTC is included in the timezone
    database for compatibility.

15) getchar only ever reads one character at a time. putchar only ever writes
    one character at a time. You need just as many calls to putchar as you had
    to getchar to transfer all the characters.

I left out a couple of buffers in the above comment. When I said the OS makes
the input "available" to your program... well, that needs to be stored
somewhere. That's another buffer. And getchar is likely to transfer the whole
input line from the operating system, including the trailing newline. It will
store that in your program's input buffer, and only return one character out of
this buffer at a time. A call to getchar will only ask the OS for more data —
and wait until that becomes available — if that input buffer is empty.


16) There are a couple of internal arrays. One of them is on the data path
    before the getchar function. This is your operating system's terminal line
    editing buffer. It handles a few editing operations, like Backspace to
    remove the previous character. It's only when you press Enter the contents
    of this buffer are made available to the program reading from your terminal.

(Fun fact: on Linux and Unix systems, at least, you can actually do this last
step without pressing Enter. Instead of pressing Enter, use Ctrl+D. You might
think this is only to signal an "end of file", but all it actually does is
instruct the OS to make the contents of the line-editing buffer immediately
available to the program. It happens to be the case that when this buffer is
empty that means a program reading from the terminal will have a zero-byte read,
and it will interpret that as if the end of file had been encountered. But if
the line-editing buffer is not empty its contents will just be made available to
your program, even though you haven't yet pressed Enter.)

Another internal buffer is what putchar writes into. Normally the contents of
this buffer are only transferred back to the operating system — to be displayed
on your terminal — when your program outputs a newline character. You can force
the contents of this buffer to be transferred immediately using the fflush
function.

17) In C89, the first clause of a for loop cannot be a declaration.

18) GNU Grep doesn't use Perl, it uses PCRE

19) Grep uses Gnulib's RE library.

20) X programs receive events in order. It shouldn't matter how much time
    elapses between the events. They would have to go out of their way to make
    that time duration significant in some way

21) Rsync only considers the last modification timestamp as part of its "are
    these files definitely different without me having to checksum them?" logic.

22) Rsync only looks at last modification timestamps, not last metadata change
    timestamps

23) Use the --hard-links option (aka -H) to preserve the links between files on
    the source when they are synced to the target. This option, --archive
    --hard-links --acls --xattrs --inplace --no-inc-recursive

24) A filesystem on Linux is simply anything that provides the appropriate VFS
    API. What it does internally is up to it. For ramfs in particular, the API
    essentially manipulates the dentry, inode and page caches directly.

Sure, GRUB can preload an initrd into memory before executing the Linux kernel —
a peculiar quirk of one of Linux's boot protocols — but GRUB itself doesn't even
know what the initrd is for. Nowadays, Linux doesn't even use the initrd
directly as a filesystem; instead, it unpacks it into a ramfs.

25) Get a backtrace for the task with bt <pid>.

bt takes a whole bunch of options — see help bt for details. The -l option in
particular will resolve the return addresses to particular source code lines.

The name of the kworker encodes information about the CPU it is bound to, or the
pool ID if the worker is unbound, as well as the current or last work performed
by the worker.

26) Kernel tasks always are forked off PID 2, kthreadd

27) Some shells will send SIGHUP to all jobs when they exit. For Bash, that is
    only in a login shell, and only when the huponexit shell option has been
    enabled (which is not the default setting).

    The terminal line discipline will also send SIGHUP to the foreground process
    group with that terminal as its controlling terminal. A background process,
    by definition, is not in the foreground process group.

28) screen with set-user-ID or set-group-ID after all, i.e. you can't do
    multiuser sessions in it.)

29) ls shows length of the file in size, whereas du shows the actual byte it
    took to reside there.

30) Signal numbers only go up to 64

31) When your CPU is running a program, it isn't reading assembly, it's reading
    machine code. The different assembly flavours all get assembled down to the
    same machine code.

Assembly is really just about making machine code easier to read and write for
humans.

32)  an ordinary disk-backed filesystem! When you write some data to a file, the
    data first goes into the page cache, and the kernel starts writing that out
    to storage in the background

33) when you use bash -c the first argument after the command is assigned to $0,
    not $1.

34) Cron changed between UNIX System III and UNIX System V

35) Anyway, Vixie Cron took on the System V interpretation of crontab (and in
    its code comments it has the temerity to call it the "standard" behaviour
    for Cron), and then all the later Cron implementations copied Vixie Cron.

36) a mouse device reports relative positioning information, whereas a
    touchscreen or touchpad might report absolute positioning information
    instead.

37) Operating systems don't install their own keys into the firmware

38) POSIX doesn't care how the system is implemented. It merely specifies a C
    API. What the system does internally in order to perform the operations
    provided by that API is out-of-scope for POSIX.

39) oop condition expressions must be "conditiony" in their form, i.e. have an
    explicit == or != or < operator, or something similar. That's not how C
    works. Any scalar expression can be used as a condition

40) C23 requires two's complement signed integers,


41) File creation time usually cannot be changed from userspace.


42) socat doesn't support HTTPS proxies

43) GRUB on Fedora normally uses the blscfg GRUB module, so on boot GRUB just
    reads the boot loader specification files to dynamically generate the menu
    entries.

44) Inline assembly within C code is one way to provide this hardware
    interface. The compiler is already in the business of turning C code into
    assembly code, so letting you add your own assembly in the middle of that is
    a natural extension.


45) gparted requires a polkit authentication agent


46) The Java Virtual Machine describes in detail how a real machine should
    interpret the Java bytecode. That is, you can take the Java VM specification
    and literally implement it as described (in hardware or in software), and
    your Java programs should "just work".

But the C abstract machine doesn't do that. It doesn't say anything about how
the program is evaluated. It simply says "given this C code, this is the
result". It specifies the observable behaviour of the C code, not how that
behaviour is actually enacted.

47) gets reads a line from standard input and writes it to the buffer you give
    it. There is no limit to the length of this line, which means there is no
    limit to the amount of data gets will write to memory, which means it can
    always run off the end of any buffer you give it, no matter how big that
    buffer is.

In other words, it is impossible to use gets without introducing the possibility
of a buffer overflow into your program.

48) I'm assuming you meant to write bs=1GB there.

That means all read and write operations performed by dd are 1 gigabyte in
size. Reads and writes to storage are uninterruptible. When a signal is sent to
the process, the IO operation must complete, or hit some kind of error. The
operation cannot be partially aborted.

1 GB is quite a lot, so all of this means that it can take quite some time
before the signal is actually delivered.

IO operations to "things that aren't storage" can be interrupted: you can get
what's called a "short read" or a "short write". Indeed that is a big gotcha
when using dd on non-storage file descriptors, like pipes or sockets. The count=
parameter just tells it the number of loops to perform, but if there are short
IO operations in the mix that doesn't necessarily mean the amount of data you
thought would be copied actually is copied.

There is little point to using such a large block size in dd — past a couple of
dozen kilobytes, the syscall overhead is negligible. In fact, large block sizes
like that tend to be slower since it means the copied data cannot live entirely
in your CPU's cache. You are essentially forcing the program to read a whole
gigabyte of data into memory, then go back to the top and write it all out
again. With a smaller block size the entire read and write operation can often
be done without hitting RAM at all.

49)When you run reboot, systemd asks Polkit whether you are authorised to reboot
the system. By default this will be configured so that if you are a local user
in an active session then no password is necessary. All other users must
authenticate as an admin — i.e. as root or as a user in the wheel group.

But this is only the default policy. You can add your own rules to override
it. For instance, if you were to drop:

polkit.addRule(function(action, subject) {
    if (action == "org.freedesktop.login1.reboot") {
        return polkit.Result.AUTH_ADMIN_KEEP;
    }
});

into /etc/polkit-1/rules.d/50-local.rules, then all users, no matter whether
they're local or not, would be required to authenticate as an admin to perform
this action.

50)Lifetime is the period of time during which an object has a usable value. It
is mostly governed by the storage duration for the allocation:

    automatic storage duration, which begins when execution enters the scope in
    which the object's variable is declared and ends when it leaves that scope;
    allocated storage duration, which begins when malloc is called, and ends
    when the pointer returned from that is given to free; thread storage
    duration, which lasts during the entire runtime of a particular thread in
    the program; static storage duration, which lasts during the entire runtime
    of the program.

The object's lifetime is the sub-period of that storage duration where the
object has a defined value.

Normally, the "one good choice" is the shortest storage duration that is long
enough to do what you want. In other words it's determined based on how and
where you intend to use the object.

51) start en interactive shell with empty environment bu running this
    env --ignore-environment bash -i -c env

52)The blocked signal mask is a per-process thing. The su process has SIGINT
blocked. The child process does not.

53) When you press Ctrl+C, the terminal line discipline sends SIGINT to all
    processes in the terminal's foreground process group that do not have the
    signal ignored. It is sent to processes where it is blocked, but it won't do
    anything to that process unless and until that process unblocks it.

54) You can't mmap a binary for execution (PROT_EXEC) if it's on a noexec
    filesystem.


55)When you are raising your privileges to the superuser, su will always add
SIGINT and SIGQUIT to its blocked signal mask. With that in place you don't have
to worry about them killing the su process itself.

I think the only time su keeps SIGINT and SIGQUIT unblocked are when you are
dropping to an unprivileged user and using --command= (not
--session-command=). That is when su uses the setsid(2) syscall, running the
child process in a new session, and so it now has to propagate terminal signals
into that session.

56) Bash has an echo builtin

57) Command substitutions in Bash will always strip any and all null bytes.  in
    Bash, there will only be a single null byte at the end. All of the null
    bytes produced by the substitution will be discarded. The single null byte
    at the end of this command's output is the one you provided in the format
    string.

    Shells have inconsistent behaviour with null bytes due to the way strings
    are traditionally handled in C: a string is terminated by a null byte, so it
    simply cannot contain any other null bytes. Bash avoids any issues by
    stripping null bytes in a command substitution.

58) EINTR should only be returned if a signal handler was actually executed. I
    am having trouble finding anything to say that must be the behaviour though
    (e.g. in POSIX).

    current Linux does not appear to return EINTR in the absence of a signal
    handler... but I only tested things with pause, not any other syscall.

(If you're wondering what ERESTARTNOHAND means in that thread, it's the internal
Linux error code to have a syscall automatically restarted if there is no signal
handler, without returning to userspace.)


59)  Signal issues related to Linux : 2006
    https://lore.kernel.org/lkml/44A92DC8.9000401@gmx.net/


60)set -e is implicitly disabled during the execution of commands on the
left-hand side of && or || (plus various other places).

61)Stream buffers are managed entirely by the C library. They're not part of the
kernel's process state, so their properties are not inherited across
execve. There's languages other than C, of course, so it's entirely possible the
new process doesn't even have stream buffers.

https://man7.org/linux/man-pages/man1/stdbuf.1.html

62)

    alias e='echo $@' # semi-worked

No, this doesn't even semi-work the way you think it does. The $@ there doesn't
mean "arguments for the alias", because aliases don't have arguments. In the
syntactic locations aliases are used, an alias is quite literally expanded as
soon as it is read. The shell doesn't even know what text occurs after it, since
it hasn't read that text yet. There is no opportunity to turn that text into
arguments.

63) A pipe doesn't use any permanent storage when transferring data.

A named pipe isn't any different. The name lives in the filesystem, so that name
may be on permanent storage... but the pipe it names does not. The pipe exists
in RAM, and only for as long as any processes have it open.

64)Single-quoted strings don't interpret any escapes, period.

65) Cygwin provides a POSIX system interface. It doesn't "translate system
    calls"; it allows programs that are written to use the POSIX system
    interface to be compiled for Windows.

The POSIX system interface consists of the C standard library, a large number of
extensions to that library, and the overall behaviour of the operating system
provided by that library. As an example of this last point, applications using
POSIX expect signals to work in a particular way. Cygwin will try to emulate
that even though Windows has completely different interprocess communication
mechanisms.

Programs built to use Cygwin are just ordinary Windows programs that happen to
use a particular library. You cannot run those programs without that library.

Theoretically you could run a Windows program that uses Cygwin through Wine
though. It'd be kind of silly, unless for some reason you had to use that
program without access to its original source code.

An alternative to Cygwin is MinGW, however its focus is somewhat
different. MinGW is about providing just enough tooling so that developers can
easily compile software for Windows, rather than providing a full POSIX
environment for existing POSIX software to be ported to Windows. Software that
can be built under both POSIX and MinGW will often need different code paths for
the two environments. Cygwin, on the other hand, attempts to make POSIX software
portable to Windows without needing Windows-specific code paths.

66) buffer is just a memory region for temporarily storing data

67) Why do I need to pass .so to gcc? It needs more than just the name of the
    library. It needs to know what symbols the library provides.  linker is
    completely language-agnostic, so it certainly can't be given this
    information by the C compiler from a C header file associated with the
    library.
The C compiler itself doesn't need it. However, that gcc command also
automatically runs the linker, and the linker does need it.

There's a few reasons for this. Let's say your program is calling a hello
function in that library. If you don't actually tell the linker about the
library, then it will simply assume that you've forgotten to implement the hello
function, and it will abort with an error message.

You don't want that; you want the linker to prepare the call sites so that they
will work once the program and its libraries have all been loaded. This
preparation involves the generation of stub functions that can be rewritten to
jumps to the library's real functions, once the addresses of those real
functions are known, i.e. when your program is actually executed.

And that leads to a second reason the linker needs to know about the library. It
needs to record information in the executable saying your program needs the
library at runtime, and what relocations are needed at runtime to glue
everything together. Without that, the library wouldn't even get loaded when you
ran your program.


68) Same page cache is used for read and write.

69) Linux has sometimes been called "nothing more than a page cache and a
    scheduler". The page cache is a fundamental component. It is not there to do
    ordinary file IO faster, it's there to do it at all.

70) The contents of the page cache is simply a result of the operations on the
    filesystem performed by the user's programs.

71) It's not as if the operating system is making decisions about "what it
    should load" and "what it should not". If it were doing that then you could
    legitimately ask "but what if it makes the wrong decisions?"

No, that's not what's happening at all. The user needs to access files. It's the
user making those decisions. The contents of the page cache is simply a result
of the operations on the filesystem performed by the user's programs.

If all of your programs are idle, for instance, the contents of the page cache
do not change at all. If you read or write a file that will exist (at least in
part) in the page cache... for some period of time. Perhaps not very long if you
only access it once, and other things need RAM. Somewhat longer if you access it
multiple times, even if other things need RAM. But regardless, the contents of
the page cache are simply a result of the read and write operations up to that
point in time, not something the OS has decided upon on its own.


Run some programs that need lots of memory. That number will go down when the
memory is used by something else.

Sometimes I wish the page cache wasn't actually called a "cache". It doesn't
work much like the cache in, say, your browser. The size of the page cache is
simply a natural consequence of the way Linux accesses files.

What do I mean by this? When you read or write to a file, the parts of the file
that get accessed need to be mapped somewhere in memory. This mapping normally
occurs in the page cache. Subsequent reads from those portions of the file can
be satisfied without having to access storage. Subsequent writes to those
portions of the file can be staged in the page cache. The kernel performs the
actual writeback to storage asynchronously, and most of the time the program
doesn't actually need to wait for that to happen.

In other words, the page cache is quite literally the view the kernel has onto
the active parts of files that are being accessed. It's not primarily a means to
make file access faster. It's simply the way ordinary file accesses are
done. This is why I think calling it a "cache" is so misleading.

The kernel will automatically remove mappings in the page cache that haven't
been accessed for a while, but it only needs to do that if something else
actually needs the memory. It's not going to expend CPU cycles doing that if
there is no good reason for it.


72) .link files are Udev configuration files, not systemd-networkd configuration
    files. They tell Udev what to do with a network link when it is detected.

systemd-networkd doesn't directly use .link files at all. systemd-networkd
learns independently of Udev that a network link has become available by simply
using the kernel's netlink interface. It waits until Udev has finished
processing it, and then it applies the appropriate .network file for it.

And .netdev files are just static configuration for virtual network
devices. Yes, systemd-networkd needs to track which .netdevs reference which
.networks, but apart from that they are processed pretty much
independently. There's never any need to wait for a virtual network device to be
"detected"; it gets configured as soon as the non-virtual networks it needs
become available.

So I'd say the whole lot is a lot less tangled up than you've got it
there. There's basically just three independent systems, with three sets of
config files.


73)

:LOGBOOK:
CLOCK: [2025-02-24 Mon 10:42]--[2025-02-24 Mon 19:10] =>  8:28
:END:
[2025-02-24 Mon 10:42]
* git submodule count git log and git rev-parse                        :NOTE:
For both git log and git rev-list, if you give it a pathspec it limits the
revisions to those in which that pathspec changed. If you don't give it a
pathspec, it doesn't.

Furthermore, if the submodule directory is actually a Git submodule, not just a
subdirectory, then your two commands are listing revisions from completely
different repositories. Git uses the repository that covers the current
directory, so if you're using submodules that depends on what your current
directory is. The reference to the submodule lives in the parent repository;
that reference need not have the same number of changes as there are commits in
the submodule itself.
:LOGBOOK:
CLOCK: [2025-03-03 Mon 12:50]--[2025-03-03 Mon 12:51] =>  0:01
:END:
[2025-03-03 Mon 12:50]
* bash string operator quirk EOF                                       :NOTE:


    "If the redirection operator is ‘<<-’, then all leading tab characters are
    stripped from input lines and the line containing delimiter. This allows
    here-documents within shell scripts to be indented in a natural fashion."

    https://www.gnu.org/savannah-checkouts/gnu/bash/manual/bash.html#Here-Documents

:LOGBOOK:
CLOCK: [2025-03-12 Wed 03:50]--[2025-03-12 Wed 03:51] =>  0:01
:END:
[2025-03-12 Wed 03:50]
